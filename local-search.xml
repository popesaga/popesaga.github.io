<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>HBase 学习：Region 迁移、合并、分裂和负载均衡</title>
    <link href="/2020/11/13/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9ARegion%20%E8%BF%81%E7%A7%BB%E3%80%81%E5%90%88%E5%B9%B6%E3%80%81%E5%88%86%E8%A3%82%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
    <url>/2020/11/13/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9ARegion%20%E8%BF%81%E7%A7%BB%E3%80%81%E5%90%88%E5%B9%B6%E3%80%81%E5%88%86%E8%A3%82%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
    
    <content type="html"><![CDATA[<p>负载均衡是分布式系统的必备功能，多个节点组成的分布式系统必须通过负载均衡机制保证各个节点之间负载的均衡性，一旦出现负载非常集中的情况，就很有可能导致对应的部分节点响应变慢，进而拖慢甚至拖垮整个集群。HBase 基于 Region 的数量实现负载均衡。平衡 Region 数量会进行 Region 迁移。为了避免单个 Region 过大或者过小，可以对 Region 做合并或分裂操作。本文首先介绍 Region 的迁移、合并和分裂，最后介绍基于 Region 的负载均衡策略。</p><a id="more"></a><h3 id="Region-迁移"><a href="#Region-迁移" class="headerlink" title="Region 迁移"></a>Region 迁移</h3><p>作为一个分布式系统，分片迁移是最基础的核心功能。集群负载均衡、故障恢复等功能都是建立在分片迁移的基础之上的。比如集群负载均衡，可以简单理解为集群中所有节点上的分片数目保持相同。实际执行分片迁移时可以分为两个步骤：第一步，根据负载均衡策略制定分片迁移计划；第二步，根据迁移计划执行分片的实际迁移。</p><p>HBase 系统中，分片迁移就是 Region 迁移。和其他很多分布式系统不同，HBase 中 Region 迁移是一个非常轻量级的操作。所谓轻量级，是因为 HBase 的数据实际存储在 HDFS 上，不需要独立进行管理，因而 Region 在迁移的过程中不需要迁移实际数据，只要将读写服务迁移即可。</p><h4 id="迁移过程"><a href="#迁移过程" class="headerlink" title="迁移过程"></a>迁移过程</h4><p>Region 迁移大体上分为两个阶段：unassign 阶段和 assign 阶段。</p><p>unassign region 的具体流程为：</p><ul><li>create zk closing node。该节点在/unassigned 路径下， 包含(znode 状态，region 名字，原始 RS 名，payload)这些数据。</li><li>hmaster 调用 rpc 服务关闭 region server。region-close 的流程大致为先获取 region 的 writeLock ， 然后 flush memstore, 再并发关闭该 region 下的所有的 store file 文件(注意一个 region 有多个 store，每个 store 又有多个 store file , 所以可以实现并发 close store file) 。最后释放 region 的 writeLock。</li><li>设置 zk closing node 的 znode 状态为 closed。</li></ul><p>assgin region 的具体流程为：</p><ul><li>获取到对应的 Region Plan。</li><li>HMaster 调用 rpc 服务去 Region Plan 对应的 RegionServer 上 open region。这里会先更新/unassigned 节点为 opening。然后并发 Load HStore，再更行 zk/ROOT/META 表信息，这里是为了 client 下次能获取到正确的路由信息， 最后更新 region 状态为 OPEN。</li></ul><h4 id="Region-In-Transition-RIT"><a href="#Region-In-Transition-RIT" class="headerlink" title="Region In Transition (RIT)"></a>Region In Transition (RIT)</h4><p>Region-In-Transition 说的是 Region 变迁机制，实际上是指在一次特定操作行为中 Region 状态的变迁。HBase 定义的 Region 状态见下表：</p><div class="table-container"><table><thead><tr><th>状态</th><th>说明</th></tr></thead><tbody><tr><td>OFFLINE</td><td>下线状态</td></tr><tr><td>OPENING</td><td>region 正在打开</td></tr><tr><td>OPEN</td><td>region 正常打开</td></tr><tr><td>FAILED_OPEN</td><td>region 打开失败</td></tr><tr><td>CLOSING</td><td>region 正在关闭</td></tr><tr><td>CLOSED</td><td>region 正常关闭</td></tr><tr><td>FAILED_CLOSE</td><td>region 关闭失败</td></tr><tr><td>SPLITTING</td><td>region 正在执行分裂</td></tr><tr><td>SPLIT</td><td>region 完成分裂</td></tr><tr><td>SPLITTING_NEW</td><td>分裂过程中产生新 region</td></tr><tr><td>MERGING</td><td>region 正在执行合并</td></tr><tr><td>MERGED</td><td>region 合并完成</td></tr><tr><td>MERGING_NEW</td><td>两个 region 合并过程后形成新 region</td></tr></tbody></table></div><p>region 迁移过程中会伴随 region 状态的不断变迁。因为 unassign/assign 操作都是由多个子操作组成的，涉及到 RS、Master、ZK 多个组件之间的协调合作，只有记录 region 状态才能知道当前 unassign/assign 的进度，在异常发生时才能根据具体进度继续执行。</p><p>region 的这些状态会存储在三个区域： meta 表、master 内存和 zk 的 region-in-transition 节点。它们的作用分别如下：</p><ul><li>meta 表只存储 region 所在 rs，并不存储迁移过程中的中间状态。</li><li>master 内存中存储了整个集群所有的 region 信息，可以获取任意一个 region 当前的状态以及存储在哪个 rs 上。<br>master 存储的 region 状态变更都是由 rs 通过 zk 通知给 master 的，因此 master 存储的 region 状态变更总是滞后于真正的 region 状态变更。</li><li>zk 中存储的是临时性的状态转移信息，作为 master 和 rs 之间反馈 region 状态爱的通道。</li></ul><p>只有 rs、master 和 zk 中 region 的状态都保持一致时，对应的 region 才处于正常的工作状态。当 region 状态在三个地方不能保持一致时，就会出现 region-in-transition(RIT)现象。</p><h4 id="Assignment-Manager-V2"><a href="#Assignment-Manager-V2" class="headerlink" title="Assignment Manager V2"></a>Assignment Manager V2</h4><p>通过上面的分析发现，虽然迁移是一个轻量级操作，但是实现逻辑非常复杂。存在如下缺点：</p><ul><li>region 状态变化复杂</li><li>region 状态多处缓存</li><li>重度依赖 Zookeeper</li></ul><p>当 region 数达到百万级别时，Assignment Mananger 成为了一个严重瓶颈。目前 hbase2.0 已结有了新的实现方案（Assignment ManangerV2），完全摆脱了 zk 依赖。它引入了 ProcedureV2 这个持久化存储来保存 Region transition 中的各个状态，保证在 master 重启时，之前的 assing/unassign，split 等任务能够从中断点重新执行。再者，之前的 AM 使用的 Zookeeper watch 机制通知 master region 状态的改变，而现在每当 RegionServer Open 或者 close 一个 region 后，都会直接发送 RPC 给 master 汇报，因此也不需要 Zookeeper 来做状态的通知。</p><h3 id="Region-合并"><a href="#Region-合并" class="headerlink" title="Region 合并"></a>Region 合并</h3><p>相比 Region 分裂，在线合并 Region 的使用场景比较有限，最典型的一个应用场景是，在某些业务中本来接收写入的 Region 在之后的很长时间都不再接收任何写入，而且 Region 上的数据因为 TTL 过期被删除。这种场景下的 Region 实际上没有任何存在的意义，称为空闲 Region。一旦集群中空闲 Region 很多，就会导致集群管理运维成本增加。此时，可以使用在线合并功能将这些 Region 与相邻的 Region 合并，减少集群中空闲 Region 的个数。</p><p>Region 合并的主要流程如下：</p><ol><li>客户端发送 merge 请求给 Master。</li><li>Master 将待合并的所有 Region 都 move 到同一个 RegionServer 上。</li><li>Master 发送 merge 请求给该 RegionServer。</li><li>RegionServer 启动一个本地事务执行 merge 操作。</li><li>merge 操作将待合并的两个 Region 下线，并将两个 Region 的文件进行合并。</li><li>将这两个 Region 从 hbase：meta 中删除，并将新生成的 Region 添加到 hbase：meta 中。</li><li>将新生成的 Region 上线。</li></ol><p>HBase 使用 merge_region 命令执行 Region 合并。merge_region 是一个异步操作，命令执行之后会立刻返回，用户需要一段时间之后手动检测合并是否成功。默认情况下 merge_region 命令只能合并相邻的两个 Region，非相邻的 Region 无法执行合并操作。同时 HBase 也提供了一个可选参数 true，使用此参数可以强制让不相邻的 Region 进行合并，因为该参数风险较大，一般并不建议生产线上使用。</p><h3 id="Region-分裂"><a href="#Region-分裂" class="headerlink" title="Region 分裂"></a>Region 分裂</h3><p>Region 分裂是 HBase 最核心的功能之一，是实现分布式可扩展性的基础。HBase 中，Region 分裂有多种触发策略可以配置，一旦触发，HBase 会寻找分裂点，然后执行真正的分裂操作。</p><h4 id="Region分裂触发策略"><a href="#Region分裂触发策略" class="headerlink" title="Region分裂触发策略"></a>Region分裂触发策略</h4><p>在当前版本中，HBase 已经有 6 种分裂触发策略。每种触发策略都有各自的适用场景，用户可以根据业务在表级别选择不同的分裂触发策略。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201113165639.png" alt="image-20201113165629125"></p><ul><li>ConstantSizeRegionSplitPolicy：0.94 版本前默认切分策略。这是最容易理解但也最容易产生误解的切分策略，从字面意思来看，当 region 大小大于某个阈值（<code>hbase.hregion.max.filesize</code>）之后就会触发切分，实际上并不是这样，真正实现中这个阈值是对于某个 store 来说的，即一个 region 中最大 store 的大小大于设置阈值之后才会触发切分。另外一个大家比较关心的问题是这里所说的 store 大小是压缩后的文件总大小还是未压缩文件总大小，实际实现中 store 大小为压缩后的文件大小（采用压缩的场景）。ConstantSizeRegionSplitPolicy 相对来来说最容易想到，但是在生产线上这种切分策略却有相当大的弊端：切分策略对于大表和小表没有明显的区分。阈值（<code>hbase.hregion.max.filesize</code>）设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就 1 个，这对业务来说并不是什么好事。如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的 region，这对于集群的管理、资源使用、failover 来说都不是一件好事。</li><li><p>IncreasingToUpperBoundRegionSplitPolicy: 0.94 版本~2.0 版本默认切分策略。这种切分策略微微有些复杂，总体来看和 ConstantSizeRegionSplitPolicy 思路相同，一个 region 中最大 store 大小大于设置阈值就会触发切分。但是这个阈值并不像 ConstantSizeRegionSplitPolicy 是一个固定的值，而是会在一定条件下不断调整，调整规则和 region 所属表在当前 regionserver 上的 region 个数有关系 ：<code>(#regions) * (#regions) * (#regions) * flush size * 2</code>，当然阈值并不会无限增大，最大值为用户设置的 MaxRegionFileSize。这种切分策略很好的弥补了 ConstantSizeRegionSplitPolicy 的短板，能够自适应大表和小表。而且在大集群条件下对于很多大表来说表现很优秀，但并不完美，这种策略下很多小表会在大集群中产生大量小 region，分散在整个集群中。而且在发生 region 迁移时也可能会触发 region 分裂。</p></li><li><p>SteppingSplitPolicy: 2.0 版本默认切分策略。这种切分策略的切分阈值又发生了变化，相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些，依然和待分裂 region 所属表在当前 regionserver 上的 region 个数有关系，如果 region 个数等于 1，切分阈值为<code>flush size * 2</code>，否则为<code>MaxRegionFileSize</code>。这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小 region，而是适可而止。</p></li></ul><p>另外，还有一些其他分裂策略，比如使用 DisableSplitPolicy 可以禁止 region 发生分裂；而 KeyPrefixRegionSplitPolicy，DelimitedKeyPrefixRegionSplitPolicy 对于切分策略依然依据默认切分策略，但对于切分点有自己的看法，比如 KeyPrefixRegionSplitPolicy 要求必须让相同的 PrefixKey 待在一个 region 中。</p><h4 id="寻找SplitPoint"><a href="#寻找SplitPoint" class="headerlink" title="寻找SplitPoint"></a>寻找SplitPoint</h4><p>region 切分策略会触发 region 切分，切分开始之后的第一件事是寻找切分点 splitpoint。切分点整个 region 中最大 store 中的最大文件中最中心的一个 block 的首个 rowkey。另外，HBase 还规定，如果定位到的 rowkey 是整个文件的首个 rowkey 或者最后一个 rowkey 的话，就认为没有切分点。没有切分点最常见的就是一个文件只有一个 block，执行 split 的时候就会发现无法切分。</p><h4 id="Region切分核心流程"><a href="#Region切分核心流程" class="headerlink" title="Region切分核心流程"></a>Region切分核心流程</h4><p>HBase 将整个切分过程包装成了一个事务，意图能够保证切分事务的原子性。整个分裂事务过程分为三个阶段：prepare – execute – (rollback) 。</p><ul><li><p>prepare 阶段：在内存中初始化两个子 region，具体是生成两个 HRegionInfo 对象，包含 tableName、regionName、startkey、endkey 等。同时会生成一个 transaction journal，这个对象用来记录切分的进展，具体见 rollback 阶段。</p></li><li><p>execute 阶段：</p><ul><li><p>RegionServer 修改 <code>/hbase/region-in-transition/region-name</code> 节点信息为 SPLITING。</p></li><li><p>master 通过 watch 节点/region-in-transition 检测到 region 状态改变，并修改内存中 region 的状态，在 master 页面 RIT 模块就可以看到 region 执行 split 的状态信息。</p></li><li><p>在父存储目录下新建临时文件夹.split 保存 split 后的 daughter region 信息。</p></li><li><p>关闭 parent region：parent region 关闭数据写入并触发 flush 操作，将写入 region 的数据全部持久化到磁盘。此后短时间内客户端落在父 region 上的请求都会抛出异常 NotServingRegionException。客户端会间隔一定时间后重试几次，正在关闭的 region 会被刷新。</p></li><li><p>在.split 文件夹下新建两个子文件夹，称之为 daughter A、daughter B，并在文件夹中生成 reference 文件，分别指向父 region 中对应文件。这个步骤是所有步骤中最核心的一个环节。</p></li><li><p>父 region 分裂为两个子 region 后，将 daughter A、daughter B 拷贝到 HBase 根目录下，形成两个新的 region。</p></li><li><p>parent region 通知修改<code>.META.</code>表后下线，不再提供服务。下线后 parent region 在 meta 表中的信息并不会马上删除，而是标注 split 列、offline 列为 true，并记录两个子 region。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201113183923.png" alt="image-20201113183922514"></p></li><li><p>开启 daughter A、daughter B 两个子 region。通知修改 hbase.meta 表，正式对外提供服务。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201113183956.png" alt="image-20201113183955342"></p></li><li><p>RegionServer 更新<code>/hbase/region-in-transition/region-name</code> 节点信息为<code>SPLIT，</code> Master 会知道状态改变，如果有需要，Balancer 会重新分派子 region 到其他 RegionServer。</p></li><li><p>在分裂后，<code>.META.</code> 和 HDFS 有 reference file 指向父 region，这些 references 在 region 被压缩重写数据文件时被清除。在 Master 中的 Garbage collection 任务将周期性的检查子 region 是否仍然引用父 region，如果没有引用，父 region 将被清除。</p></li></ul></li><li><p>rollback 阶段：如果 execute 阶段出现异常，则执行 rollback 操作。为了实现回滚，整个切分过程被分为很多子阶段，回滚程序会根据当前进展到哪个子阶段清理对应的垃圾数据。代码中使用 JournalEntryType 来表征各个子阶段，具体见下图：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201113175427.png" alt="image-20201113175426581"></p></li></ul><h4 id="Region切分事务性保证"><a href="#Region切分事务性保证" class="headerlink" title="Region切分事务性保证"></a>Region切分事务性保证</h4><p>整个 region 切分是一个比较复杂的过程，涉及到父 region 中 HFile 文件的切分、两个子 region 的生成、系统 meta 元数据的更改等很多子步骤，因此必须保证整个切分过程的事务性，即要么切分完全成功，要么切分完全未开始，在任何情况下也不能出现切分只完成一半的情况。</p><p>为了实现事务性，hbase 设计了使用状态机（见 SplitTransaction 类）的方式保存切分过程中的每个子步骤状态，这样一旦出现异常，系统可以根据当前所处的状态决定是否回滚，以及如何回滚。</p><p>遗憾的是，目前实现中这些中间状态都只存储在内存中，因此一旦在切分过程中出现 regionserver 宕机的情况，有可能会出现切分处于中间状态的情况，也就是 RIT 状态。这种情况下需要使用 hbck 工具进行具体查看并分析解决方案。</p><p>在 2.0 版本之后，HBase 实现了新的分布式事务框架 Procedure V2(HBASE-12439)，新框架将会使用 HLog 存储这种单机事务（DDL 操作、Split 操作、Move 操作等）的中间状态，因此可以保证即使在事务执行过程中参与者发生了宕机，依然可以使用 HLog 作为协调者对事务进行回滚操作或者重试提交，大大减少甚至杜绝 RIT 现象。</p><h4 id="Region切分后续"><a href="#Region切分后续" class="headerlink" title="Region切分后续"></a>Region切分后续</h4><p>通过 region 切分流程的了解，我们知道整个 region 切分过程并没有涉及数据的移动，所以切分成本本身并不是很高，可以很快完成。切分后子 region 的文件实际没有任何用户数据，文件中存储的仅是一些元数据信息－切分点 rowkey 等，那通过引用文件如何查找数据呢？子 region 的数据实际在什么时候完成真正迁移？数据迁移完成之后父 region 什么时候会被删掉？</p><h5 id="通过reference文件如何查找数据？"><a href="#通过reference文件如何查找数据？" class="headerlink" title="通过reference文件如何查找数据？"></a>通过reference文件如何查找数据？</h5><p>整个流程如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201113185602.png" alt="image-20201113185600836"></p><ol><li>根据 reference 文件名（region 名+真实文件名）定位到真实数据所在文件路径。</li><li>定位到真实数据文件就可以在整个文件中扫描待查 KV 了么？非也。因为 reference 文件通常都只引用了数据文件的一半数据，以切分点为界，要么上半部分文件数据，要么下半部分数据。那到底哪部分数据？切分点又是哪个点？还记得上文又提到 reference 文件的文件内容吧，没错，就记录在文件中。</li></ol><h5 id="父region的数据什么时候会迁移到子region目录？"><a href="#父region的数据什么时候会迁移到子region目录？" class="headerlink" title="父region的数据什么时候会迁移到子region目录？"></a>父region的数据什么时候会迁移到子region目录？</h5><p>子 region 执行 Major Compaction 后会将父目录中属于该子 region 的所有数据读出来并写入子 region 目录数据文件中。</p><h5 id="父region什么时候会被删除？"><a href="#父region什么时候会被删除？" class="headerlink" title="父region什么时候会被删除？"></a>父region什么时候会被删除？</h5><p>实 HMaster 会启动一个线程定期遍历检查所有处于 splitting 状态的父 region，确定检查父 region 是否可以被清理。检测线程首先会在 meta 表中揪出所有 split 列为 true 的 region，并加载出其分裂后生成的两个子 region（meta 表中 splitA 列和 splitB 列），只需要检查此两个子 region 是否还存在引用文件，如果都不存在引用文件就可以认为该父 region 对应的文件可以被删除。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201113185631.png" alt="image-20201113185629824"></p><h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><h4 id="Rebalance-策略"><a href="#Rebalance-策略" class="headerlink" title="Rebalance 策略"></a>Rebalance 策略</h4><p>HBase 官方目前支持两种负载均衡策略：SimpleLoadBalancer 策略和 StochasticLoadBalancer 策略。</p><p>此外，<a href="https://issues.apache.org/jira/browse/HDFS-6133">HDFS-6133</a> 通过在 HDFS 服务配置中将<code>dfs.datanode.block-pinning.enabled</code>属性设置为<code>true</code>，可以从 HDFS 负载均衡器中排除优先节点（固定）块。通过将 HBase 平衡器类（conf：<code>hbase.master.loadbalancer.class</code>）切换为<code>org.apache.hadoop.hbase.favored.FavoredNodeLoadBalancer</code>，可以启用 HBase 以使用 HDFS 优先节点功能。HDFS-6133 在 HDFS 2.7.0 及更高版本中可用，但 HBase 不支持在 HDFS 2.7.0 上运行，因此必须使用 HDFS 2.7.1 或更高版本才能将此功能与 HBase 一起使用。</p><h5 id="SimpleLoadBalancer策略"><a href="#SimpleLoadBalancer策略" class="headerlink" title="SimpleLoadBalancer策略"></a>SimpleLoadBalancer策略</h5><p>这种策略能够保证每个 RegionServer 的 Region 个数基本相等，假设集群中一共有<code>n</code>个 RegionServer，<code>m</code>个 Region，那么集群的平均负载就是<code>average=m/n</code>，这种策略能够保证所有 RegionServer 上的 Region 个数都在<code>［floor（average），ceil（average）］</code>之间。<br>因此，SimpleLoadBalancer 策略中负载就是 Region 个数，集群负载迁移计划就是 Region 从个数较多的 RegionServer 上迁移到个数较少的 RegionServer 上。</p><p>这种策略没有考虑 RegionServer 上的读写 QPS、数据量大小等因素。这样就可能出现一种情况：虽然集群中每个 RegionServer 的 Region 个数都基本相同，但因为某台 RegionServer 上的 Region 全部都是热点数据，导致 90%的读写请求还是落在了这台 RegionServer 上，这样显而易见没有达到负载均衡的目的。</p><h5 id="StochasticLoadBalancer策略"><a href="#StochasticLoadBalancer策略" class="headerlink" title="StochasticLoadBalancer策略"></a>StochasticLoadBalancer策略</h5><p>StochasticLoadBalancer 是 HBase 的默认策略，简单来讲，是一种综合权衡一下 6 个因素的均衡策略：</p><ul><li>每台 RegionServer 读请求数(ReadRequestCostFunction)</li><li>每台 RegionServer 写请求数(WriteRequestCostFunction)</li><li>每台 RegionServer 的 Region 个数(RegionCountSkewCostFunction)</li><li>移动代价(MoveCostFunction)</li><li>数据 locality(TableSkewCostFunction)</li><li>每张表占据 RegionServer 中 region 个数上限(LocalityCostFunction)</li></ul><p>对于 cluster 的每一种 region 分布， 采用 6 个因素加权的方式算出一个代价值，这个代价值就用来评估当前 region 分布是否均衡，越均衡则代价值越低。然后通过成千上万次随机迭代来找到一组 RegionMove 的序列，使得最终的代价值严格递减。 得到的这一组 RegionMove 就是 HMaster 最终执行的 region 迁移方案。</p><p>生成 RegionMove 的策略有以下三种：</p><ol><li>随机选择两个 RS, 从每个 RS 中随机选择两个 Region，然后生成一个 Action, 这个 Action 有一半概率做 RegionMove（从 Region 多的 RS 迁移到 Region 少的 RS）, 另一半概率做 RegionSwap(两个 RS 之间做 Region 的交换)。</li><li>选择 Region 最多的 RS 和 Region 最少的 RS，然后生成一个 Action, 这个 Action 一半概率做 RegionMove, 一半概率做 RegionSwap。</li><li>随机找一个 RS，然后找到该 RS 上数据 locality 最差的 Region，再找到 Region 大部分数据落在的 RS，然后生成一个 Action，该 Action 用来把 Region 迁移到它应该所在的 RS，用来提高 locality。</li></ol><h4 id="Rebalance-触发"><a href="#Rebalance-触发" class="headerlink" title="Rebalance 触发"></a>Rebalance 触发</h4><p>HMaster 会有个 BalancerChore 定时类去检查触发，间隔时间：<code>hbase.balancer.period</code> (默认值：300000s=5min) 。当然 HBaseAdmin 或 shell 也提供了命令接口可以手动触发。</p><h4 id="Rebalance-方式"><a href="#Rebalance-方式" class="headerlink" title="Rebalance 方式"></a>Rebalance 方式</h4><p>有两种方式，默认是把当前 RS 的 region 混在一起去 rebalance，若<code>hbase.master.loadbalance.bytable=true</code>，则会按照一个表一个表来 rebalance，这样至少可以确保某些表中途是 rebalance 完的。有人会称后者为表级 rebalance，但实际上这并不是纯粹的表级 rebalance，如果你真的只想触发某个表的 rebalance，可能得考虑自定义策略去过滤其他表的 region 并且将上述 bytable 配置配置为 true。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="http://hbasefly.com/2016/09/08/hbase-rit/">HBase运维实践－聊聊RIT的那点事</a></p><p><a href="http://www.ibigdata.io/?p=175">HBase – Region分裂</a></p><p><a href="https://www.cnblogs.com/lhfcws/p/7825932.html">HBase rebalance 负载均衡源码角度解读使用姿势</a></p><p><a href="https://openinx.github.io/2016/06/21/hbase-balance/">HBase Region Balance实践</a></p>]]></content>
    
    
    <categories>
      
      <category>技术文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase 学习：读取流程</title>
    <link href="/2020/10/19/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B/"/>
    <url>/2020/10/19/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>和写流程相比，HBase 读数据的流程更加复杂。主要基于两个方面的原因：一是因为 HBase 一次范围查询可能会涉及多个 Region、多块缓存甚至多个数据存储文件；二是因为 HBase 中更新操作以及删除操作的实现都很简单，更新操作并没有更新原有数据，而是使用时间戳属性实现了多版本；删除操作也并没有真正删除原有数据，只是插入了一条标记为”deleted”标签的数据，而真正的数据删除发生在系统异步执行 Major Compact 的时候。很显然，这种实现思路大大简化了数据更新、删除流程，但是对于数据读取来说却意味着套上了层层枷锁：读取过程需要根据版本进行过滤，对已经标记删除的数据也要进行过滤。本文系统地将 HBase 读取流程的各个环节串起来进行解读。</p><a id="more"></a><h3 id="读取模式"><a href="#读取模式" class="headerlink" title="读取模式"></a>读取模式</h3><h4 id="Get"><a href="#Get" class="headerlink" title="Get"></a>Get</h4><p>Get 是指基于确切的 RowKey 去获取一行数据，通常被称之为随机点查，这正是 HBase 所擅长的读取模式。一次 Get 操作，包含两个主要步骤：</p><ol><li><p>构建 Get 对象</p><p>基于 RowKey 构建 Get 对象的最简单示例代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] key = Bytes.toBytes(<span class="hljs-string">&quot;66660000431^201803011300&quot;</span>);Get get = <span class="hljs-keyword">new</span> Get(key);</code></pre><p>可以为构建的 Get 对象指定返回的列族：</p><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] family = Bytes.toBytes(<span class="hljs-string">&quot;I&quot;</span>);<span class="hljs-comment">// 指定返回列族I的所有列</span>get.addFamily(family);</code></pre><p>也可以直接指定返回某列族中的指定列：</p><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] family = Bytes.toBytes(<span class="hljs-string">&quot;I&quot;</span>);<span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] qualifierMobile = Bytes.toBytes(<span class="hljs-string">&quot;M&quot;</span>);<span class="hljs-comment">// 指定返回列族I中的列M</span>get.addColumn(family, qualifierMobile);</code></pre></li><li><p>发送 Get 请求并且获取对应的记录</p><p>与写数据类似，发送 Get 请求的接口也是由 Table 提供的，获取到的一行记录，被封装成一个 Result 对象。也可以这么理解一个 Result 对象：</p><ul><li>关联一行数据，一定不可能包含跨行的结果</li><li>包含一个或多个被请求的列。有可能包含这行数据的所有列，也有可能仅包含部分列</li></ul><p>示例代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">try</span> (Table table = conn.getTable(TABLE)) &#123;  <span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] key = Bytes.toBytes(<span class="hljs-string">&quot;66660000431^201803011300&quot;</span>);  Get get = <span class="hljs-keyword">new</span> Get(key);  <span class="hljs-comment">// 记录Table提供的get接口获取一行记录</span>  Result result = table.get(get);  <span class="hljs-comment">// 通过CellScanner来遍历该Result中的所有列</span>  CellScanner scanner = result.cellScanner();  <span class="hljs-keyword">while</span> (scanner.advance()) &#123;     Cell cell = scanner.current();     <span class="hljs-comment">// 读取Cell中的信息...</span>  &#125;    &#125;</code></pre><p>上面给出的是一次随机获取一行记录的例子，但事实上，一次获取多行记录的需求也是普遍存在的，Table 中也定义了 Batch Get 的接口，这样可以在一次网络请求中同时获取多行数据。示例代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">try</span> (Table table = conn.getTable(TN)) &#123;  <span class="hljs-comment">// 基于多个Get对象构建一个List</span>  List&lt;Get&gt; gets = <span class="hljs-keyword">new</span> ArrayList&lt;&gt;(<span class="hljs-number">8</span>);  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;11110000431^201803011300&quot;</span>)));  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;22220000431^201803011300&quot;</span>)));  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;33330000431^201803011300&quot;</span>)));  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;44440000431^201803011300&quot;</span>)));  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;55550000431^201803011300&quot;</span>)));  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;66660000431^201803011300&quot;</span>)));  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;77770000431^201803011300&quot;</span>)));  gets.add(<span class="hljs-keyword">new</span> Get(Bytes.toBytes(<span class="hljs-string">&quot;88880000431^201803011300&quot;</span>)));  <span class="hljs-comment">// 调用Table的Batch Get的接口获取多行记录</span>  Result[] results = table.get(gets);  <span class="hljs-keyword">for</span> (Result result : results) &#123;    CellScanner scanner = result.cellScanner();      <span class="hljs-keyword">while</span> (scanner.advance()) &#123;        Cell cell = scanner.current();          <span class="hljs-comment">// 读取Cell中的信息...</span>      &#125;  &#125;&#125;</code></pre><p>关于 Batch Get 需要补充说明一点信息：获取到的 Result 列表中的结果的顺序，与给定的 RowKey 顺序是一致的。</p></li></ol><h4 id="Scan"><a href="#Scan" class="headerlink" title="Scan"></a>Scan</h4><p>HBase 中的数据表通过划分成一个个的 Region 来实现数据的分片，每一个 Region 关联一个 RowKey 的范围区间，而每一个 Region 中的数据，按 RowKey 的字典顺序进行组织。</p><p>正是基于这种设计，使得 HBase 能够轻松应对这类查询：”指定一个 RowKey 的范围区间，获取该区间的所有记录”， 这类查询在 HBase 被称之为 Scan。</p><p>从技术实现的角度来看，get 请求也是一种 scan 请求（最简单的 scan 请求，scan 的条数为 1）。从这个角度讲，所有读取操作都可以认为是一次 scan 操作。</p><p>HBase Client 端与 Server 端的 scan 操作并没有设计为一次 RPC 请求，这是因为一次大规模的 scan 操作很有可能就是一次全表扫描，扫描结果非常之大，通过一次 RPC 将大量扫描结果返回客户端会带来至少两个非常严重的后果：</p><ul><li>大量数据传输会导致集群网络带宽等系统资源短时间被大量占用，严重影响集群中其他业务。</li><li>客户端很可能因为内存无法缓存这些数据而导致客户端 OOM。</li></ul><p>实际上 HBase 会根据设置条件将一次大的 scan 操作拆分为多个 RPC 请求，每个 RPC 请求称为一次 next 请求，每次只返回规定数量的结果。</p><p>一次 Scan 操作，包括如下几个关键步骤：</p><ol><li><p>构建 Scan</p><p>最简单也最常用的构建 Scan 对象的方法，就是仅仅指定 Scan 的 StartRow 与 StopRow。示例如下：</p><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] startKey = Bytes.toBytes(<span class="hljs-string">&quot;66660000431^20180301&quot;</span>);<span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] stopKey = Bytes.toBytes(<span class="hljs-string">&quot;66660000431^20180501&quot;</span>);Scan scan = <span class="hljs-keyword">new</span> Scan();<span class="hljs-comment">/**</span><span class="hljs-comment">  * 2.0版本之前，设置Scan Key Range的接口为：</span><span class="hljs-comment">  * scan.setStartRow(startKey).setStopRow(stopKey);</span><span class="hljs-comment">  * 但在2.0版本中，该接口已经被标注为Deprecated接口，即已不推荐使用.</span><span class="hljs-comment">  * 下面是推荐的接口：</span><span class="hljs-comment">  */</span>scan.withStartRow(startKey).withStopRow(stopKey);</code></pre><ul><li><p>如果 StartRow 未指定，则本次 Scan 将从表的第一行数据开始读取。</p></li><li><p>如果 StopRow 未指定，而且在不主动停止本次 Scan 操作的前提下，本次 Scan 将会一直读取到表的最后一行记录。</p></li><li><p>如果 StartRow 与 StopRow 都未指定，那本次 Scan 就是一次全表扫描操作。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019142026.png" alt="image-20201019142021329"></p><p>同 Get 类似，Scan 也可以主动指定返回的列族或列：</p><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] family = Bytes.toBytes(<span class="hljs-string">&quot;I&quot;</span>);<span class="hljs-comment">/**</span><span class="hljs-comment">  * 为本次Scan操作指定返回的列族</span><span class="hljs-comment">  * scan.addFamily(family);</span><span class="hljs-comment">  */</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] qualifierMobile = Bytes.toBytes(<span class="hljs-string">&quot;M&quot;</span>);scan.addColumn(family, qualifierMobile);</code></pre></li><li><p>获取 ResultScanner</p><pre><code class="hljs java">ResultScanner scanner = table.getScanner(scan);</code></pre></li><li><p>遍历查询结果</p><pre><code class="hljs java">Result result = <span class="hljs-keyword">null</span>;<span class="hljs-comment">// 通过scanner.next方法获取返回的每一行数据</span><span class="hljs-keyword">while</span> ((result = scanner.next()) != <span class="hljs-keyword">null</span>) &#123;    <span class="hljs-comment">// 读取result中的结果...</span>&#125;</code></pre><p>每执行一次 next()操作，客户端先会从本地缓存中检查是否有数据，如果有就直接返回给用户，如果没有就发起一次 RPC 请求到服务器端获取，获取成功之后缓存到本地。</p></li><li><p>关闭 ResultScanner</p><p>通过下面的方法可以关闭一个 ResultScanner：</p><pre><code class="hljs java">scanner.close();</code></pre><p>如果基于 Java 传统的 try-catch-finally 语法，上述 close 方式需要在 finally 模块显式调用。但如果是是基于 try-with-resource 语法，则由 Java 框架自动调用。</p><p>将上面 1~4 步骤联合起来的示例代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">try</span> (Table table = conn.getTable(TABLE)) &#123;  <span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] start = Bytes.toBytes(<span class="hljs-string">&quot;66660000431^20180301&quot;</span>);  <span class="hljs-keyword">final</span> <span class="hljs-keyword">byte</span>[] stop = Bytes.toBytes(<span class="hljs-string">&quot;66660000431^20180501&quot;</span>);  Scan scan = <span class="hljs-keyword">new</span> Scan();  scan.withStartRow(start).withStopRow(stop);  <span class="hljs-comment">// 基于try-with-resource语法，try语句块结束后</span>  <span class="hljs-comment">// scanner的close方法会自动被调用</span>  <span class="hljs-keyword">try</span> (ResultScanner scanner = table.getScanner(scan)) &#123;    Result result = <span class="hljs-keyword">null</span>;    <span class="hljs-keyword">while</span> ((result = scanner.next()) != <span class="hljs-keyword">null</span>) &#123;      <span class="hljs-comment">// 读取result中的结果...</span>    &#125;  &#125;&#125;</code></pre></li></ol><h4 id="Scan-参数设置"><a href="#Scan-参数设置" class="headerlink" title="Scan 参数设置"></a>Scan 参数设置</h4><ul><li><p>Caching: 设置一次 RPC 请求批量读取的 Results 数量</p><p>下面的示例代码设定了一次读取回来的 Results 数量为 100：</p><pre><code class="hljs java">scan.setCaching(<span class="hljs-number">100</span>);</code></pre><p>Client 每一次往 RegionServer 发送 scan 请求，都会批量拿回一批数据(由 Caching 决定过了每一次拿回的 Results 数量)，然后放到本次的 Result Cache 中：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019143337.png" alt="image-20201019143336198"></p><p>应用每一次读取数据时，都是从本地的 Result Cache 中获取的。如果 Result Cache 中的数据读完了，则 Client 会再次往 RegionServer 发送 scan 请求获取更多的数据。</p></li><li><p>Batch: 设置每一个 Result 中的列的数量</p><p>下面的示例代码设定了每一个 Result 中的列的数量的限制值为 3：</p><pre><code class="hljs java">scan.setBatch(<span class="hljs-number">3</span>);</code></pre><p>该参数适用于一行数据过大的场景，这样，一行数据被请求的列会被拆成多个 Results 返回给 Client。</p></li><li><p>Limit: 限制一次 Scan 操作所获取的行的数量</p><p>同 SQL 语法中的 limit 子句，限制一次 Scan 操作所获取的行的总量：</p><pre><code class="hljs java">scan.setLimit(<span class="hljs-number">10000</span>);</code></pre></li><li><p>CacheBlock: RegionServer 侧是否要缓存本次 Scan 所涉及的 HFileBlocks</p><pre><code class="hljs java">scan.setCacheBlocks(<span class="hljs-keyword">true</span>);</code></pre></li><li><p>Raw Scan: 是否可以读取到删除标识以及被删除但尚未被清理的数据</p><pre><code class="hljs java">scan.setRaw(<span class="hljs-keyword">true</span>);</code></pre></li><li><p>MaxResultSize: 从内存占用量的维度限制一次 Scan 的返回结果集</p><p>下面的示例代码将返回结果集的最大值设置为 5MB：</p><pre><code class="hljs java">scan.setMaxResultSize(<span class="hljs-number">5</span> * <span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>);</code></pre></li><li><p>Reversed Scan: 反向扫描</p><p>普通的 Scan 操作是按照字典顺序从小到大的顺序读取的，而 Reversed Scan 则恰好相反：</p><pre><code class="hljs java">scan.setReversed(<span class="hljs-keyword">true</span>);</code></pre></li><li><p>带 Filter 的 Scan</p><p>Filter 可以在 Scan 的结果集基础之上，对返回的记录设置更多条件值，这些条件可以与 RowKey 有关，可以与列名有关，也可以与列值有关，还可以将多个 Filter 条件组合在一起，等等。</p><p>最常用的 Filter 是 SingleColumnValueFilter，基于它，可以实现如下类似的查询：”返回满足条件{<em>列I:D</em>的值大于等于 10}的所有行”</p><p>示例代码如下：</p><pre><code class="hljs java">Filter filter = <span class="hljs-keyword">new</span> SingleColumnValueFilter(FAMILY,      QUALIFIER, CompareOperator.GREATER_OR_EQUAL,      Bytes.toBytes(<span class="hljs-string">&quot;10&quot;</span>));scan.setFilter(filter);</code></pre><p>Filter 丰富了 HBase 的查询能力，但使用 Filter 之前，需要注意一点：Filter 可能会导致查询响应时延变的不可控制。因为我们无法预测，为了找到一条符合条件的记录，背后需要扫描多少数据量，如果在有效限制了 Scan 范围区间(通过设置 StartRow 与 StopRow 限制)的前提下，该问题能够得到有效的控制。这些信息都要求使用 Filter 之前应该详细调研自己的业务数据模型。</p></li></ul><h3 id="客户端发送请求"><a href="#客户端发送请求" class="headerlink" title="客户端发送请求"></a>客户端发送请求</h3><p>无论是 Get，还是 Scan，Client 在发送请求到 RegionServer 之前，也需要先获取路由信息：</p><ol><li><p>定位该请求所关联的 Region：</p><p>因为 Get 请求仅关联一个 RowKey，所以，直接定位该 RowKey 所关联的 Region 即可。</p><p>对于 Scan 请求，先定位 Scan 的 StartRow 所关联的 Region。</p></li><li><p>往该 Region 所关联的 RegionServer 发送读取请求：</p><p>与写入流程中的数据路由过程类似。如果一次 Scan 涉及到跨 Region 的读取，读完一个 Region 的数据以后，需要继续读取下一个 Region 的数据，这需要在 Client 侧不断记录和刷新 Scan 的进展信息。如果一个 Region 中已无更多的数据，在 scan 请求的响应结果中会带有提示信息，这样可以让 Client 侧切换到下一个 Region 继续读取。</p></li></ol><h3 id="服务端处理请求"><a href="#服务端处理请求" class="headerlink" title="服务端处理请求"></a>服务端处理请求</h3><p>从宏观视角来看，一次 scan 可能会同时扫描一张表的多个 Region，对于这种扫描，客户端会根据 hbase：meta 元数据将扫描的起始区间[startKey，stopKey）进行切分，切分成多个互相独立的查询子区间，每个子区间对应一个 Region。RegionServer 接收到客户端的 get/scan 请求之后做了两件事情：首先构建 scanner iterator 体系；然后执行 next 函数获取 KeyValue，并对其进行条件过滤。</p><h4 id="Scanner-Iterator-体系"><a href="#Scanner-Iterator-体系" class="headerlink" title="Scanner Iterator 体系"></a>Scanner Iterator 体系</h4><p>在 Store/Column Family 内部，KeyValue 可能存在于 MemStore 的 Segment 中，也可能存在于 HFile 文件中，无论是 Segment 还是 HFile，我们统称为 KeyValue 数据源。HBase 使用了各种 Scanner 来抽象每一层/每一类 KeyValue 数据源的 Scan 操作：</p><ul><li>关于一个 Region 的读取，被封装成一个 RegionScanner 对象。</li><li>每一个 Store/Column Family 的读取操作，被封装在一个 StoreScanner 对象中。</li><li>SegmentScanner 与 StoreFileScanner 分别用来描述关于 MemStore 中的 Segment 以及 HFile 的读取操作。</li><li>StoreFileScanner 中关于 HFile 的实际读取操作，由 HFileScanner 完成。</li></ul><p>RegionScanner 的构成如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019151328.png" alt="image-20201019151327035"></p><p>在 StoreScanner 内部，多个 SegmentScanner 与多个 StoreFileScanner 被组织在一个称之为 KeyValueHeap 的对象中：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019151848.png" alt="image-20201019151847674"></p><p>每一个 Scanner 内部有一个指针指向当前要读取的 KeyValue，KeyValueHeap 的核心是一个优先级队列(PriorityQueue)，在这个 PriorityQueue 中，按照每一个 Scanner 当前指针所指向的 KeyValue 进行排序：</p><pre><code class="hljs java"><span class="hljs-comment">// 用来组织所有的Scanner</span><span class="hljs-keyword">protected</span> PriorityQueue&lt;KeyValueScanner&gt; heap = <span class="hljs-keyword">null</span>;<span class="hljs-comment">// PriorityQueue当前排在最前面的Scanner</span><span class="hljs-keyword">protected</span> KeyValueScanner current = <span class="hljs-keyword">null</span>;</code></pre><p>同样的，RegionScanner 中的多个 StoreScanner，也被组织在一个 KeyValueHeap 对象中：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019151934.png" alt="image-20201019151932889"></p><h4 id="KeyValueScanner"><a href="#KeyValueScanner" class="headerlink" title="KeyValueScanner"></a>KeyValueScanner</h4><p>KeyValueScanner 定义了读取 KeyValue 的基础接口：</p><pre><code class="hljs java"><span class="hljs-comment">/**</span><span class="hljs-comment">  * 查看当前Scanner中当前指针位置的KeyValue，该接口不会移动指针.</span><span class="hljs-comment">  */</span><span class="hljs-function">Cell <span class="hljs-title">peek</span><span class="hljs-params">()</span></span>;<span class="hljs-comment">/**</span><span class="hljs-comment">  * 返回Scanner当前指针位置的KeyValue，而后移动指针到下一个KeyValue.</span><span class="hljs-comment">  */</span><span class="hljs-function">Cell <span class="hljs-title">next</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> IOException</span>;<span class="hljs-comment">/**</span><span class="hljs-comment">  * 将当前Scanner的指针定位到指定的KeyValue的位置,如果不存在，则定位到</span><span class="hljs-comment">  * 比该Cell大的下一个KeyValue位置.Seek操作会从当前的HFile Block的开</span><span class="hljs-comment">  * 始位置查找.</span><span class="hljs-comment">  */</span><span class="hljs-function"><span class="hljs-keyword">boolean</span> <span class="hljs-title">seek</span><span class="hljs-params">(Cell key)</span> <span class="hljs-keyword">throws</span> IOException</span>;<span class="hljs-comment">/**</span><span class="hljs-comment">  * 与seek接口类似，也是将当前Scanner的指针定位到指定的KeyValue的位置,</span><span class="hljs-comment">  * 如果不存在，则定位到比该KeyValue大的下一个KeyValue位置.与seek接口不</span><span class="hljs-comment">  * 同点在于，该操作会从上一次读到的HFile Block的位置开始查找.</span><span class="hljs-comment">  */</span><span class="hljs-function"><span class="hljs-keyword">boolean</span> <span class="hljs-title">reseek</span><span class="hljs-params">(Cell key)</span> <span class="hljs-keyword">throws</span> IOException</span>;<span class="hljs-comment">/**</span><span class="hljs-comment">  * 与seek/reseek类似，但不同点在于采用了Lazy Seek机制来降低磁盘IO请求，</span><span class="hljs-comment">  * 其原理在于充分利用Bloom Filter的判断结果，以及待Seek的KeyValue与该</span><span class="hljs-comment">  * Scanner中最大时间戳的对比，减少一些不必要的Seek操作</span><span class="hljs-comment">  */</span><span class="hljs-function"><span class="hljs-keyword">boolean</span> <span class="hljs-title">requestSeek</span><span class="hljs-params">(Cell kv, <span class="hljs-keyword">boolean</span> forward, </span></span><span class="hljs-function"><span class="hljs-params">     <span class="hljs-keyword">boolean</span> useBloom)</span> <span class="hljs-keyword">throws</span> IOException</span>;</code></pre><p>实现了 KeyValueScanner 接口类的主要 Scanner 包括：</p><ul><li>StoreFileScanner</li><li>SegmentScanner</li><li>StoreScanner</li></ul><h4 id="RegionScanner-初始化"><a href="#RegionScanner-初始化" class="headerlink" title="RegionScanner 初始化"></a>RegionScanner 初始化</h4><p>RegionScanner 初始化过程，包括几个关键操作：</p><ol><li><p>获取 ReadPoint</p><p>ReadPoint 决定了此次 Scan 操作能看到哪些数据。Scan 过程中新写入的数据，对此次 Scan 是不可见的。</p></li><li><p>按需选择对应的 Store，并初始化对应的 StoreScanner</p><p>StoreScanner 在初始化的时候，也会按需选择对应的 SegmentScanner 以及 StoreFileScanner，筛选规则包括：</p><ul><li>如果一次 Scan 操作指定了 Time Range，则只选择与该 Time Range 有关的 Scanners。</li><li>对于 Get 操作，可以通过 BloomFilter 过滤掉不符合条件的 Scanners。</li></ul><p>StoreScanner 中筛选除了 Scanner 以后，会将每一个 Scanner seek 到 Scan 的 StartRow 位置：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019154904.png" alt="image-20201019154901937"></p></li></ol><h4 id="RegionScanner-读取流程"><a href="#RegionScanner-读取流程" class="headerlink" title="RegionScanner 读取流程"></a>RegionScanner 读取流程</h4><p>为了简单的解释该流程，我们先假定一个 RegionScanner 中仅包含一个 StoreScanner，那么，这个 RegionScanner 中的核心读取操作，是由 StoreScanner 完成的，我们进一步假定 StoreScanner 由 4 个 Scanners 组成（我们泛化了 SegmentScanner 与 StoreFileScanner 的区别，统称为 Scanner），直观起见，在下图中我们使用了四种不同的颜色：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019155015.png" alt="image-20201019155013730"></p><p>每一个 Scanner 中都有一个 current 指针指向下一个即将要读取的 KeyValue，KeyValueHeap 中的 PriorityQueue 正是按照每一个 Scanner 的 current 所指向的 KeyValue 进行排序。</p><p>第一次 next 请求，将会返回 ScannerA 中的 Row01:FamA:Col1，而后 ScannerA 的指针移动到下一个 KeyValue Row01:FamA:Col2，PriorityQueue 中的 Scanners 排序依然不变：</p><p><img src="/Users/wangrui/Library/Application Support/typora-user-images/image-20201019155040971.png" alt="image-20201019155040971"></p><p>第二次 next 请求，依然返回 ScannerA 中的 Row01:FamA:Col2，ScannerA 的指针移动到下一个 KeyValue Row02:FamA:Col1，此时，PriorityQueue 中的 Scanners 排序发生了变化：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019155122.png" alt="image-20201019155121803"></p><p>下一次 next 请求，将会返回 ScannerB 中的 KeyValue，周而复始，直到某一个 Scanner 所读取的数据耗尽，该 Scanner 将会被 close，不再出现在上面的 PriorityQueue 中。</p><p>SegmentScanner/StoreFileScanner 中返回的 KeyValue，包含了各种类型的 KeyValue：</p><ul><li>已被更新过的旧 KeyValue</li><li>已被标记删除但尚未被及时清理的 KeyValue</li><li>已过期的尚未被及时清理的 KeyValue</li><li>用来描述一次删除操作的 KeyValue(删除还包含了多种类型)</li><li>承载最新用户数据的普通 KeyValue</li></ul><p>因此，在 StoreScanner 层，需要对这些 KeyValue 做更复杂的逻辑校验，这些校验由 ScanQueryMatcher 完成。默认地，可作为返回数据的 KeyValue，应该满足如下条件：</p><ul><li>KeyValue 类型为 Put</li><li>KeyValue 所关联的列为用户 Scan 所涉及的列</li><li>KeyValue 的时间戳符合 Scan 的 TimeRange 要求</li><li>版本最新</li><li>未被标记删除</li><li>通过了 Filter 的过滤条件</li></ul><p>上述条件，只针对一些普通的 Scan，不同的 Scan 参数配置，可能会导致条件集发生变化，如 Scan 启用了 Raw Scan 模式时，Delete 类型的 KeyValue 也会被返回。</p><p>在 Scanner 中，如果允许读取多个版本（由 Scan#readVersions 配置），那正常的读取顺序应该为：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019155408.png" alt="image-20201019155407062"></p><p>上面这种读取的顺序与实际存在的数据的逻辑顺序也是相同的。</p><p>由于不同的 Scan 所读取的每一行中的数据不同，有的限定了列的数量，有的限定了版本的数量，这使得读取时可以通过一些优化，减少不必要的数据扫描。如某次 Scan 在允许读多个版本的同时，限定了只读取 C1~C3，那么，读取顺序应该为：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019182116.png" alt="image-20201019155451197"></p><p>最普通的 Scan，其实只需要读取每一列的最新版本即可，那读取的顺序应该为：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019182112.png" alt="image-20201019155523799"></p><p>我们知道 KeyValueScanner 定义了基础的 seek/reseek/requestSeek 等接口，可以将指针移动到指定 KeyValue 位置。关于指针如何移动的决策信息，由 ScanQueryMatcher 提供的。ScanQueryMatcher 对每一个 KeyValue 的逻辑检查结果称之为 MatchCode，MatchCode 不仅包含了是否应该返回该 KeyValue 的结果，还可能给出了 Scanner 的下一步操作的提示信息。关于它的枚举值，简单举例如下：</p><ul><li><p>INCLUDE_AND_SEEK_NEXT_ROW</p><p>包含当前 KeyValue，并提示 Scanner 当前行已无需继续读取，请 Seek 到下一行。</p></li><li><p>INCLUDE_AND_SEEK_NEXT_COL</p><p>包含当前 KeyValue，并提示 Scanner 当前列已无需继续读取，请 Seek 到下一列。</p></li></ul><p>无论是 StoreScanner 还是 RegionScanner，返回的都是符合条件的 KeyValue 列表。这些 KeyValues 在 RSRpcServices 层被进一步组装成 Results 响应给 Client 侧。</p><h4 id="从HFile中读取待查找Key"><a href="#从HFile中读取待查找Key" class="headerlink" title="从HFile中读取待查找Key"></a>从HFile中读取待查找Key</h4><p>在一个 HFile 文件中 seek 待查找的 Key，该过程可以分解为 4 步操作：</p><ol><li><p>根据 HFile 索引树定位目标 Block</p><p>HRegionServer 打开 HFile 时会将所有 HFile 的 Trailer 部分和 Load-on-open 部分加载到内存，Load-on-open 部分有个非常重要的 Block——Root Index Block，即索引树的根节点。在 Root Index Block 中通过二分查找定位中间节点。因为 Root Index Block 常驻内存，所以这个过程很快。将中间节点索引块加载到内存，然后通过二分查找定位叶子节点。最终访问最后需要访问索引指向的 Data Block 节点。</p><p>上述流程中，Intermediate Index Block、Leaf Index Block 以及 Data Block 都需要加载到内存，所以一次查询的 IO 正常为 3 次。但是实际上 HBase 为 Block 提供了缓存机制，可以将频繁使用的 Block 缓存在内存中，以便进一步加快实际读取过程。</p></li><li><p>BlockCache 中检索目标 Block</p><p>Block 缓存到 BlockCache 之后会构建一个 Map，Map 的 Key 是 BlockKey，Value 是 Block 在内存中的地址。其中 BlockKey 由两部分构成——HFile 名称以及 Block 在 HFile 中的偏移量。BlockKey 很显然是全局唯一的。根据 BlockKey 可以获取该 Block 在 BlockCache 中内存位置，然后直接加载出该 Block 对象。如果在 BlockCache 中没有找到待查 Block，就需要在 HDFS 文件中查找。</p></li><li><p>HDFS 文件中检索目标 Block</p><p>根据文件索引提供的 Block Offset 以及 Block DataSize 这两个元素可以在 HDFS 上读取到对应的 Data Block 内容（核心代码可以参见 HFileBlock.java 中内部类 FSReaderImpl 的 readBlockData 方法）。这个阶段 HBase 会下发命令给 HDFS，HDFS 执行真正的 Data Block 查找工作。</p></li><li><p>从 Block 中读取待查找 KeyValue</p><p>HFile Block 由 KeyValue（由小到大依次存储）构成，但这些 KeyValue 并不是固定长度的，只能遍历扫描查找。</p></li></ol><p>如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201019170115.png" alt="image-20201019170114050"></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="http://www.nosqlnotes.com/technotes/hbase/hbase-read/">一条数据的HBase之旅，简明HBase入门教程-Read全流程</a></p>]]></content>
    
    
    <categories>
      
      <category>技术文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase 学习：Flush 和 Compaction</title>
    <link href="/2020/10/16/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9AFlush%20%E5%92%8C%20Compaction/"/>
    <url>/2020/10/16/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9AFlush%20%E5%92%8C%20Compaction/</url>
    
    <content type="html"><![CDATA[<p>当数据已经被写入 WAL 与 MemStore，就可以说数据已经被成功写到 HBase 中了。随着数据的不断写入，MemStore 中存储的数据会越来越多，系统会将 MemStore 中的数据进行 Flush 操作写入文件形成 HFile。而随着 Flush 产生的 HFile 文件越来越多，系统还会对 HFile 文件进行 Compaction 操作。本文主要介绍数据写入后 Flush &amp; Compaction 的流程和策略。</p><a id="more"></a><h3 id="FLush-amp-Compaction-概述"><a href="#FLush-amp-Compaction-概述" class="headerlink" title="FLush &amp; Compaction 概述"></a>FLush &amp; Compaction 概述</h3><p>MemStore 中的数据，达到一定的阈值，会被 Flush 成 HDFS 中的 HFile 文件。但随着 Flush 次数的不断增多，HFile 的文件数量也会不断增多。从多个 HFile 文件中读取记录，将导致更多的 IOPS，这会使得读取时延不断增大。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201016112247.png" alt="image-20201016111939976"></p><p>Compaction 可以将一些 HFile 文件合并成较大的 HFile 文件，也可以把所有的 HFile 文件合并成一个大的 HFile 文件，这个过程可以理解为：将多个 HFile 的“交错无序状态”，变成单个 HFile 的“有序状态”，降低读取时延。</p><p>小范围的 HFile 文件合并，称之为 Minor Compaction，一个列族中将所有的 HFile 文件合并，称之为 Major Compaction。除了文件合并范围的不同之外，Major Compaction 还会清理一些 TTL 过期/版本过旧以及被标记删除的数据。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201016112417.png" alt="image-20201016112314778"></p><h3 id="Flush"><a href="#Flush" class="headerlink" title="Flush"></a>Flush</h3><h4 id="触发条件"><a href="#触发条件" class="headerlink" title="触发条件"></a>触发条件</h4><p>HBase 会在以下几种情况下触发 flush 操作。需要注意的是最小的 flush 单元是 HRegion 而不是 MemStore。如果一个 HRegion 中 Memstore 过多，每次 flush 的开销必然会很大，因此在进行表设计的时应该候尽量减少 ColumnFamily 的个数。</p><ol><li>Memstore 级别限制：当 Region 中任意一个 MemStore 的大小达到了上限（<code>hbase.hregion.memstore.flush.size</code>，默认 128MB），会触发 Memstore 刷新。</li><li>Region 级别限制：当 Region 中所有 Memstore 的大小总和达到了上限（<code>hbase.hregion.memstore.block.multiplier</code> <em> <code>hbase.hregion.memstore.flush.size</code>，默认 2 </em> 128M = 256M），会触发 memstore 刷新。</li><li>Region Server 级别限制：当一个 Region Server 中所有 Memstore 的大小总和达到了上限（<code>hbase.regionserver.global.memstore.upperLimit</code> * <code>hbase_heapsize</code>，默认 40%的 JVM 内存使用量），会触发部分 Memstore 刷新。Flush 顺序是按照 Memstore 由大到小执行，先 Flush Memstore 最大的 Region，再执行次大的，直至总体 Memstore 内存使用量低于阈值（<code>hbase.regionserver.global.memstore.lowerLimit</code> *  <code>hbase_heapsize</code>，默认 38%的 JVM 内存使用量）。</li><li>当一个 Region Server 中 HLog 数量达到上限（可通过参数<code>hbase.regionserver.maxlogs</code>配置）时，系统会选取最早的一个 HLog 对应的一个或多个 Region 进行 flush</li><li>定期刷新 Memstore：默认周期为 1 小时，确保 Memstore 不会长时间没有持久化。为避免所有的 MemStore 在同一时间都进行 flush 导致的问题，定期的 flush 操作有 20000 左右的随机延时。</li><li>手动执行 flush：用户可以通过 shell 命令 flush ‘tablename’或者 flush ‘region name’分别对一个表或者一个 Region 进行 flush。</li></ol><h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>在 HBase 1.1 之前，MemStore 刷写是 Region 级别的。就是说，如果要刷写某个 MemStore ，MemStore 所在的 Region 中其他 MemStore 也是会被一起刷写的。针对这个问题，HBase 引入列族级别的刷写。我们可以通过 <code>hbase.regionserver.flush.policy</code> 参数选择不同的刷写策略。</p><p>HBase 2.0 的刷写策略全部都是实现 <code>FlushPolicy</code> 抽象类的。并且自带三种刷写策略：<code>FlushAllLargeStoresPolicy</code>、<code>FlushNonSloppyStoresFirstPolicy</code> 以及 <code>FlushAllStoresPolicy</code>。</p><h5 id="FlushAllStoresPolicy"><a href="#FlushAllStoresPolicy" class="headerlink" title="FlushAllStoresPolicy"></a>FlushAllStoresPolicy</h5><p>这种刷写策略实现最简单，直接返回当前 Region 对应的所有 MemStore。也就是每次刷写都是对 Region 里面所有的 MemStore 进行的，这个行为和 HBase 1.1 之前是一样的。</p><h5 id="FlushAllLargeStoresPolicy"><a href="#FlushAllLargeStoresPolicy" class="headerlink" title="FlushAllLargeStoresPolicy"></a>FlushAllLargeStoresPolicy</h5><p>在 HBase 2.0 之前版本是 <code>FlushLargeStoresPolicy</code>，后面被拆分成分 <code>FlushAllLargeStoresPolicy</code> 和<code>FlushNonSloppyStoresFirstPolicy</code>。<code>FlushAllLargeStoresPolicy</code> 是 2.0 版本中的默认策略。</p><p>这种策略会先判断 Region 中每个 MemStore 的使用内存（ＯnHeap +　OffHeap）是否大于某个阀值，大于这个阀值的 MemStore 将会被刷写。阀值的计算是由 <code>hbase.hregion.percolumnfamilyflush.size.lower.bound</code> 、<code>hbase.hregion.percolumnfamilyflush.size.lower.bound.min</code> 以及 <code>hbase.hregion.memstore.flush.size</code> 参数决定的。计算逻辑如下：</p><ul><li><p>如果设置了 <code>hbase.hregion.percolumnfamilyflush.size.lower.bound</code>，<code>flushSizeLowerBound = hbase.hregion.percolumnfamilyflush.size.lower.bound</code></p></li><li><p>否则<code>flushSizeLowerBound = max(region.getMemStoreFlushSize() / familyNumber, hbase.hregion.percolumnfamilyflush.size.lower.bound.min)</code></p></li></ul><p><code>hbase.hregion.percolumnfamilyflush.size.lower.bound.min</code> 默认值为 16MB，而 <code>hbase.hregion.percolumnfamilyflush.size.lower.bound</code> 没有默认值。</p><p>比如当前表有 3 个列族，其他用默认的值，那么 <code>flushSizeLowerBound = max((long)128 / 3, 16) = 42</code>。</p><p>如果当前 Region 中没有 MemStore 的使用内存大于上面的阀值，<code>FlushAllLargeStoresPolicy</code> 策略就退化成 <code>FlushAllStoresPolicy</code> 策略了，也就是会对 Region 里面所有的 MemStore 进行 Flush。</p><h5 id="FlushNonSloppyStoresFirstPolicy"><a href="#FlushNonSloppyStoresFirstPolicy" class="headerlink" title="FlushNonSloppyStoresFirstPolicy"></a>FlushNonSloppyStoresFirstPolicy</h5><p>HBase 2.0 引入了 in-memory compaction。如果我们对相关列族 <code>hbase.hregion.compacting.memstore.type</code> 参数的值不是 <code>NONE</code>，也就是启用 in-memory compaction 时，那么这个 MemStore 的 <code>isSloppyMemStore</code> 值就是 true，否则就是 false。</p><p><code>FlushNonSloppyStoresFirstPolicy</code> 策略将 Region 中的 MemStore 按照 <code>isSloppyMemStore</code> 分到两个 HashSet 里面（<code>sloppyStores</code> 和 <code>regularStores</code>）。然后</p><ul><li>判断 <code>regularStores</code> 里面是否有 MemStore 内存占用大于相关阀值的 MemStore ，有的话就会对这些 MemStore 进行刷写，其他的不做处理，这个阀值计算和 <code>FlushAllLargeStoresPolicy</code> 的阀值计算逻辑一致。</li><li>如果 <code>regularStores</code> 里面没有 MemStore 内存占用大于相关阀值的 MemStore，这时候就开始在 <code>sloppyStores</code> 里面寻找是否有 MemStore 内存占用大于相关阀值的 MemStore，有的话就会对这些 MemStore 进行刷写，其他的不做处理。</li><li>如果上面 <code>sloppyStores</code> 和 <code>regularStores</code> 都没有满足条件的 MemStore 需要刷写，这时候就 <code>FlushNonSloppyStoresFirstPolicy</code> 策略久退化成 <code>FlushAllStoresPolicy</code> 策略了。</li></ul><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><p>为了减少 flush 过程对读写的影响，HBase 采用了类似于两阶段提交的方式，将整个 flush 过程分为三个阶段：</p><ol><li>prepare 阶段：遍历当前 Region 中的所有 Memstore，将 Memstore 中当前数据集 kvset 做一个快照 snapshot，然后再新建一个新的 kvset。后期的所有写入操作都会写入新的 kvset 中，而整个 flush 阶段读操作会首先分别遍历 kvset 和 snapshot，如果查找不到再会到 HFile 中查找。prepare 阶段需要加一把 updateLock 对写请求阻塞，结束之后会释放该锁。因为此阶段没有任何费时操作，因此持锁时间很短。</li><li>flush 阶段：遍历所有 Memstore，将 prepare 阶段生成的 snapshot 持久化为临时文件，临时文件会统一放到目录.tmp 下。这个过程因为涉及到磁盘 IO 操作，因此相对比较耗时。</li><li>commit 阶段：遍历所有的 Memstore，将 flush 阶段生成的临时文件移到指定的 ColumnFamily 目录下，针对 HFile 生成对应的 storefile 和 Reader，把 storefile 添加到 HStore 的 storefiles 列表中，最后再清空 prepare 阶段生成的 snapshot。</li></ol><h4 id="对业务读写的影响"><a href="#对业务读写的影响" class="headerlink" title="对业务读写的影响"></a>对业务读写的影响</h4><p>正常情况下，大部分 Memstore Flush 操作都不会对业务读写产生太大影响，比如这几种场景：HBase 定期刷新 Memstore、手动执行 flush 操作、触发 Memstore 级别限制、触发 HLog 数量限制以及触发 Region 级别限制等，这几种场景只会阻塞对应 Region 上的写请求，阻塞时间很短，毫秒级别。</p><p>然而一旦触发 Region Server 级别限制导致 flush，就会对用户请求产生较大的影响。会阻塞所有落在该 Region Server 上的更新操作，阻塞时间很长，甚至可以达到分钟级别。</p><h3 id="Compaction"><a href="#Compaction" class="headerlink" title="Compaction"></a>Compaction</h3><h4 id="作用-amp-副作用"><a href="#作用-amp-副作用" class="headerlink" title="作用 &amp; 副作用"></a>作用 &amp; 副作用</h4><p>Compaction 主要有以下几个核心作用：</p><ul><li>合并小文件，减少文件数，稳定随机读延迟。</li><li>提高数据的本地化率。本地化率越高，在 HDFS 上访问数据时延迟就越小；相反，本地化率越低，访问数据就可能大概率需要通过网络访问，延迟必然会比较大。</li><li>清除无效数据，减少数据存储量。</li></ul><p>同时 Compaction 也会带来以下几个副作用：</p><ul><li><p>读延迟毛刺：</p><p>Compaction 操作重写文件会带来很大的带宽压力以及短时间 IO 压力。要将小文件的数据读出来需要 IO，很多小文件数据跨网络传输需要带宽，读出来之后又要写成一个大文件，因为是三副本写入，必然需要网络开销和写入 IO 开销。在稳定数据读取延迟的同时，也会产生读取延迟毛刺。</p></li><li><p>写阻塞：</p><p>Compaction 对写入也会有很大的影响。当写请求非常多，导致不断生成 HFile，但 compact 的速度远远跟不上 HFile 生成的速度时。这样就会使 HFile 的数量会越来越多，导致读性能急剧下降。为了避免这种情况，在 HFile 的数量过多的时候会限制写请求的速度：在每次执行 MemStore flush 的操作前，如果 HStore 的 HFile 数超过<code>hbase.hstore.blockingStoreFiles</code> （默认 7），则会阻塞 flush 操作<code>hbase.hstore.blockingWaitTime</code>时间，在这段时间内，如果 compact 操作使得 HStore 文件数下降到回这个值，则停止阻塞。另外阻塞超过时间后，也会恢复执行 flush 操作。这样做就可以有效地控制大量写请求的速度，但同时这也是影响写请求速度的主要原因之一。</p></li></ul><p>可见，Compaction 会使得数据读取延迟一直比较平稳，但付出的代价是大量的读延迟毛刺和一定的写阻塞。</p><h4 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h4><ol><li><p>触发 Compaction 后，HBase 会将该 Compaction 交由一个独立的线程处理。</p></li><li><p>该线程首先会从对应 store 中选择合适的 hfile 文件进行合并，这一步是整个 Compaction 的核心，选取文件需要遵循很多条件。实际实现中，HBase 提供了多个文件选取算法：RatioBasedCompactionPolicy、ExploringCompactionPolicy 和 StripeCompactionPolicy 等。用户也可以通过特定接口实现自己的 Compaction 算法。</p></li><li>选出待合并的文件后，HBase 会根据这些 hfile 文件总大小挑选对应的线程池处理。</li><li>最后对这些文件执行具体的合并操作。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201016141441.png" alt="image-20201016141440053"></p><h4 id="触发条件-1"><a href="#触发条件-1" class="headerlink" title="触发条件"></a>触发条件</h4><p>最常见的触发 compaction 的因素有三种：Memstore Flush、后台线程周期性检查、手动触发。</p><ol><li><p>Memstore Flush: 应该说 compaction 操作的源头就来自 flush 操作，memstore flush 会产生 HFile 文件，文件越来越多就需要 compact。因此在每次执行完 Flush 操作之后，都会对当前 Store 中的文件数进行判断，都会对当前 Store 中的文件数进行判断，一旦 Store 中总文件数大于<code>hbase.hstore.compactionThreshold</code>，就会触发 compaction。需要说明的是，compaction 都是以 Store 为单位进行的，而在 Flush 触发条件下，整个 Region 的所有 Store 都会执行 compact，所以会在短时间内执行多次 compaction。</p></li><li><p>后台线程周期性检查：后台线程 CompactionChecker 定期触发检查是否需要执行 compaction，检查周期为：<code>hbase.server.thread.wakefrequency</code> * <code>hbase.server.compactchecker.interval.multiplier</code>。和 flush 不同的是，该线程优先检查 Store 中总文件数是否大于阈值<code>hbase.hstore.compactionThreshold</code>，一旦大于就会触发 compaction。如果不满足，它会接着检查是否满足 major compaction 条件。简单来说，如果当前 store 中 hfile 的最早更新时间早于某个值 mcTime，就会触发 major compaction，HBase 预想通过这种机制定期删除过期数据。上文 mcTime 是一个浮动值，浮动区间默认为［7 - 7 * 0.2，7 + 7 * 0.2］，其中 7 为<code>hbase.hregion.majorcompaction</code>，0.2 为<code>hbase.hregion.majorcompaction.jitter</code>，可见默认在 7 天左右就会执行一次 major compaction。用户如果想禁用 major compaction，只需要将参数<code>hbase.hregion.majorcompaction</code>设为 0。</p></li><li><p>手动触发：一般来讲，手动触发 compaction 通常是为了执行 major compaction，原因有三，其一是因为很多业务担心自动 major compaction 影响读写性能，因此会选择低峰期手动触发；其二也有可能是用户在执行完 alter 操作之后希望立刻生效，执行手动触发 major compaction；其三是 HBase 管理员发现硬盘容量不够的情况下手动触发 major compaction 删除大量过期数据；无论哪种触发动机，一旦手动触发，HBase 会不做很多自动化检查，直接执行合并。</p></li></ol><h4 id="HFile-选择"><a href="#HFile-选择" class="headerlink" title="HFile 选择"></a>HFile 选择</h4><p>选择合适的文件进行合并是整个 compaction 的核心，因为合并文件的大小以及其当前承载的 IO 数直接决定了 compaction 的效果。</p><p>最理想的情况是，这些文件承载了大量 IO 请求但是大小很小，这样 compaction 本身不会消耗太多 IO，而且合并完成之后对读的性能会有显著提升。</p><p>无论什么策略，选择时都会首先对该 Store 中所有 HFile 逐一进行排查，排除不满足条件的部分文件：</p><ol><li><p>排除当前正在执行 compact 的文件及其比这些文件更新的所有文件（SequenceId 更大）</p></li><li><p>排除某些过大的单个文件，如果文件大小大于 hbase.hzstore.compaction.max.size（默认 Long 最大值），则被排除，否则会产生大量 IO 消耗。经过排除的文件称为候选文件，HBase 接下来会再判断是否满足 major compaction 条件，如果满足，就会选择全部文件进行合并。判断条件有下面三条，只要满足其中一条就会执行 major compaction：</p><ul><li><p>用户强制执行 major compaction</p></li><li><p>长时间没有进行 compact（CompactionChecker 的判断条件 2）且候选文件数小于 hbase.hstore.compaction.max（默认 10）</p></li><li><p>Store 中含有 Reference 文件，Reference 文件是 split region 产生的临时文件，只是简单的引用文件，一般必须在 compact 过程中删除</p></li></ul></li></ol><p>如果不满足 major compaction 条件，那么就是执行 minor compaction。HBase 提供了了两种最基本的策略，RatioBasedCompactionPolicy 和 ExploringCompactionPolicy，后者继承自前者，也是当前版本的默认策略。</p><h5 id="RatioBasedCompactionPolicy"><a href="#RatioBasedCompactionPolicy" class="headerlink" title="RatioBasedCompactionPolicy"></a>RatioBasedCompactionPolicy</h5><p>从老到新逐一扫描所有候选文件，满足其中条件之一便停止扫描：</p><ol><li>当前文件大小 &lt; 比它更新的所有文件大小总和 * ratio，其中 ratio 是一个可变的比例，在高峰期时 ratio 为 1.2，非高峰期为 5，也就是非高峰期允许 compact 更大的文件。用户可以配置参数<code>hbase.offpeak.start.hour</code>和<code>hbase.offpeak.end.hour</code>来设置高峰期</li><li>当前所剩候选文件数 &lt;= <code>hbase.store.compaction.min</code>（默认为 3）</li></ol><p>停止扫描后，待合并文件就选择出来了，即为当前扫描文件+比它更新的所有文件</p><h5 id="ExploringCompactionPolicy"><a href="#ExploringCompactionPolicy" class="headerlink" title="ExploringCompactionPolicy"></a>ExploringCompactionPolicy</h5><p>该策略思路基本和 RatioBasedCompactionPolicy 相同，不同的是，Ratio 策略在找到一个合适的文件集合之后就停止扫描了，而 Exploring 策略会记录下所有合适的文件组合，并在这些文件组合中寻找最优解。优先选择文件组合文件数多的，当文件数一样多时选择文件数小的，此目的是为了尽可能合并更多的文件并且产生的 IO 越少越好。</p><h4 id="挑选线程池"><a href="#挑选线程池" class="headerlink" title="挑选线程池"></a>挑选线程池</h4><p>HBase 实现中有一个专门的线程 CompactSplitThead 负责接收 compact 请求以及 split 请求，而且为了能够独立处理这些请求，这个线程内部构造了多个线程池：largeCompactions、smallCompactions 以及 splits 等，其中 splits 线程池负责处理所有的 split 请求，largeCompactions 和 smallCompaction 负责处理所有的 compaction 请求，区别在于文件总大小。</p><ol><li><p>上述设计目的是为了能够将请求独立处理，提供系统的处理性能。</p></li><li><p>分配原则：待 compact 的文件总大小如果大于值 throttlePoint（可以通过参数<code>hbase.regionserver.thread.compaction.throttle</code>配置，默认为 2.5G），分配给 largeCompactions 处理，否则分配给 smallCompactions 处理。</p></li><li><p>largeCompactions 和 smallCompactions 默认都只有一个线程，用户可以通过参数<code>hbase.regionserver.thread.compaction.large</code>和<code>hbase.regionserver.thread.compaction.small</code>进行配置</p></li></ol><h4 id="执行HFile文件合并"><a href="#执行HFile文件合并" class="headerlink" title="执行HFile文件合并"></a>执行HFile文件合并</h4><p>合并流程主要分为如下几步：</p><ol><li><p>分别读出待合并 hfile 文件的 KV，并顺序写到位于./tmp 目录下的临时文件中</p></li><li><p>将临时文件移动到对应 region 的数据目录</p></li><li><p>将 compaction 的输入文件路径和输出文件路径封装为 KV 写入 WAL 日志，并打上 compaction 标记，最后强制执行 sync</p></li><li><p>将对应 region 数据目录下的 compaction 输入文件全部删除</p></li></ol><p>上述四个步骤看起来简单，但实际是很严谨的，具有很强的容错性和完美的幂等性：</p><ol><li><p>如果 RS 在步骤 2 之前发生异常，本次 compaction 会被认为失败，如果继续进行同样的 compaction，上次异常对接下来的 compaction 不会有任何影响，也不会对读写有任何影响。唯一的影响就是多了一份多余的数据。</p></li><li><p>如果 RS 在步骤 2 之后、步骤 3 之前发生异常，同样的，仅仅会多一份冗余数据。</p></li><li><p>如果在步骤 3 之后、步骤 4 之前发生异常，RS 在重新打开 region 之后首先会从 WAL 中看到标有 compaction 的日志，因为此时输入文件和输出文件已经持久化到 HDFS，因此只需要根据 WAL 移除掉 compaction 输入文件即可</p></li></ol><h4 id="高级策略"><a href="#高级策略" class="headerlink" title="高级策略"></a>高级策略</h4><p>compaction 的策略，一方面需要保证 compaction 的基本效果，另一方面又不会带来严重的 IO 压力。然而，并没有一种设计策略能够适用于所有应用场景或所有数据集。</p><p>HBase 从 0.96 版本开始对架构进行了一定的调整：</p><ul><li>提供了 Compaction 插件接口，用户只需要实现这些特定的接口，就可以根据自己的应用场景以及数据集定制特定的 compaction 策略。</li><li>支持 table/cf 粒度的策略设置，使得用户可以根据应用场景为不同表/列族选择不同的 compaction 策略。</li></ul><p>HBase 也逐步新增了一些其他 Compaction 策略，策略有一些共同的优化方向，总结如下：</p><ol><li><p>减少参与 compaction 的文件数：首先需要将文件根据 rowkey、version 或其他属性进行分割，再根据这些属性挑选部分重要的文件参与合并；另一方面，尽量不要合并那些大文件，减少参与合并的文件数。</p></li><li><p>不要合并那些不需要合并的文件：比如 OpenTSDB 应用场景下的老数据，这些数据基本不会查询到，因此不进行合并也不会影响查询性能。</p></li><li><p>更小的 region：更小的 region 只会生成少量文件，这些文件合并不会引起很大的 IO 放大。</p></li></ol><h5 id="FIFO-Compaction"><a href="#FIFO-Compaction" class="headerlink" title="FIFO Compaction"></a>FIFO Compaction</h5><p>FIFO Compaction 策略主要参考了<a href="https://github.com/facebook/rocksdb/wiki/FIFO-compaction-style">rocksdb的实现</a>，它会选择那些过期的数据文件，即该文件内所有数据都已经过期。因此，对应业务的列族必须设置 TTL，否则肯定不适合该策略。需要注意的是，该策略只做这么一件事情：收集所有已经过期的文件并删除。这样的应用场景主要包括：</p><ol><li><p>大量短时间存储的原始数据，比如推荐业务，上层业务只需要最近时间内用户的行为特征，利用这些行为特征进行聚合为用户进行推荐。再比如 Nginx 日志，用户只需要存储最近几天的日志，方便查询某个用户最近一段时间的操作行为等等</p></li><li><p>所有数据能够全部加载到 block cache（RAM/SSD），假如 HBase 有 1T 大小的 SSD 作为 block cache，理论上就完全不需要做合并，因为所有读操作都是内存操作。</p></li></ol><p>因为 FIFO Compaction 只是收集所有过期的数据文件并删除，并没有真正执行重写（几个小文件合并成大文件），因此不会消耗任何 CPU 和 IO 资源，也不会从 block cache 中淘汰任何热点数据。所以，无论对于读还是写，该策略都会提升吞吐量、降低延迟。</p><h5 id="Tier-Based-Compaction"><a href="#Tier-Based-Compaction" class="headerlink" title="Tier-Based Compaction"></a>Tier-Based Compaction</h5><p>现实业务中，有很大比例的业务都存在明显的热点数据，而其中最常见的情况是：最近写入到的数据总是最有可能被访问到，而老数据被访问到的频率就相对比较低。按照之前的文件选择策略，并没有对新文件和老文件进行一定的区别对待，每次 compaction 都有可能会有很多老文件参与合并，这必然会影响 compaction 效率，却对降低读延迟没有太大的帮助。</p><p>针对这种情况，HBase 社区借鉴 Facebook HBase 分支的解决方案，引入了 Tier-Based Compaction。这种方案会根据候选文件的新老程度将其分为多个不同的等级，每个等级都有对应等级的参数，比如参数 Compation Ratio，表示该等级文件选择时的选择几率，Ratio 越大，该等级的文件越有可能被选中参与 Compaction。而等级数、每个等级参数都可以通过 CF 属性在线更新。</p><p>该方案的具体实现思路，HBase 更多地参考了 Cassandra 的实现方案：基于时间窗的时间概念。如下图所示，时间窗的大小可以进行配置，其中参数 base_time_seconds 代表初始化时间窗的大小，默认为 1h，表示最近一小时内 flush 的文件数据都会落入这个时间窗内，所有想读到最近一小时数据请求只需要读取这个时间窗内的文件即可。后面的时间窗窗口会越来越大，另一个参数 max_age_days 表示比其更老的文件不会参与 compaction。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201016162743.png" alt="image-20201016162742561"></p><p>上图所示，时间窗随着时间推移朝右移动，图一中没有任何时间窗包含 4 个（可以通过参数 min_thresold 配置）文件，因此 compaction 不会被触发。随着时间推移来到图二所示状态，此时就有一个时间窗包含了 4 个 HFile 文件，compaction 就会被触发，这四个文件就会被合并为一个大文件。</p><p>对比上文说到的分级策略以及 Compaction Ratio 参数，Cassandra 的实现方案中通过设置多个时间窗来实现分级，时间窗的窗口大小类似于 Compaction Ratio 参数的作用，可以通过调整时间窗的大小来调整不同时间窗文件选择的优先级，比如可以将最右边的时间窗窗口调大，那新文件被选择参与 Compaction 的概率就会大大增加。然而，这个方案里面并没有类似于当前 HBase 中的 Major Compaction 策略来实现过期文件清理的功能，只能借助于 TTL 来主动清理过期的文件，比如这个文件中所有数据都过期了，就可以将这个文件清理掉。</p><p>因此，我们可以总结得到使用 Date Tierd Compaction Policy 需要遵守的原则：</p><ol><li>特别适合使用的场景：时间序列数据，默认使用 TTL 删除。类似于“获取最近一小时／三小时／一天”场景，同时不会执行 delete 操作。最典型的例子就是基于 Open-TSDB 的监控系统。</li><li>比较适合的应用场景：时间序列数据，但是会有全局数据的更新操作以及少部分的删除操作。</li><li>不适合的应用场景：非时间序列数据，或者大量的更新数据更新操作和删除操作。</li></ol><h5 id="Stripe-Compaction"><a href="#Stripe-Compaction" class="headerlink" title="Stripe Compaction"></a>Stripe Compaction</h5><p>通常情况下，major compaction 都是无法绕过的，很多业务都会执行 delete/update 操作，并设置 TTL 和 Version，这样就需要通过执行 major compaction 清理被删除的数据以及过期版本数据、过期 TTL 数据。然而，major compaction 是一个特别昂贵的操作，会消耗大量系统资源，而且执行一次可能会持续几个小时，严重影响业务应用。因此，一般线上都会选择关闭 major compaction 自动触发，而是选择在业务低峰期的时候手动触发。为了彻底消除 major compaction 所带来的影响，hbase 社区提出了 strip compaction 方案。</p><p>解决 major compaction 的最直接办法是减少 region 的大小，最好整个集群都是由很多小 region 组成，这样参与 compaction 的文件总大小就必然不会太大。可是，region 设置小会导致 region 数量很多，这一方面会导致 hbase 管理 region 的开销很大，另一方面，region 过多也要求 hbase 能够分配出来更多的内存作为 memstore 使用，否则有可能导致整个 regionserver 级别的 flush，进而引起长时间的写阻塞。因此单纯地通过将 region 大小设置过小并不能本质解决问题。</p><p>stripe compaction 会将整个 store 中的文件按照 Key 划分为多个 Range，在这里称为 stripe，stripe 的数量可以通过参数设定，相邻的 stripe 之间 key 不会重合。实际上在概念上来看这个 stripe 类似于 sub-region 的概念，即将一个大 region 切分成了很多小的 sub-region。</p><p>随着数据写入，memstore 执行 flush 之后形成 hfile，这些 hfile 并不会马上写入对应的 stripe，而是放到一个称为 L0 的地方，用户可以配置 L0 可以放置 hfile 的数量。一旦 L0 放置的文件数超过设定值，系统就会将这些 hfile 写入对应的 stripe：首先读出 hfile 的 KVs，再根据 KV 的 key 定位到具体的 stripe，将该 KV 插入对应 stripe 的文件中即可，如下图所示。之前说过 stripe 就是一个个小的 region，所以在 stripe 内部，依然会像正常 region 一样执行 minor compaction 和 major compaction，可以预想到，stripe 内部的 major compaction 并不会太多消耗系统资源。另外，数据读取也很简单，系统可以根据对应的 Key 查找到对应的 stripe，然后在 stripe 内部执行查找，因为 stripe 内数据量相对很小，所以也会一定程度上提升数据查找性能。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201016163431.png" alt="image-20201016163300597"></p><p>和任何一种 compaction 机制一样，stripe compaction 也有它特别擅长的业务场景，也有它并不擅长的业务场景。下面是两种 stripe compaction 比较擅长的业务场景：</p><ol><li>大 Region：小 region 没有必要切分为 stripes，一旦切分，反而会带来额外的管理开销。一般默认如果 region 大小小于 2G，就不适合使用 stripe compaction。</li><li>RowKey 具有统一格式：stripe compaction 要求所有数据按照 Key 进行切分，切分为多个 stripe。如果 rowkey 不具有统一格式的话，无法进行切分。</li></ol><h5 id="MOB-Compaction"><a href="#MOB-Compaction" class="headerlink" title="MOB Compaction"></a>MOB Compaction</h5><p>有的场景下，需要使用 HBase 来存储 MB 级别的 Blob(如图片之类的小文件)数据，如果像普通的结构化数据/半结构化数据一样，直接写到 HBase 中，数 MB 级别的 Blob 数据，被反复多次合并以后，会严重抢占 IO 资源。</p><p>因此，HBase 的 MOB 特性的设计思想为：将 Blob 数据与描述 Blob 的元数据分离存储，Blob 元数据采用正常的 HBase 的数据存储方式，而 Blob 数据存储在额外的 MOB 文件中，但在 Blob 元数据行中，存储了这个 MOB 文件的路径信息。MOB 文件本质还是一个 HFile 文件，但这种 HFile 文件不参与 HBase 正常的 Compaction 流程。仅仅合并 Blob 元数据信息，写 IO 放大的问题就得到了有效的缓解。</p><p>MOB Compaction 也主要是针对 MOB 特性而存在的，这里涉及到数据在 MOB 文件与普通的 HFile 文件之间的一些流动，尤其是 MOB 的阈值大小发生变更的时候(即当一个列超过预设的配置值时，才被认定为 MOB)。</p><p>在每月压缩策略的情况下，基于 MOB 配置的阈值，当前日历周中的文件按照天进行压缩，本月的之前日历周中文件是按照周进行压缩，过去几个月的文件基于月进行压缩。通过这种设计，MOB 文件在月策略的情况下最多只会压缩 3 次；在周策略的情况下最多只会压缩 2 次。</p><p>默认情况下，MOB 压缩分区策略为天级别。按天压缩可能导致产生过多的文件，超出文件数量限制。因此可以使用每周或每月的策略。我们需要使用 MOB 列族的<code>MOB_COMPACT_PARTITION_POLICY</code> 属性。用户可以在 HBase shell 创建表时设置此属性。</p><p>如果压缩策略从每天更改为每周或每月，或每周更改为每月，则下一个 MOB 压缩将重新压缩之前策略压缩过的 MOB 文件。如果策略从每月或每周更改为每天，或者每月更改为每周，则已经压缩过的 MOB 文件在新的压缩策略将不再被压缩。</p><h4 id="吞吐量限制和带宽占用"><a href="#吞吐量限制和带宽占用" class="headerlink" title="吞吐量限制和带宽占用"></a>吞吐量限制和带宽占用</h4><h5 id="Limit-Compaction-Speed"><a href="#Limit-Compaction-Speed" class="headerlink" title="Limit Compaction Speed"></a>Limit Compaction Speed</h5><p>上述几种策略都是根据不同的业务场景设置对应的文件选择策略，核心都是减少参与 compaction 的文件数，缩短整个 compaction 执行的时间，间接降低 compaction 的 IO 放大效应，减少对业务读写的延迟影响。然而，如果不对 Compaction 执行阶段的读写吞吐量进行限制的话也会引起短时间大量系统资源消耗，影响用户业务延迟。</p><p>通过感知 Compaction 的压力情况自动调节系统的 Compaction 吞吐量，在压力大的时候降低合并吞吐量，压力小的时候增加合并吞吐量。基本原理为：</p><ol><li>在正常情况下，用户需要设置吞吐量下限参数<code>hbase.hstore.compaction.throughput.lower.bound</code>（默认 10MB/sec）和上限参数<code>hbase.hstore.compaction.throughput.higher.bound</code>（默认 20MB/sec），而 hbase 实际会工作在吞吐量为 lower + (higer – lower) * ratio 的情况下，其中 ratio 是一个取值范围在 0 到 1 的小数，它由当前 store 中待参与 compation 的 file 数量决定，数量越多，ratio 越小，反之越大。</li><li>如果当前 store 中 hfile 的数量太多，并且超过了参数<code>hbase.hstore.blockingStoreFiles</code>，此时所有写请求就会阻塞等待 compaction 完成，这种场景下上述限制会自动失效。</li></ol><h5 id="Compaction-BandWidth-Limit"><a href="#Compaction-BandWidth-Limit" class="headerlink" title="Compaction BandWidth Limit"></a>Compaction BandWidth Limit</h5><p>在某些情况下 Compaction 还会因为大量消耗带宽资源从而严重影响其他业务，主要有两点原因：</p><ol><li><p>正常请求下，compaction 尤其是 major compaction 会将大量数据文件合并为一个大 HFile，读出所有数据文件的 KVs，然后重新排序之后写入另一个新建的文件。如果待合并文件都在本地，那么读就是本地读，不会出现跨网络的情况。但是因为数据文件都是三副本，因此写的时候就会跨网络执行，必然会消耗带宽资源。</p></li><li><p>在有些场景下待合并文件有可能并不全在本地，即本地化率没有达到 100%，比如执行过 balance 之后就会有很多文件并不在本地。这种情况下读文件的时候就会跨网络读，如果是 major compaction，必然也会大量消耗带宽资源。</p></li></ol><p>可以看出来，跨网络读是可以通过一定优化避免的，而跨网络写却是不可能避免的。因此优化 Compaction 带宽消耗，一方面需要提升本地化率，减少跨网络读；另一方面，虽然跨网络写不可避免，但也可以通过控制手段使得资源消耗控制在一个限定范围。Facebook 在这方面做了一些优化。</p><p>原理其实和 Limit Compaction Speed 思路基本一致，它主要涉及两个参数：compactBwLimit 和 numOfFilesDisableCompactLimit，作用分别如下：</p><ol><li>compactBwLimit：一次 compaction 的最大带宽使用量，如果 compaction 所使用的带宽高于该值，就会强制令其 sleep 一段时间</li><li>numOfFilesDisableCompactLimit：很显然，在写请求非常大的情况下，限制 compaction 带宽的使用量必然会导致 HFile 堆积，进而会影响到读请求响应延时。因此该值意义就很明显，一旦 store 中 hfile 数量超过该设定值，带宽限制就会失效。</li></ol><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul><li><p>Compaction 对于查询毛刺的影响</p><p>关于 Compaction 的参数调优，我们可能看到过这样的一些建议：尽可能的减少每一次 Compaction 的文件数量，目的是为了减短每一次 Compaction 的执行时间。但在实践中，这可能并不是一个合理的建议，例如，HBase 默认的触发 Minor Compaction 的最小文件数量为 3，但事实上，对于大多数场景而言，这可能是一个非常不合理的默认值，在我们的测试中，我们将最小文件数加大到 10 个，我们发现对于整体的吞吐量以及查询毛刺，都有极大的改进，所以，这里的建议为：Minor Compaction 的文件数量应该要结合实际业务场景设置合理的值。另外，在实践中，合理的限制 Compaction 资源的占用也是非常关键的，如 Compaction 的并发执行度，以及 Compaction 的吞吐量以及网络带宽占用等等。</p></li><li><p>Compaction 会影响 Block Cache</p><p>HFile 文件发生合并以后，旧 HFile 文件所关联的被 Cache 的 Block 将会失效。这也会影响到读取时延。</p></li></ul><h3 id="In-memory-Flush-and-Compaction"><a href="#In-memory-Flush-and-Compaction" class="headerlink" title="In-memory Flush and Compaction"></a>In-memory Flush and Compaction</h3><p>HBase 2.0 新增的特性，默认禁用。开启之后，MemStore 由一个可写的 Segment，以及一个或多个不可写的 Segments 构成。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201016112735.png" alt="image-20201016112734469"></p><p>MemStore 中的数据先 Flush 成一个 Immutable 的 Segment，多个 Immutable Segments 可以在内存中进行 Compaction，当达到一定阈值以后才将内存中的数据持久化成 HDFS 中的 HFile 文件。</p><p>如果 MemStore 中的数据被直接 Flush 成 HFile，而多个 HFile 又被 Compaction 合并成了一个大 HFile，随着一次次 Compaction 发生以后，一条数据往往被重写了多次，这带来显著的 IO 放大问题，另外，频繁的 Compaction 对 IO 资源的抢占，其实也是导致 HBase 查询时延大毛刺的罪魁祸首之一。而 In-memory Flush and Compaction 特性可以有力改善这一问题。</p><p>默认 MemStore 使用 ConcurrentSkipListMap 索引数据，这种结构支持动态修改，但是其中存在大量小对象，内存浪费比较严重。In-memory Flush and Compaction 将 MemStore 分为 MutableSegment 和 ImmutableSegment。其中 MutableSegment 仍然使用 ConcurrentSkipListMap 实现，而对 ImmutableSegment 就可以使用更紧凑的数据结构来存储索引，减少内存使用。</p><p>在融入了 In-Memory Flush and Compaction 特性之后，Flush 与 Compaction 的整体流程演变为：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201016113242.png" alt="image-20201016113241464"></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="http://www.nosqlnotes.com/technotes/hbase/flush-compaction/">一条数据的HBase之旅，简明HBase入门教程-Flush与Compaction</a></p><p><a href="http://hbasefly.com/2016/03/23/hbase-memstore-flush/">HBase – Memstore Flush深度解析</a></p><p><a href="https://www.iteblog.com/archives/2497.html?from=like#MemStore_FlushPolicy">HBase 入门之数据刷写(Memstore Flush)详细说明</a></p><p><a href="http://hbasefly.com/2016/07/13/hbase-compaction-1/">HBase Compaction的前生今世－身世之旅</a></p><p><a href="http://hbasefly.com/2016/07/25/hbase-compaction-2/">HBase Compaction的前生今世－改造之路</a></p><p><a href="https://www.jianshu.com/p/b22d664d0c2f">HBase Compaction-(2)ExploringCompactionPolicy以及RatioBasedCompactionPolicy</a></p><p><a href="https://my.oschina.net/u/220934/blog/363270">HBase Compaction算法之ExploringCompactionPolicy</a></p><p><a href="https://juejin.im/post/6844903727590031373">HBase HFile Compact吞吐量参数控制优化剖析-OLAP商业环境实战</a></p><p><a href="https://www.iteblog.com/archives/2416.html">Apache HBase中等对象存储MOB压缩分区策略介绍</a></p><p><a href="https://developer.aliyun.com/article/683101">HBase实战之MOB使用指南</a></p>]]></content>
    
    
    <categories>
      
      <category>技术文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase 学习：写入流程</title>
    <link href="/2020/10/15/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/"/>
    <url>/2020/10/15/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>本文介绍了 HBase 中写数据的接口和方式，数据路由和分发，以及 RegionServer 侧将数据写入到 Region 中的全部流程。</p><a id="more"></a><h3 id="客户端接口"><a href="#客户端接口" class="headerlink" title="客户端接口"></a>客户端接口</h3><p>HBase 服务端并没有提供 update、delete 接口，HBase 中对数据的更新、删除操作在服务器端也认为是写入操作，不同的是，更新操作会写入一个最新版本数据，删除操作会写入一条标记为 deleted 的 KV 数据。所以 HBase 中更新、删除操作的流程与写入流程完全一致。</p><p>HBase 中提供了如下几种主要的接口：</p><ul><li><p>Java Client API：HBase 的基础 API，应用最为广泛。</p></li><li><p>HBase Shell：基于 Shell 的命令行操作接口，基于 Java Client API 实现。</p></li><li><p>Restful API：Rest Server 侧基于 Java Client API 实现。</p></li><li><p>Thrift API：Thrift Server 侧基于 Java Client API 实现。</p></li><li><p>MapReduce Based Batch Manipulation API：基于 MapReduce 的批量数据读写 API。</p></li><li><p>除了上述主要的 API，HBase 还提供了基于 Spark 的批量操作接口以及 C++ Client 接口，但这两个特性都被规划在了 3.0 版本中，当前尚在开发中。</p></li></ul><p>无论是 HBase Shell/Restful API 还是 Thrift API，都是基于 Java Client API 实现的。因此，接下来关于流程的介绍，都是基于 Java Client API 的调用流程展开的。</p><h4 id="表服务接口抽象"><a href="#表服务接口抽象" class="headerlink" title="表服务接口抽象"></a>表服务接口抽象</h4><p>HBase 支持同步连接与异步连接，分别提供了不同的表服务接口抽象：</p><ul><li><p>Table：同步连接中的表服务接口定义</p></li><li><p>AsyncTable：异步连接中的表服务接口定义</p></li></ul><p>异步连接 AsyncConnection 获取 AsyncTable 实例的接口默认实现：</p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">default</span> AsyncTable&lt;AdvancedScanResultConsumer&gt; <span class="hljs-title">getTable</span><span class="hljs-params">(TableName tableName)</span> </span>&#123;    <span class="hljs-keyword">return</span> getTableBuilder(tableName).build();&#125;</code></pre><p>同步连接 ClusterConnection 的实现类 ConnectionImplementation 中获取 Table 实例的接口实现：</p><pre><code class="hljs java"><span class="hljs-meta">@Override</span><span class="hljs-function"><span class="hljs-keyword">public</span> Table <span class="hljs-title">getTable</span><span class="hljs-params">(TableName tableName)</span> <span class="hljs-keyword">throws</span> IOException </span>&#123;    <span class="hljs-keyword">return</span> getTable(tableName, getBatchPool());&#125;</code></pre><h4 id="写数据方式"><a href="#写数据方式" class="headerlink" title="写数据方式"></a>写数据方式</h4><ul><li><p>Single Put</p><p>单条记录单条记录的随机 put 操作。Single Put 所对应的接口定义如下：</p><p>在 AsyncTable 接口中的定义：</p><pre><code class="hljs java"><span class="hljs-function">CompletableFuture&lt;Void&gt; <span class="hljs-title">put</span><span class="hljs-params">(Put put)</span></span>;</code></pre><p>在 Table 接口中的定义：</p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">put</span><span class="hljs-params">(Put put)</span> <span class="hljs-keyword">throws</span> IOException</span>;</code></pre></li><li><p>Batch Put</p><p>汇聚了几十条甚至是几百上千条记录之后的小批次随机 put 操作。</p><p>Batch Put 只是本文对该类型操作的称法，实际的接口名称如下所示：</p><p>在 AsyncTable 接口中的定义：</p><pre><code class="hljs java">List&lt;CompletableFuture&lt;Void&gt;&gt; put(List&lt;Put&gt; puts);</code></pre><p>在 Table 接口中的定义：</p><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">put</span><span class="hljs-params">(List&lt;Put&gt; puts)</span> <span class="hljs-keyword">throws</span> IOException</span>;</code></pre></li><li><p>Bulkload</p><p>基于 MapReduce API 提供的数据批量导入能力，导入数据量通常在 GB 级别以上，Bulkload 能够绕过 Java Client API 直接生成 HBase 的底层数据文件(HFile)。</p></li></ul><h3 id="客户端处理阶段"><a href="#客户端处理阶段" class="headerlink" title="客户端处理阶段"></a>客户端处理阶段</h3><h4 id="数据路由"><a href="#数据路由" class="headerlink" title="数据路由"></a>数据路由</h4><h5 id="初始化-ZooKeeper-Session"><a href="#初始化-ZooKeeper-Session" class="headerlink" title="初始化 ZooKeeper Session"></a>初始化 ZooKeeper Session</h5><p>因为 meta Region 的路由信息存放于 ZooKeeper 中，在第一次从 ZooKeeper 中读取 META Region 的地址时，需要先初始化一个 ZooKeeper Session。ZooKeeper Session 是 ZooKeeper Client 与 ZooKeeper Server 端所建立的一个会话，通过心跳机制保持长连接。</p><h5 id="获取-Region-路由信息"><a href="#获取-Region-路由信息" class="headerlink" title="获取 Region 路由信息"></a>获取 Region 路由信息</h5><p>通过前面建立的连接，从 ZooKeeper 中读取 meta Region 所在的 RegionServer，这个读取流程，当前已经是异步的。获取了 meta Region 的路由信息以后，再从 meta Region 中定位要读写的 RowKey 所关联的 Region 信息。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015135751.png" alt="image-20201015135749815"></p><p>因为每一个用户表 Region 都是一个 RowKey Range，meta Region 中记录了每一个用户表 Region 的路由以及状态信息，以 RegionName(包含表名，Region StartKey，Region ID，副本 ID 等信息)作为 RowKey。基于一条用户数据 RowKey，快速查询该 RowKey 所属的 Region 的方法其实很简单：只需要基于表名以及该用户数据 RowKey，构建一个虚拟的 Region Key，然后通过 Reverse Scan 的方式，读到的第一条 Region 记录就是该数据所关联的 Region。如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015135820.png" alt="image-20201015135815875"></p><p>Region 只要不被迁移，那么获取的该 Region 的路由信息就是一直有效的，因此，HBase Client 有一个 Cache 机制来缓存 Region 的路由信息，避免每次读写都要去访问 ZooKeeper 或者 meta Region。</p><h4 id="客户端的写缓冲区"><a href="#客户端的写缓冲区" class="headerlink" title="客户端的写缓冲区"></a>客户端的写缓冲区</h4><p>用户提交 put 请求后，HBase 客户端会将写入的数据添加到本地缓冲区中，符合一定条件就会通过 AsyncProcess 异步批量提交。HBase 默认设置 autoflush=true，表示 put 请求直接会提交给服务器进行处理；用户可以设置 autoflush=false，这样，put 请求会首先放到本地缓冲区，等到本地缓冲区大小超过一定阈值（默认为 2M，可以通过配置文件配置）之后才会提交。很显然，后者使用批量提交请求，可以极大地提升写入吞吐量，但是因为没有保护机制，如果客户端崩溃，会导致部分已经提交的数据丢失。</p><h4 id="客户端的数据分组"><a href="#客户端的数据分组" class="headerlink" title="客户端的数据分组"></a>客户端的数据分组</h4><p>如果待写入的数据采用 Batch Put 的方式，那么，客户端在将所有的数据写到对应的 RegionServer 之前，会先分组，流程如下：</p><ol><li>遍历每一条数据的 RowKey，然后，依据 meta 表中记录的 Region 信息，确定每一条数据所属的 Region。此步骤可以获取到 Region 到 RowKey 列表的映射关系。</li><li>因为 Region 一定归属于某一个 RegionServer（未考虑 Region Replica 特性），那属于同一个 RegionServer 的多个 Regions 的写入请求，被打包成一个 MultiAction 对象，这样可以一并发送到每一个 RegionServer 中。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015154401.png" alt="image-20201015140336881"></p><h4 id="客户端发送写数据请求"><a href="#客户端发送写数据请求" class="headerlink" title="客户端发送写数据请求"></a>客户端发送写数据请求</h4><p>HBase 会为每个 HRegionLocation 构造一个远程 RPC 请求 MultiServerCallable，并通过 rpcCallerFactory.newCaller（）执行调用。将请求经过 Protobuf 序列化后发送给对应的 RegionServer。</p><h3 id="Region-写入阶段"><a href="#Region-写入阶段" class="headerlink" title="Region 写入阶段"></a>Region 写入阶段</h3><h4 id="Region-分发"><a href="#Region-分发" class="headerlink" title="Region 分发"></a>Region 分发</h4><p>RegionServer 的 RPC Server 侧，接收到来自 Client 端的 RPC 请求以后，将该请求交给 Handler 线程处理。</p><p>如果是 single put，则该步骤比较简单，因为在发送过来的请求参数 MutateRequest 中，已经携带了这条记录所关联的 Region，那么直接将该请求转发给对应的 Region 即可。</p><p>如果是 batch puts，则接收到的请求参数为 MultiRequest，在 MultiRequest 中，混合了这个 RegionServer 所持有的多个 Region 的写入请求，每一个 Region 的写入请求都被包装成了一个 RegionAction 对象。RegionServer 接收到 MultiRequest 请求以后，遍历所有的 RegionAction，而后写入到每一个 Region 中，此过程是串行的。</p><h4 id="核心操作"><a href="#核心操作" class="headerlink" title="核心操作"></a>核心操作</h4><p>服务器端 RegionServer 接收到客户端的写入请求后，首先会反序列化为 put 对象，然后执行各种检查操作，比如检查 Region 是否是只读、MemStore 大小是否超过 blockingMemstoreSize 等。检查完成之后，执行一系列核心操作。</p><ol><li>Acquire locks：HBase 中使用行锁保证对同一行数据的更新都是互斥操作，用以保证更新的原子性，要么更新成功，要么更新失败。</li><li>Update LATEST_TIMESTAMP timestamps：更新所有待写入（更新）KeyValue 的时间戳为当前系统时间。</li><li>Build WAL edit：HBase 使用 WAL 机制保证数据可靠性，即首先写日志再写缓存，即使发生宕机，也可以通过恢复 HLog 还原出原始数据。该步骤就是在内存中构建 WALEdit 对象，为了保证 Region 级别事务的写入原子性，一次写入操作中所有 KeyValue 会构建成一条 WALEdit 记录。</li><li>Append WALEdit To WAL：将步骤 3 中构造在内存中的 WALEdit 记录顺序写入 HLog 中，此时不需要执行 sync 操作。当前版本的 HBase 使用了 disruptor 实现了高效的生产者消费者队列，来实现 WAL 的追加写入操作。</li><li>Write back to MemStore：写入 WAL 之后再将数据写入 MemStore。</li><li>Release row locks：释放行锁。</li><li>Sync wal：HLog 真正 sync 到 HDFS，在释放行锁之后执行 sync 操作是为了尽量减少持锁时间，提升写性能。如果 sync 失败，执行回滚操作将 MemStore 中已经写入的数据移除。</li><li>结束写事务：此时该线程的更新操作才会对其他读请求可见，更新才实际生效。</li></ol><h4 id="写-WAL"><a href="#写-WAL" class="headerlink" title="写 WAL"></a>写 WAL</h4><p>HBase 采用了 LSM-Tree 的架构设计，每一个 Region 中随机写入的数据，都暂时先缓存在内存中(HBase 中存放这部分内存数据的模块称之为 MemStore)，为了保障数据可靠性，将这些随机写入的数据顺序写入到一个称之为 WAL（Write-Ahead-Log）的日志文件中，WAL 中的数据按时间顺序组织：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015144306.png" alt=""></p><p>如果位于内存中的数据尚未持久化，而且突然遇到了机器断电，只需要将 WAL 中的数据回放到 Region 中即可:</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015144331.png" alt=""></p><p>在 HBase 中，默认一个 RegionServer 只有一个可写的 WAL 文件。WAL 中写入的记录，以 Entry 为基本单元，而一个 Entry 中，包含：</p><ul><li>WALKey：包含{Encoded Region Name，Table Name，Sequence ID，Timestamp}等关键信息，其中，Sequence ID 在维持数据一致性方面起到了关键作用，可以理解为一个事务 ID。</li><li>WALEdit：保存待写入数据的所有的 KeyValues，而这些 KeyValues 可能来自一个 Region 中的多行数据。</li></ul><h5 id="WAL-持久化等级"><a href="#WAL-持久化等级" class="headerlink" title="WAL 持久化等级"></a>WAL 持久化等级</h5><p>HBase 可以通过设置持久化等级决定是否开启 WAL 机制以及 WAL 的落盘方式。持久化等级分为如下五个等级。</p><ul><li>SKIP_WAL：只写缓存，不写 HLog 日志。因为只写内存，因此这种方式可以极大地提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。</li><li>ASYNC_WAL：异步将数据写入 HLog 日志中。</li><li>SYNC_WAL：同步将数据写入日志文件中，需要注意的是，数据只是被写入文件系统中，并没有真正落盘。</li><li>FSYNC_WAL：同步将数据写入日志文件并强制落盘。这是最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。</li><li>USER_DEFAULT：如果用户没有指定持久化等级，默认 HBase 使用 SYNC_WAL 等级持久化数据。</li></ul><h5 id="WAL-写入模型"><a href="#WAL-写入模型" class="headerlink" title="WAL 写入模型"></a>WAL 写入模型</h5><p>WAL 写入都需要经过三个阶段：首先将数据写入本地缓存，然后将本地缓存写入文件系统，最后执行 sync 操作同步到磁盘，可以采用“生产者-消费者”队列实现。在高并发随机写入场景下，会带来大量的 Sync 操作。HBase 中采用了 Disruptor 的 RingBuffer 来减少竞争。如果将瞬间并发写入 WAL 中的数据，合并执行 Sync 操作，可以有效降低 Sync 操作的次数，来提升写吞吐量。</p><p>对于 append，WALEdit 和 HLogKey 会被封装成 FSWALEntry 类，进而再封装成 RingBufferTruck 类放入 Disruptor 无锁有界队列中。当调用 sync 后，会生成一个 SyncFuture，再封装成 RingBufferTruck 类放入同一个队列中，然后工作线程会被阻塞，等待 notify（）来唤醒。</p><p>Disruptor 框架中有且仅有一个消费者线程工作。这个框架会从 Disruptor 队列中依次取出 RingBufferTruck 对象，然后根据如下选项来操作：</p><ul><li>如果 RingBufferTruck 对象中封装的是 FSWALEntry，就会执行文件 append 操作，将记录追加写入 HDFS 文件中。需要注意的是，此时数据有可能并没有实际落盘，而只是写入到文件缓存。</li><li>如果 RingBufferTruck 对象是 SyncFuture，会调用线程池的线程异步地批量刷盘，刷盘成功之后唤醒工作线程完成 HLog 的 sync 操作。</li></ul><h5 id="WAL-Roll-and-Archive"><a href="#WAL-Roll-and-Archive" class="headerlink" title="WAL Roll and Archive"></a>WAL Roll and Archive</h5><p>当正在写的 WAL 文件达到一定大小以后，会创建一个新的 WAL 文件，上一个 WAL 文件依然需要被保留，因为这个 WAL 文件中所关联的 Region 中的数据，尚未被持久化存储，因此，该 WAL 可能会被用来回放数据。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015145621.png" alt="image-20201015145619940"></p><p>如果一个 WAL 中所关联的所有的 Region 中的数据，都已经被持久化存储了，那么，这个 WAL 文件会被暂时归档到另外一个目录中：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015145734.png" alt="image-20201015145733709"></p><p>注意，这里不是直接将 WAL 文件删除掉，这是一种稳妥且合理的做法，原因如下：</p><ul><li>避免因为逻辑实现上的问题导致 WAL 被误删，暂时归档到另外一个目录，为错误发现预留了一定的时间窗口</li><li>按时间维度组织的 WAL 数据文件还可以被用于其它用途，如增量备份，跨集群容灾等等，因此，这些 WAL 文件通常不允许直接被删除，至于何时可以被清理，还需要额外的控制逻辑</li></ul><p>另外，如果对写入 HBase 中的数据的可靠性要求不高，那么，HBase 允许通过配置跳过写 WAL 操作。</p><h5 id="Multi-WAL"><a href="#Multi-WAL" class="headerlink" title="Multi-WAL"></a>Multi-WAL</h5><p>默认情形下，一个 RegionServer 只有一个被写入的 WAL Writer，尽管 WAL Writer 依靠顺序写提升写吞吐量，在基于普通机械硬盘的配置下，此时只能有单块盘发挥作用，其它盘的 IOPS 能力并没有被充分利用起来，这是 Multi-WAL 设计的初衷。Multi-WAL 可以在一个 RegionServer 中同时启动几个 WAL Writer，可按照一定的策略，将一个 Region 与其中某一个 WAL Writer 绑定，这样可以充分发挥多块盘的性能优势。</p><h4 id="写-MemStore"><a href="#写-MemStore" class="headerlink" title="写 MemStore"></a>写 MemStore</h4><p>每一个 Column Family，在 Region 内部被抽象为了一个 HStore 对象，而每一个 HStore 拥有自身的 MemStore，用来缓存一批最近被随机写入的数据，这是 LSM-Tree 核心设计的一部分。</p><p>MemStore 中用来存放所有的 KeyValue 的数据结构，称之为 CellSet，而 CellSet 的核心是一个 ConcurrentSkipListMap，我们知道，ConcurrentSkipListMap 是 Java 的跳表实现，数据按照 Key 值有序存放，而且在高并发写入时，性能远高于 ConcurrentHashMap。</p><p>因此，写 MemStore 的过程，事实上是将 batch put 提交过来的所有的 KeyValue 列表，写入到 MemStore 的以 ConcurrentSkipListMap 为组成核心的 CellSet 中：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015150140.png" alt="image-20201015150138287"></p><p>MemStore 因为涉及到大量的随机写入操作，会带来大量 Java 小对象的创建与消亡，会导致大量的内存碎片，给 GC 带来比较重的压力，HBase 为了优化这里的机制，借鉴了操作系统的内存分页的技术，增加了一个名为 MSLab 的特性，通过分配一些固定大小的 Chunk，来存储 MemStore 中的数据，这样可以有效减少内存碎片问题，降低 GC 的压力。当然，ConcurrentSkipListMap 本身也会创建大量的对象，这里也有很大的优化空间，有文章介绍过阿里如何通过优化 ConcurrentSkipListMap 的结构来有效减少 GC 时间。</p><h4 id="Flush-amp-Compaction"><a href="#Flush-amp-Compaction" class="headerlink" title="Flush &amp; Compaction"></a>Flush &amp; Compaction</h4><p>随着数据的不断写入，MemStore 中存储的数据会越来越多，系统为了将使用的内存保持在一个合理的水平，会将 MemStore 中的数据写入文件形成 HFile。Flush 阶段是 HBase 的非常核心的阶段，需要重点关注三个问题：</p><ul><li>MemStore Flush 的触发时机。即在哪些情况下 HBase 会触发 flush 操作。</li><li>MemStore Flush 的整体流程。</li><li>HFile 的构建流程。HFile 构建是 MemStore Flush 整体流程中最重要的一个部分，这部分内容会涉及 HFile 文件格式的构建、布隆过滤器的构建、HFile 索引的构建以及相关元数据的构建等。</li></ul><p>随着 flush 产生的 HFile 文件越来越多，系统还会对 HFile 文件进行 Compaction 操作。Compaction 会从一个 Region 的一个 Store 中选择部分 HFile 文件进行合并。合并原理是，先从这些待合并的数据文件中依次读出 KeyValue，再由小到大排序后写入一个新的文件。之后，这个新生成的文件就会取代之前已合并的所有文件对外提供服务。</p><p>有关 Flush &amp; Compaction 的内容，将会在后续文章中深入介绍。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="http://www.nosqlnotes.com/technotes/hbase/hbase-overview-writeflow/">一条数据的HBase之旅，简明HBase入门教程-Write全流程</a></p>]]></content>
    
    
    <categories>
      
      <category>技术文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase 学习：RegionServer</title>
    <link href="/2020/09/29/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9ARegionServer/"/>
    <url>/2020/09/29/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9ARegionServer/</url>
    
    <content type="html"><![CDATA[<p>RegionServer 是 HBase 系统中最核心的组件，主要负责用户数据写入、读取等基础操作。RegionServer 包含多个核心模块：HLog、MemStore、HFile 以及 BlockCache。本文主要介绍 RegionServer 核心模块的作用、内部结构等内容。后续文章中将会进一步介绍 HBase 的写入读取流程。</p><a id="more"></a><h3 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h3><p>RegionServer 是 HBase 系统响应用户读写请求的工作节点组件，由多个核心模块组成，其内部结构如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/3241429573-5f1ea380df677.png" alt="3241429573-5f1ea380df677"></p><p>一个 RegionServer 由一个（或多个）HLog、一个 BlockCache 以及多个 Region 组成。</p><ul><li>HLog 用来保证数据写入的可靠性；</li><li>BlockCache 可以将数据块缓存在内存中以提升数据读取性能；</li><li>Region 是 HBase 中数据表的一个数据分片，一个 RegionServer 上通常会负责多个 Region 的数据读写。</li><li>一个 Region 由多个 Store 组成，每个 Store 存放对应列族的数据，比如一个表中有两个列族，这个表的所有 Region 就都会包含两个 Store。</li><li>每个 Store 包含一个 MemStore 和多个 HFile，用户数据写入时会将对应列族数据写入相应的 MemStore，一旦写入数据的内存大小超过设定阈值，系统就会将 MemStore 中的数据落盘形成 HFile 文件。HFile 存放在 HDFS 上，是一种定制化格式的数据存储文件，方便用户进行数据读取。</li></ul><h3 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h3><p>HBase 中系统故障恢复以及主从复制都基于 HLog 实现。默认情况下，所有写入操作（写入、更新以及删除）的数据都先以追加形式写入 HLog，再写入 MemStore。大多数情况下，HLog 并不会被读取，但如果 RegionServer 在某些异常情况下发生宕机，此时已经写入 MemStore 中但尚未 flush 到磁盘的数据就会丢失，需要回放 HLog 补救丢失的数据。此外，HBase 主从复制需要主集群将 HLog 日志发送给从集群，从集群在本地执行回放操作，完成集群之间的数据复制。</p><h4 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h4><p>HLog 文件的基本结构如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/118663058-5f1ea423acaf6.png" alt="HLog文件结构"></p><ul><li><p>每个 RegionServer 拥有一个或多个 HLog（默认只有 1 个，1.1 版本可以开启 MultiWAL 功能，允许多个 HLog）。每个 HLog 是多个 Region 共享的，图中 Region A、Region B 和 Region C 共享一个 HLog 文件。</p></li><li><p>HLog 中，日志单元 WALEntry（图中小方框）表示一次行级更新的最小追加单元，它由 HLogKey 和 WALEdit 两部分组成，其中 HLogKey 由 table name、region name 以及 sequenceid 等字段构成。</p></li><li>为了保证行级事务的原子性，HBase 将一个行级事务的写入操作表示为一条记录。WALEdit 会被序列化为格式<code>&lt;-1, # of edits, , , &gt;</code>，-1 为标识符，表示这种新的日志结构。假设一个行级事务更新 R 行中的 3 列（c1, c2, c3），WAL 结构为 <code>&lt;-1, 3, &lt;Keyvalue-for-edit-c1&gt;, &lt;KeyValue-for-edit-c2&gt;, &lt;KeyValue-for-edit-c3&gt;&gt;</code>。</li></ul><h4 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h4><p>HBase 中所有数据（包括 HLog 以及用户实际数据）都存储在 HDFS 的指定目录下。假设指定目录为 /hbase，/hbase/WALs 存储当前还未过期的日志；/hbase/oldWALs 存储已经过期的日志。/hbase/WALs 目录下通常会有多个子目录，每个子目录代表一个对应的 RegionServer。</p><h4 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h4><p>HLog 文件生成之后并不会永久存储在系统中，它的使命完成后，文件就会失效最终被删除。HLog 整个生命周期如图所示。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200929161150036.png" alt="image-20200929161150036"></p><p>HLog 生命周期包含 4 个阶段：</p><ol><li><p>HLog 构建：HBase 的任何写入（更新、删除）操作都会先将记录追加写入到 HLog 文件中。</p></li><li><p>HLog 滚动：HBase 后台启动一个线程，每隔一段时间（由参数 <code>hbase.regionserver. logroll.period</code> 决定，默认 1 小时）进行日志滚动。日志滚动会新建一个新的日志文件，接收新的日志数据。日志滚动机制主要是为了方便过期日志数据能够以文件的形式直接删除。</p></li><li><p>HLog 失效：写入数据一旦从 MemStore 中落盘，对应的日志数据就会失效。为了方便处理，HBase 中日志失效删除总是以文件为单位执行。查看某个 HLog 文件是否失效只需确认该 HLog 文件中所有日志记录对应的数据是否已经完成落盘，如果日志中所有日志记录已经落盘，则可以认为该日志文件失效。一旦日志文件失效，就会从 WALs 文件夹移动到 oldWALs 文件夹。注意此时 HLog 并没有被系统删除。</p></li><li><p>HLog 删除：Master 后台会启动一个线程，每隔一段时间（参数 <code>hbase.master.cleaner. interval</code>，默认 1 分钟）检查一次文件夹 oldWALs 下的所有失效日志文件，确认是否可以删除，确认可以删除之后执行删除操作。确认条件主要有两个：</p><ol><li>该 HLog 文件是否还在参与主从复制。对于使用 HLog 进行主从复制的业务，需要继续确认是否该 HLog 还在应用于主从复制。</li><li>该 HLog 文件是否已经在 OldWALs 目录中存在 10 分钟。为了更加灵活地管理 HLog 生命周期，系统提供了参数设置日志文件的 TTL（参数 ‘hbase.master.logcleaner.ttl’，默认 10 分钟），默认情况下 oldWALs 里面的 HLog 文件最多可以再保存 10 分钟。</li></ol></li></ol><p>HLog 中的 sequenceId 的具体作用可以参考： <a href="http://hbasefly.com/2017/07/02/hbase-sequenceid/">HBase原理－要弄懂的sequenceId</a>。</p><h3 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h3><p>HBase 系统中一张表会被水平切分成多个 Region，每个 Region 负责自己区域的数据读写请求。水平切分意味着每个 Region 会包含所有的列族数据，HBase 将不同列族的数据存储在不同的 Store 中，每个 Store 由一个 MemStore 和一系列 HFile 组成。</p><p>HBase 基于 LSM 树模型实现，所有的数据写入操作首先会顺序写入日志 HLog，再写入 MemStore，当 MemStore 中数据大小超过阈值之后再将这些数据批量写入磁盘，生成一个新的 HFile 文件。LSM 树架构有如下几个非常明显的优势：</p><ul><li>这种写入方式将一次随机 IO 写入转换成一个顺序 IO 写入（HLog 顺序写入）加上一次内存写入（MemStore 写入），使得写入性能得到极大提升。</li><li>HFile 中 KeyValue 数据需要按照 Key 排序，排序之后可以在文件级别根据有序的 Key 建立索引树，极大提升数据读取效率。然而 HDFS 本身只允许顺序读写，不能更新，因此需要数据在落盘生成 HFile 之前就完成排序工作，MemStore 就是 KeyValue 数据排序的实际执行者。</li><li>MemStore 作为一个缓存级的存储组件，总是缓存着最近写入的数据。对于很多业务来说，最新写入的数据被读取的概率会更大。</li><li>在数据写入 HFile 之前，可以在内存中对 KeyValue 数据进行很多更高级的优化。比如，如果业务数据保留版本仅设置为 1，在业务更新比较频繁的场景下，MemStore 中可能会存储某些数据的多个版本。这样，MemStore 在将数据写入 HFile 之前实际上可以丢弃老版本数据，仅保留最新版本数据。</li></ul><h4 id="内部结构-1"><a href="#内部结构-1" class="headerlink" title="内部结构"></a>内部结构</h4><p>实现 MemStore 模型的数据结构是 SkipList（跳表），跳表可以实现高效的查询、插入、删除操作，这些操作的期望复杂度都是   O(logN)。HBase 使用了 JDK 自带的数据结构 ConcurrentSkipListMap，MemStore 由两个 ConcurrentSkipListMap（称为 A 和 B）实现。写入操作（包括更新删除操作）会将数据写入 ConcurrentSkipListMap A，当 ConcurrentSkipListMap A 中数据量超过一定阈值之后会创建一个新的 ConcurrentSkipListMap B 来接收用户新的请求，之前已经写满的 ConcurrentSkipListMap A 会执行异步 flush 操作落盘形成 HFile。</p><h4 id="GC问题"><a href="#GC问题" class="headerlink" title="GC问题"></a>GC问题</h4><p>对于 HBase 这样基于 LSM 实现的 MemStore 来说，上述实现方案每写入一个 KeyValue，在没有写入 ConcurrentSkipList 之前就需要申请一个内存对象，这些对象会在内存中存在很长一段时间。</p><p>又因为一个 RegionServer 由多个 Region 构成，每个 Region 根据列族的不同又包含多个 MemStore，这些 MemStore 都是共享内存的。这样，不同 Region 的数据写入对应的 MemStore，因为共享内存，在 JVM 看来所有 MemStore 的数据都是混合在一起写入 Heap 的。</p><p>一旦某个 Region 的所有 MemStore 数据执行 flush 操作，所占用的内存就会被释放。而由于其他 MemStore 数据还未执行 flush，因此释放的内存空间就变成内存碎片。这些内存空间继续为写入 MemStore 的数据分配空间，在下一次 flush 后会形成更小的内存碎片。最终因为无法分配一块完成可用的内存空间，频发触发长时间的 Full GC。</p><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><h5 id="MSLAB"><a href="#MSLAB" class="headerlink" title="MSLAB"></a>MSLAB</h5><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200929193419405.png" alt="image-20200929193419405"></p><p>MemStore 借鉴 TLAB（Thread Local Allocation Buffer）机制，实现了 MemStoreLAB，简称 MSLAB。基于 MSLAB 实现写入的核心流程如下：</p><ol><li><p>一个 KeyValue 写入之后不再单独为 KeyValue 申请内存，而是提前申请好一个 2M 大小的内存区域（Chunk）。</p></li><li><p>将写入的 KeyValue 顺序复制到申请的 Chunk 中，一旦 Chunk 写满，再申请下一个 Chunk。</p></li><li><p>将 KeyValue 复制到 Chunk 中后，生成一个 Cell 对象（这个 Cell 对象在源码中为 ByteBufferChunkKeyValue），这个 Cell 对象指向 Chunk 中的 KeyValue 内存区域。</p></li><li><p>将这个 Cell 对象作为 Key 和 Value 写入 ConcurrentSkipListMap 中。</p></li><li><p>原生的 KeyValue 对象写入到 Chunk 之后就没有再被引用，所以很快就会被 Young GC 回收掉。</p></li></ol><h5 id="ChunkPool"><a href="#ChunkPool" class="headerlink" title="ChunkPool"></a>ChunkPool</h5><p>MSLAB 机制中 KeyValue 写入 Chunk，如果 Chunk 写满了会在 JVM 堆内存申请一个新的 Chunk。引入 ChunkPool 后，申请 Chunk 都从 ChunkPool 中申请，如果 ChunkPool 中没有可用的空闲 Chunk，才会从 JVM 堆内存中申请新 Chunk。如果一个 MemStore 执行 flush 操作后，这个 MemStore 对应的所有 Chunk 都可以被回收，回收后重新进入池子中，以备下次使用。</p><p>每个 RegionServer 会有一个全局的 Chunk 管理器，负责 Chunk 的生成、回收等。MemStore 申请 Chunk 对象会发送请求让 Chunk 管理器创建新 Chunk，Chunk 管理器会检查当前是否有空闲 Chunk，如果有空闲 Chunk，就会将这个 Chunk 对象分配给 MemStore，否则从 JVM 堆上重新申请。每个 MemStore 仅持有 Chunk 内存区域的引用</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200929193821128.png" alt="基于 ChunkPool 实现的 Chunk 管理模型"></p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930101450.png" alt="MemStore Flush 过程中 Chunk 回收过程"></p><h5 id="Chunk-Offheap"><a href="#Chunk-Offheap" class="headerlink" title="Chunk Offheap"></a>Chunk Offheap</h5><p>HBase 2.0 为了尽最大可能避免 Java GC 对其造成的性能影响，已经对读写两条核心路径做了 offheap 化，也就是对象的申请都直接向 JVM offheap 申请，而 offheap 分出来的内存都是不会被 JVM GC 的，需要用户自己显式地释放。在写路径上，客户端发过来的请求包都会被分配到 offheap 的内存区域，直到数据成功写入 WAL 日志和 Memstore，其中维护 Memstore 的 ConcurrentSkipListSet 其实也不是直接存 Cell 数据，而是存 Cell 的引用，真实的内存数据被编码在 MSLAB 的多个 Chunk 内，这样比较便于管理 offheap 内存。类似地，在读路径上，先尝试去读 BucketCache，Cache 未命中时则去 HFile 中读对应的 Block，这其中占用内存最多的 BucketCache 就放在 offheap 上，拿到 Block 后编码成 Cell 发送给用户，整个过程基本上都不涉及 heap 内对象申请。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930102524.png" alt="image-20200930102523527"></p><p>Chunk 堆外化实现比较简单，在创建新 Chunk 时根据用户配置选择是否使用堆外内存，如果使用堆外内存，就使用 JDK 提供的 ByteBuffer.allocateDirect 方法在堆外申请特定大小的内存区域，否则使用 ByteBuffer.allocate 方法在堆内申请。如果不做配置，默认使用堆内内存，用户可以设置 hbase.regionserver.offheap.global.memstore.size 这个值为大于 0 的值开启堆外，表示 RegionServer 中所有 MemStore 可以使用的堆外内存总大小。</p><h5 id="In-memory-Compaction"><a href="#In-memory-Compaction" class="headerlink" title="In-memory Compaction"></a>In-memory Compaction</h5><p>In-Memory Compaction 是 HBase2.0 中的重要特性之一，通过在内存中引入 LSM tree 结构，减少多余数据，实现降低 flush 频率和减小写放大的效果。</p><p>In-Memory Compaction 中引入了 MemStore 的一个新的实现类 CompactingMemStore 。在默认的 MemStore 中，对 Cell 的索引使用 ConcurrentSkipListMap，这种结构支持动态修改，但是其中存在大量小对象，内存浪费比较严重。而在 CompactingMemStore 中，MemStore 分为 MutableSegment 和 ImmutableSegment，其中 ImmutableSegment 就可以使用更紧凑的数据结构来存储索引，减少内存使用。</p><p>CompactingMemStore 的核心工作原理如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930110309.png" alt="image-20200930110308424"></p><ol><li>一个 Cell 写入到 Region 后会先写 入 MutableSegment 中。MutableSegment 可以认为就是一个小的 MemStore，MutableSegment 包含一个 MSLAB 存储 Chunk，同时包含一个 ConcurrentSkipListMap。</li><li>默认情况下一旦 MutableSegment 的大小超过 2M，就会执行 In-memory Flush 操作，将 MutableSegment 变为 ImmutableSegment，并重新生成一个新的 MutableSegment 接收写入。ImmutableSegment 有多个实现类，In-memory Flush 生成的 ImmutableSegment 为 CSLMImmutableSegment，可以预见这个 ImmutableSegment 在数据结构上也是使用 ConcurrentSkipListMap。</li><li>每次执行完 In-memory Flush 之后，RegionServer 都会启动一个异步线程执行 In-memory Compaction。In-memory Compaction 的本质是将 CSLMImmutableSegment 变为 CellArrayImmutableSegment 或者 CellChunkImmutableSegment。</li></ol><p>CellArrayImmutableSegment 和 CSLMImmutableSegment 相比，相当于将 ConcurrentSkipListMap 拉平为数组。CellChunkImmutableSegment 借鉴 Chunk 思路申请一块 2M 的大内存空间，遍历数组中的 Cell 对象，将其顺序拷贝到这个 Chunk（这种 Chunk 称为 Index Chunk，区别与存储 KV 数据的 Data Chunk）中，就变成了 CellChunkImmutableSegment。</p><p>如果 RegionServer 需要把 MemStore 的数据 flush 到磁盘，会首先选择其他类型 的 MemStore，然后再选择 CompactingMemStore。这是因为 CompactingMemStore 对内存的管理更有效率，所以延长 CompactingMemStore 的生命周期可以减少总的 I/O。当 CompactingMemStore 被 flush 到磁盘时， 所有 segment 会被移到一个 snapshot 中进行合并然后写入 HFile。</p><h3 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h3><p>HFile 是 HBase 存储数据的文件组织形式，参考 BigTable 的 SSTable 和 Hadoop 的 TFile 实现。从 HBase 开始到现在，HFile 经历了三个版本，其中 V2 在 0.92 引入，V3 在 0.98 引入。HFile V1 版本的在实际使用过程中发现它占用内存多，HFile V2 版本针对此进行了优化，HFile V3 版本基本和 V2 版本相同，只是在 cell 层面添加了 Tag 数组的支持。</p><h4 id="逻辑结构"><a href="#逻辑结构" class="headerlink" title="逻辑结构"></a>逻辑结构</h4><p>HFile V2 的逻辑结构如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930142527.png" alt="image-20200930142525935"></p><p>文件主要分为四个部分：Scanned block section，Non-scanned block section，Opening-time data section 和 Trailer。</p><p>Scanned block section：顾名思义，表示顺序扫描 HFile 时所有的数据块将会被读取，包括 Leaf Index Block 和 Bloom Block。</p><p>Non-scanned block section：表示在 HFile 顺序扫描的时候数据不会被读取，主要包括 Meta Block 和 Intermediate Level Data Index Blocks 两部分。</p><p>Load-on-open-section：这部分数据在 HBase 的 region server 启动时，需要加载到内存中。包括 FileInfo、Bloom filter block、data block index 和 meta block index。</p><p>Trailer：这部分主要记录了 HFile 的基本信息、各个部分的偏移值和寻址信息。</p><h4 id="物理结构"><a href="#物理结构" class="headerlink" title="物理结构"></a>物理结构</h4><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930142741.png" alt="image-20200930142740461"></p><p>如上图所示， HFile 会被切分为多个大小相等的 block 块，每个 block 的大小可以在创建表列族的时候通过参数 blocksize ＝&gt; ‘65535’ 进行指定，默认为 64k，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询，因而需要权衡。而且所有 block 块都拥有相同的数据结构，如下图左侧所示，HBase 将 block 块抽象为一个统一的 HFileBlock。HFileBlock 支持两种类型，一种类型不支持 checksum，一种不支持。为方便讲解，下图选用不支持 checksum 的 HFileBlock 内部结构：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930143005.png" alt="image-20200930143004497"></p><p>上图所示 HFileBlock 主要包括两部分：BlockHeader 和 BlockData。其中 BlockHeader 主要存储 block 元数据，BlockData 用来存储具体数据。block 元数据中最核心的字段是 BlockType 字段，用来标示该 block 块的类型，HBase 中定义了 8 种 BlockType，每种 BlockType 对应的 block 都存储不同的数据内容，有的存储用户数据，有的存储索引数据，有的存储 meta 元数据。对于任意一种类型的 HFileBlock，都拥有相同结构的 BlockHeader，但是 BlockData 结构却不相同。下面通过一张表简单罗列最核心的几种 BlockType，下文会详细针对每种 BlockType 进行详细的讲解：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930143151.png" alt="image-20200930143149155"></p><h4 id="Block块解析"><a href="#Block块解析" class="headerlink" title="Block块解析"></a>Block块解析</h4><h5 id="Trailer-Block"><a href="#Trailer-Block" class="headerlink" title="Trailer Block"></a>Trailer Block</h5><p>主要记录了 HFile 的基本信息、各个部分的偏移值和寻址信息，下图为 Trailer 内存和磁盘中的数据结构，其中只显示了部分核心字段：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930173514.png" alt="image-20200930173513541"></p><p>HFile 在读取的时候首先会解析 Trailer Block 并加载到内存，然后再进一步加载 LoadOnOpen 区的数据，具体步骤如下：</p><ol><li><p>首先加载 version 版本信息，HBase 中 version 包含 majorVersion 和 minorVersion 两部分，前者决定了 HFile 的主版本： V1、V2  还是 V3；后者在主版本确定的基础上决定是否支持一些微小修正，比如是否支持 checksum 等。不同的版本决定了使用不同的 Reader 对象对 HFile 进行读取解析。</p></li><li><p>根据 Version 信息获取 trailer 的长度（不同 version 的 trailer 长度不同），再根据 trailer 长度加载整个 HFileTrailer Block。</p></li><li><p>最后加载 load-on-open 部分到内存中，起始偏移地址是 trailer 中的 LoadOnOpenDataOffset 字段，load-on-open 部分的结束偏移量为 HFile 长度减去 Trailer 长度，load-on-open 部分主要包括索引树的根节点以及 FileInfo 两个重要模块，FileInfo 是固定长度的块，它纪录了文件的一些 Meta 信息，例如：AVG_KEY_LEN，AVG_VALUE_LEN，LAST_KEY, COMPARATOR，MAX_SEQ_ID_KEY 等。</p></li></ol><h5 id="Data-Block"><a href="#Data-Block" class="headerlink" title="Data Block"></a>Data Block</h5><p>DataBlock 是 HBase 中数据存储的最小单元。DataBlock 中主要存储用户的 KeyValue 数据（KeyValue 后面一般会跟一个 timestamp，图中未标出），而 KeyValue 结构是 HBase 存储的核心，每个数据都是以 KeyValue 结构在 HBase 中进行存储。KeyValue 结构在内存和磁盘中可以表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930173753.png" alt="image-20200930173751947"></p><p>每个 KeyValue 都由 4 个部分构成，分别为 key length，value length，key 和 value。其中 key value 和 value length 是两个固定长度的数值，而 key 是一个复杂的结构，首先是 rowkey 的长度，接着是 rowkey，然后是 ColumnFamily 的长度，再是 ColumnFamily，之后是 ColumnQualifier，最后是时间戳和 KeyType（keytype 有四种类型，分别是 Put、Delete、 DeleteColumn 和 DeleteFamily），value 就没有那么复杂，就是一串纯粹的二进制数据。</p><h5 id="Root-Index-Block"><a href="#Root-Index-Block" class="headerlink" title="Root Index Block"></a>Root Index Block</h5><p>Root Index Block 表示索引树根节点索引块，可以作为 bloom 的直接索引，也可以作为 data 索引的根索引。而且对于 single-level 和 mutil-level 两种索引结构对应的 Root Index Block 略有不同，本文以 mutil-level 索引结构为例进行分析（single-level 索引结构是 mutual-level 的一种简化场景），在内存和磁盘中的格式如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201009112745.png" alt="image-20201009112653096"></p><p>其中 Index Entry 表示具体的索引对象，每个索引对象由 3 个字段组成，Block Offset 表示索引指向数据块的偏移量，BlockDataSize 表示索引指向数据块在磁盘上的大小，BlockKey 表示索引指向数据块中的第一个 key。除此之外，还有另外 3 个字段用来记录 MidKey 的相关信息，MidKey 表示 HFile 所有 Data Block 中中间的一个 Data Block，用于在对 HFile 进行 split 操作时，快速定位 HFile 的中间位置。需要注意的是 single-level 索引结构和 mutil-level 结构相比，就只缺少 MidKey 这三个字段。</p><p>Root Index Block 会在 HFile 解析的时候直接加载到内存中，此处需要注意在 Trailer Block 中有一个字段为 dataIndexCount，就表示此处 Index Entry 的个数。因为 Index Entry 并不定长，只有知道 Entry 的个数才能正确的将所有 Index Entry 加载到内存。</p><h5 id="InterMediate-Index-Block-amp-Ieaf-Index-Block"><a href="#InterMediate-Index-Block-amp-Ieaf-Index-Block" class="headerlink" title="InterMediate Index Block &amp; Ieaf Index Block"></a>InterMediate Index Block &amp; Ieaf Index Block</h5><p>当 HFile 中 Data Block 越来越多，single-level 结构的索引已经不足以支撑所有数据都加载到内存，需要分化为 mutil-level 结构。mutil-level 结构中 NonRoot Index Block 作为中间层节点或者叶子节点存在，无论是中间节点还是叶子节点，其都拥有相同的结构，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201009112910.png" alt="23"></p><p>和 Root Index Block 相同，NonRoot Index Block 中最核心的字段也是 Index Entry，用于指向叶子节点块或者数据块。不同的是，NonRoot Index Block 结构中增加了 block 块的内部索引 entry Offset 字段，entry Offset 表示 index Entry 在该 block 中的相对偏移量（相对于第一个 index Entry)，用于实现 block 内的二分查找。所有非根节点索引块，包括 Intermediate index block 和 leaf index block，在其内部定位一个 key 的具体索引并不是通过遍历实现，而是使用二分查找算法，这样可以更加高效快速地定位到待查找 key。</p><h5 id="BloomFilter-Meta-Block-amp-Bloom-Block"><a href="#BloomFilter-Meta-Block-amp-Bloom-Block" class="headerlink" title="BloomFilter Meta Block &amp; Bloom Block"></a>BloomFilter Meta Block &amp; Bloom Block</h5><p>HBase 中每个 HFile 都有对应的 BloomFilter 位数组，KeyValue 在写入 HFile 时会先经过几个 hash 函数的映射，映射后将对应的数组位改为 1，get 请求进来之后再进行 hash 映射，如果在对应数组位上存在 0，说明该 get 请求查询的数据不在该 HFile 中。</p><p>HFile 中的位数组就是上述 Bloom Block 中存储的值，可以想象，一个 HFile 文件越大，里面存储的 KeyValue 值越多，位数组就会相应越大。一旦太大就不适合直接加载到内存了，因此 HFile V2 在设计上将位数组进行了拆分，拆成了多个独立的位数组（根据 Key 进行拆分，一部分连续的 Key 使用一个位数组）。这样一个 HFile 中就会包含多个位数组，根据 Key 进行查询，首先会定位到具体的某个位数组，只需要加载此位数组到内存进行过滤即可，减少了内存开支。</p><p>在结构上每个位数组对应 HFile 中一个 Bloom Block，为了方便根据 Key 定位具体需要加载哪个位数组，HFile V2 又设计了对应的索引 Bloom Index Block，对应的内存和逻辑结构图如下：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200930174456.png" alt="image-20200930174454244"></p><p>Bloom Index Block 结构中 totalByteSize 表示位数组的 bit 数，numChunks 表示 Bloom Block 的个数，hashCount 表示 hash 函数的个数，hashType 表示 hash 函数的类型，totalKeyCount 表示 bloom filter 当前已经包含的 key 的数目，totalMaxKeys 表示 bloom filter 当前最多包含的 key 的数目，Bloom Index Entry 对应每一个 bloom filter block 的索引条目，作为索引分别指向 ’scanned block section’ 部分的 Bloom Block，Bloom Block 中就存储了对应的位数组。</p><p>Bloom Index Entry 的结构见上图左边所示，BlockOffset 表 示对应 Bloom Block 在 HFile 中的偏移量，FirstKey 表示对应 BloomBlock 的第一个 Key。根据上文所说，一次 get 请求进来，首先会根据 key 在所有的索引条目中进行二分查找，查找到对应的 Bloom Index Entry，就可以定位到该 key 对应的位数组，加载到内存进行过滤判断。</p><h3 id="BlockCache"><a href="#BlockCache" class="headerlink" title="BlockCache"></a>BlockCache</h3><p>为了提升读取性能，HBase 也实现了一种读缓存结构 BlockCache。客户端读取某个 Block，首先会检查该 Block 是否存在于 BlockCache，如果存在就直接加载出来，如果不存在则去 HFile 文件中加载，加载出来之后放到 BlockCache 中，后续同一请求或者邻近数据查找请求可以直接从内存中获取，以避免昂贵的 IO 操作。</p><p>BlockCache 是 Region Server 级别的，一个 Region Server 只有一个 BlockCache，在 RegionServer 启动的时候完成 BlockCache 的初始化工作。到目前为止，HBase 先后实现了 3 种 Block Cache 方案，LRUBlockCache 是最初的实现方案，也是默认的实现方案；HBase 0.92 版本实现了第二种方案 SlabCache；HBase 0.96 之后官方提供了另一种可选方案 BucketCache。</p><p>这三种方案的不同之处在于对内存的管理模式，其中 LRUBlockCache 是将所有数据都放入 JVM Heap 中，交给 JVM 进行管理。而后两者采用了不同机制将部分数据存储在堆外，交给 HBase 自己管理。这种演变过程是因为 LRUBlockCache 方案中 JVM 垃圾回收机制经常会导致程序长时间暂停，而采用堆外内存对数据进行管理可以有效避免这种情况发生。</p><h4 id="LRUBlockCache"><a href="#LRUBlockCache" class="headerlink" title="LRUBlockCache"></a>LRUBlockCache</h4><p>HBase 默认的 BlockCache 实现方案。Block 数据块都存储在 JVM heap 内，由 JVM 进行垃圾回收管理。它将内存从逻辑上分为了三块：single-access 区、mutil-access 区、in-memory 区，分别占到整个 BlockCache 大小的 25%、50%、25%。一次随机读中，一个 Block 块从 HDFS 中加载出来之后首先放入 signle 区，后续如果有多次请求访问到这块数据的话，就会将这块数据移到 mutil-access 区。而 in-memory 区表示数据可以常驻内存，一般用来存放访问频繁、数据量小的数据，比如元数据，用户也可以在建表的时候通过设置列族属性 IN-MEMORY= true 将此列族放入 in-memory 区。很显然，这种设计策略类似于 JVM 中 young 区、old 区以及 perm 区。无论哪个区，系统都会采用严格的 Least-Recently-Used 算法，当 BlockCache 总量达到一定阈值之后就会启动淘汰机制，最少使用的 Block 会被置换出来，为新加载的 Block 预留空间。</p><h4 id="SlabCache"><a href="#SlabCache" class="headerlink" title="SlabCache"></a>SlabCache</h4><p>为了解决 LRUBlockCache 方案中因为 JVM 垃圾回收导致的服务中断，SlabCache 方案使用 Java NIO DirectByteBuffer 技术实现了堆外内存存储，不再由 JVM 管理数据内存。默认情况下，系统在初始化的时候会分配两个缓存区，分别占整个 BlockCache 大小的 80%和 20%，每个缓存区分别存储固定大小的 Block 块，其中前者主要存储小于等于 64K 大小的 Block，后者存储小于等于 128K Block，如果一个 Block 太大就会导致两个区都无法缓存。和 LRUBlockCache 相同，SlabCache 也使用 Least-Recently-Used 算法对过期 Block 进行淘汰。和 LRUBlockCache 不同的是，SlabCache 淘汰 Block 的时候只需要将对应的 bufferbyte 标记为空闲，后续 cache 对其上的内存直接进行覆盖即可。</p><p>线上集群环境中，不同表不同列族设置的 BlockSize 都可能不同，很显然，默认只能存储两种固定大小 Block 的 SlabCache 方案不能满足部分用户场景，比如用户设置 BlockSize = 256K，简单使用 SlabCache 方案就不能达到这部分 Block 缓存的目的。因此 HBase 实际实现中将 SlabCache 和 LRUBlockCache 搭配使用，称为 DoubleBlockCache。一次随机读中，一个 Block 块从 HDFS 中加载出来之后会在两个 Cache 中分别存储一份；缓存读时首先在 LRUBlockCache 中查找，如果 Cache Miss 再在 SlabCache 中查找，此时如果命中再将该 Block 放入 LRUBlockCache 中。</p><p>经过实际测试，DoubleBlockCache 方案有很多弊端。比如 SlabCache 设计中固定大小内存设置会导致实际内存使用率比较低，而且使用 LRUBlockCache 缓存 Block 依然会因为 JVM GC 产生大量内存碎片。因此在 HBase 0.98 版本之后，该方案已经被不建议使用。</p><h4 id="BucketCache"><a href="#BucketCache" class="headerlink" title="BucketCache"></a>BucketCache</h4><p>SlabCache 方案在实际应用中并没有很大程度改善原有 LRUBlockCache 方案的 GC 弊端，还额外引入了诸如堆外内存使用率低的缺陷。然而它的设计并不是一无是处，至少在使用堆外内存这个方面给予了阿里大牛们很多启发。站在 SlabCache 的肩膀上，他们开发了 BucketCache 缓存方案并贡献给了社区。</p><p>BucketCache 通过配置可以工作在三种模式下：heap，offheap 和 file。无论工作在那种模式下，BucketCache 都会申请许多带有固定大小标签的 Bucket，和 SlabCache 一样，一种 Bucket 存储一种指定 BlockSize 的数据块，但和 SlabCache 不同的是，BucketCache 会在初始化的时候申请 14 个不同大小的 Bucket，而且即使在某一种 Bucket 空间不足的情况下，系统也会从其他 Bucket 空间借用内存使用，不会出现内存使用率低的情况。接下来再来看看不同工作模式，heap 模式表示这些 Bucket 是从 JVM Heap 中申请，offheap 模式使用 DirectByteBuffer 技术实现堆外内存存储管理，而 file 模式使用类似 SSD 的高速缓存文件存储数据块。</p><p>实际实现中，HBase 将 BucketCache 和 LRUBlockCache 搭配使用，称为 CombinedBlockCache。和 DoubleBlockCache 不同，系统在 LRUBlockCache 中主要存储 Index Block 和 Bloom Block，而将 Data Block 存储在 BucketCache 中。因此一次随机读需要首先在 LRUBlockCache 中查到对应的 Index Block，然后再到 BucketCache 查找对应数据块。BucketCache 通过更加合理的设计修正了 SlabCache 的弊端，极大降低了 JVM GC 对业务请求的实际影响，但也存在一些问题，比如使用堆外内存会存在拷贝内存的问题，一定程度上会影响读写性能。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="http://hbasefly.com/2016/03/25/hbase-hfile/">HBase – 存储文件HFile结构解析</a></p><p><a href="http://hbasefly.com/2016/04/03/hbase_hfile_index/">HBase – 探索HFile索引机制</a></p><p><a href="http://hbasefly.com/2016/04/08/hbase-blockcache-1/">HBase BlockCache系列 – 走进BlockCache</a></p><p><a href="http://hbasefly.com/2016/04/26/hbase-blockcache-2/">HBase BlockCache系列 － 探求BlockCache实现机制</a></p>]]></content>
    
    
    <categories>
      
      <category>技术文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
      <tag>HLog</tag>
      
      <tag>MemStore</tag>
      
      <tag>HFile</tag>
      
      <tag>BlockCache</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法：Bloom Filter</title>
    <link href="/2020/09/27/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%EF%BC%9ABloom%20filter/"/>
    <url>/2020/09/27/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%EF%BC%9ABloom%20filter/</url>
    
    <content type="html"><![CDATA[<p>如何高效判断元素 w 是否存在于集合 A 之中？首先想到的答案是，把集合 A 中的元素一个个放到哈希表中，然后在哈希表中查一下 w 即可。但如果 A 中元素数量巨大，甚至数据量远远超过机器内存空间，该如何解决问题呢？实现一个基于磁盘和内存的哈希索引当然可以解决这个问题。而另一种低成本的方式就是借助 Bloom Filter（布隆过滤器）来实现。</p><a id="more"></a><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Bloom Filter（布隆过滤器）是一种高效利用空间的概率数据结构，由 Burton Howard Bloom 于 1970 年提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。Bloom Filter 可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。</p><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/bloomFilterWorkflow.png" alt="bloomFilterWorkflow"></p><p>以上图为例，具体的操作流程：假设集合里面有 3 个元素 {x, y, z}，哈希函数的个数为 3。</p><p>首先将位数组进行初始化，将里面每个位都设置位 0。对于集合里面的每一个元素，将元素依次通过 3 个哈希函数进行映射，每次映射都会产生一个哈希值，这个值对应位数组上面的一个点，然后将位数组对应的位置标记为 1。</p><p>查询 W 元素是否存在集合中的时候，同样的方法将 W 通过哈希映射到位数组上的 3 个点。如果 3 个点的其中有一个点不为 1，则可以判断该元素一定不存在集合中。反之，如果 3 个点都为 1，则该元素可能存在集合中。</p><p>注意：此处不能判断该元素是否一定存在集合中，可能存在一定的误判率。可以从图中可以看到：假设某个元素通过映射对应下标为 4，5，6 这 3 个点。虽然这 3 个点都为 1，但是很明显这 3 个点是不同元素经过哈希得到的位置，因此这种情况说明元素虽然不在集合中，也可能对应的都是 1，这是误判率存在的原因。</p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>Bloom Filter 基于位图的模式带来两个问题：</p><ul><li><p>误报（false positives）：</p><p>在查询时能提供“一定不存在”，但只能提供“可能存在”，因为存在其它元素被映射到部分相同 bit 位上，导致该位置 1，那么一个不存在的元素可能会被误报成存在。</p></li><li><p>漏报（false nagatives）：</p><p>如果删除了某个元素，导致该映射 bit 位被置 0，那么本来存在的元素会被漏报成不存在。由于后者问题严重得多，所以 Bloom Filter 必须确保”definitely no”从而容忍“probably yes”，不允许元素的删除。</p><p>关于元素删除的问题，一个改良方案是对 Bloom Filter 引入计数（Counting Bloom filters），但这样一来，原来每个 bit 空间就要扩张成一个计数值，空间效率上又降低了。</p></li></ul><h3 id="在-HBase-中的应用"><a href="#在-HBase-中的应用" class="headerlink" title="在 HBase 中的应用"></a>在 HBase 中的应用</h3><p>Bloom Filter 只需占用极小的空间，便可给出“可能存在”和“肯定不存在”的存在性判断。因此 HBase 可以在 Get 操作时通过使用 Bloom Filter 来过滤大量无效数据块，从而节省大量磁盘 IO。</p><p>HBase 中用户可以对某些列设置不同类型的 Bloom Filter，共有三种类型：</p><ul><li>NONE：关闭 Bloom Filter 功能。</li><li>ROW：按照 rowkey 来计算 Bloom Filter 的二进制串并存储。Get 查询的时候，必须带 rowkey，所以用户可以在建表时默认把 Bloom Filter 设置为 ROW 类型。</li><li>ROWCOL：按照 rowkey + family + qualifier 这 3 个字段拼出 byte[] 来计算 Bloom Filter 值并存储。如果在查询的时候，Get 能指定 rowkey、family、qualifier 这 3 个字段，则肯定可以通过 Bloom Filter 提升性能。但是如果在查询的时候，Get 中缺少 rowkey、family、qualifier 中任何一个字段，则无法通过 Bloom Filter 提升性能，因为计算 Bloom Filter 的 Key 不确定。</li></ul><p>一般意义上的 Scan 操作，HBase 都没法使用 Bloom Filter 来提升扫描数据性能。因为此时 Bloom Filter 的 Key 值不确定，所以没法计算出哈希值对比。但是，在某些特定场景下，Scan 操作同样可以借助 Bloom Filter 提升性能。</p><p>对于 ROWCOL 类型的 Bloom Filter 来说，如果在 Scan 操作中明确指定需要扫某些列。那么在 Scan 过程中，碰到 KV 数据从一行换到新的一行时，是没法走 ROWCOL 类型 Bloom Filter 的，因为新一行的 key 值不确定；但是，如果在同一行数据内切换列时，则能通过 ROWCOL 类型 Bloom Filter 进行优化，因为 rowkey 确定，同时 column 也已知，也就是说，Bloom Filter 中的 Key 确定，所以可以通过 ROWCOL 优化性能。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter 英文维基</a></p><p><a href="https://crossoverjie.top/2018/11/26/guava/guava-bloom-filter/">如何判断一个元素在亿级数据中是否存在？</a></p><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="https://coolshell.cn/articles/17225.html">CUCKOO FILTER：设计与实现（一种 Bloom Filter 的替代方案）</a></p>]]></content>
    
    
    <categories>
      
      <category>数据结构和算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
      <tag>Bloom filter</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法：LSM Tree</title>
    <link href="/2020/09/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%EF%BC%9ALSM%20tree/"/>
    <url>/2020/09/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%EF%BC%9ALSM%20tree/</url>
    
    <content type="html"><![CDATA[<p>LSM tree 全称是 Log-structured merge-tree，是一种分层，有序，面向磁盘的数据结构。其核心原理是磁盘批量顺序写比随机写性能高出很多，可以通过围绕这一原理进行设计和优化，让写性能达到最优。相较于传统的 B+树，它减少了磁盘随机读取的需求，从而在一定程度上改善了数据库的写能力，当然在一定程度上牺牲了数据库的读能力。LSM tree 也是当今流行的各种 NoSQL 或 NewSQL 数据库最基础的底层数据结构，广泛使用在包括 Hbase，Cassandra，Leveldb，RocksDB，TiDB 等项目中。</p><a id="more"></a><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>传统的 B+ 树的缺陷就是在访问节点时涉及到了大量的磁盘随机读写，因为你无法保证节点常驻内存，尤其是当 B+ 树管理的索引量很大的时候。这导致数据库读写性能急剧下降。</p><p> LSM tree 采取的做法就是通过引入多部件索引来减少磁盘随机读写的需求。在大量插入情况下我们周期性地选取两部分索引进行合并，并且把合并后的有序文件（或内存块）添加到磁盘尾部（或成为新文件），修改节点信息以保证索引树的正确和完整，并且周期性地回收失效索引。因此与其说 LSM tree 是一种树，不如说它是通过传统索引组织有序文件或内存块的一种方式。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/LSM_Tree.png" alt="LSM_Tree"></p><p>LSM tree 的节点可以分为两种：</p><ul><li>MemTable: 保存在内存中的部分，一般可以是红黑树、跳跃表，甚至可以是 B 树。在 HBase 中使用的是跳表，在 SQLite4 中使用的是只能追加写入的红黑树。</li><li>SSTable: 保存在磁盘上的部分，一般由多个内部 KeyValue 有序的文件组成，它的 key 和 value 都是任意的字节数组，并且了提供了按指定 key 查找和指定范围的 key 区间迭代遍历的功能。SSTable 内部包含了一系列可配置大小的 Block 块。关于这些 Block 块的 index 存储在 SSTable 的尾部，用于帮助快速查找特定的 Block。当一个 SSTable 被打开的时候，index 会被加载到内存，然后根据 key 在内存 index 里面进行一个二分查找，查到该 key 对应的磁盘的 offset 之后，然后去磁盘把响应的块数据读取出来。当然如果内存足够大的话，可以直接把 SSTable 直接通过 MMap 的技术映射到内存中，从而提供更快的查找。 </li></ul><p>写操作直接作用于 MemTable, 因此写入性能接近写内存。每层 SSTable 文件到达一定条件后，进行合并操作，然后放置到更高层。合并操作在实现上一般是策略驱动、可插件化的。</p><h3 id="读写流程"><a href="#读写流程" class="headerlink" title="读写流程"></a>读写流程</h3><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20201015144912.png" alt="LSM tree读写流程"></p><h4 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h4><ol><li>当收到一个写请求时，会先把该条数据记录在 WAL（Write-ahead logging）里面，用作故障恢复。</li><li>当写完 WAL 后，会把该条数据写入内存的 MemTable 里面（删除操作也通过写入实现，会写入一个删除标记；更新则是写入一条新记录）。</li><li>当 Memtable 超过一定的大小后，会在内存里面冻结，变成不可变的 Memtable，同时为了不阻塞写操作需要新生成一个 Memtable 继续提供服务。</li><li>把内存里面不可变的 Memtable 给 flush 到到硬盘上的 SSTable 层中，此步骤也称为 Minor Compaction，这里需要注意在 L0 层的 SSTable 是没有进行合并的，所以这里的 key range 在多个 SSTable 中可能会出现重叠，在层数大于 0 层之后的 SSTable，不存在重叠 key。</li><li>当每层的磁盘上的 SSTable 的体积超过一定的大小或者个数，也会周期的进行合并。此步骤也称为 Major Compaction。这个阶段会真正的清除掉被标记删除掉的数据以及多版本数据的合并，避免浪费空间，注意由于 SSTable 都是有序的，我们可以直接采用 merge sort 进行高效合并。</li></ol><h4 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h4><ol><li>当收到一个读请求的时候，会直接先在内存里面查询，如果查询到就返回。</li><li>内存查询包括服务中的 Memtable 和不可变的 Memtable，也包括对于 SSTable 的缓存 block cache。</li><li>如果内存中没有查询到就会依次下沉查询 SSTable，直到把所有的层次的 SSTable 查询一遍得到最终结果。</li></ol><h3 id="写放大、读放大和空间放大"><a href="#写放大、读放大和空间放大" class="headerlink" title="写放大、读放大和空间放大"></a>写放大、读放大和空间放大</h3><p>LSM Tree 将随机写转化为顺序写，而作为代价带来了大量的重复写入。由此会引起写放大、读放大和空间放大。</p><ul><li><p>写放大（Write Amplification） :</p><p>平均写入 1 个字节，引擎中在数据的声明周期内实际会写入 n 个字节，其写放大率是 n。如果业务方写入速度是 10MB/s，在引擎端或者操作系统层面能观察到的数据写入速度是 30MB/s，系统的写放大率就是 3。写放大过大会制约系统的实际吞吐。对于 SSD 来说，也会导致 SSD 寿命缩短。</p><p>以下是 HBase 中的写放大示意图</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/写放大.png" alt="写放大示意图"></p></li><li><p>读放大（Read Amplification ）: </p><p>一个读请求，系统所需要读 n 个页面来完成查询，其读放大率是 n。逻辑上的读操作可能会命中引擎内部的 cache 或者文件系统 cache，命中不了 cache 就会进行实际的磁盘 IO，命中 cache 的读取操作的代价虽然很低，但是也会消耗 CPU。</p><p>以下是 HBase 中的读放大示意图</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/读放大.jpg" alt="读放大"></p></li><li><p>空间放大（Space Amplification）:</p><p>平均存储 1 个字节的数据，在存储引擎内部所占用的磁盘空间 n 个字节，其空间放大是 n。比如写入 10MB 的数据，磁盘上实际占用了 100MB，这是空间放大率就是 10。空间放大和写放大在调优的时候往往是排斥的，空间放大越大，那么数据可能不需要频繁的 compaction，其写放大就会降低；如果空间放大率设置的小，那么数据就需要频繁的 compaction 来释放存储空间，导致写放大增大。</p></li></ul><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>LSM tree 一般从以下几个方面进行优化：</p><ol><li><p>压缩</p><p>SSTable 是可以启用压缩功能的，并且这种压缩不是将整个 SSTable 一起压缩，而是根据 locality 将数据分组，每个组分别压缩，这样的好处当读取数据的时候，我们不需要解压缩整个文件而是解压缩部分 Group 就可以读取。</p></li><li><p>缓存</p><p>因为 SSTable 在写入磁盘后，除了 Compaction 之外，是不会变化的，所以我可以将 Scan 的 Block 进行缓存，从而提高检索的效率。</p></li><li><p>Bloom filter</p><p>正常情况下，一个读操作是需要读取所有的 SSTable 将结果合并后返回的，但是对于某些 key 而言，有些 SSTable 是根本不包含对应数据的，因此，我们可以对每一个 SSTable 添加 Bloom Filter，因为 Bloom Filter 在判断一个 SSTable 不存在某个 key 的时候，那么就一定不会存在，利用这个特性可以减少不必要的磁盘扫描。</p></li><li><p>合并</p><p>通过定期合并瘦身， 可以有效的清除无效数据，缩短读取路径，提高磁盘利用空间。但 Compaction 操作是非常消耗 CPU 和磁盘 IO 的，尤其是在业务高峰期，如果发生了 Major Compaction，则会降低整个系统的吞吐量，这也是在使用一些 NoSQL 数据库时，比如 Hbase，常常会禁用 Major Compaction，并在凌晨业务低峰期进行合并的原因。</p></li></ol><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">LSM tree 英文维基</a></p><p><a href="https://cloud.tencent.com/developer/article/1441835">深入理解什么是LSM-Tree</a></p><p><a href="https://swanspouse.github.io/2019/01/28/LSM-Tree/">LSM Tree</a></p><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="https://zhuanlan.zhihu.com/p/32225460">PebblesDB读后感</a></p><p><a href="https://darionyaphet.github.io/2018/10/06/RocksDB/">RocksDB</a></p><p><a href="https://zhuanlan.zhihu.com/p/65557081">LSM-tree减少写放大的一些策略</a></p>]]></content>
    
    
    <categories>
      
      <category>数据结构和算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
      <tag>LSM tree</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据结构和算法：Skip List</title>
    <link href="/2020/09/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%EF%BC%9ASkip%20List/"/>
    <url>/2020/09/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%EF%BC%9ASkip%20List/</url>
    
    <content type="html"><![CDATA[<p>Skip List（跳表）是一种能高效实现插入、删除、查找的内存数据结构。常用来对标 AVL tree 或红黑树等二叉查找树。与红黑树以及其他的二分查找树相比，跳跃表的优势在于实现简单，而且在并发场景下加锁粒度更小，从而可以实现更高的并发性。因此在一些热门项目中，如 Redis、LevelDB、HBase 都把跳跃表作为一种维护有序数据集合的基础数据结构。JDK 中也内置了基于 Skip List 实现的并发容器 ConcurrentSkipListMap 和 ConcurrentSkipListSet。</p><a id="more"></a><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>对于一个有序链表，如果要搜索一个数，需要从头到尾比较每个元素是否匹配，直到找到匹配的数为止，即时间复杂度是 $O(n)$。同理，插入或删除一个数并保持链表有序，需要先找到合适的插入或删除位置，再执行插入或删除，总计也是 $O(n)$ 的时间。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200924175020101.png" alt="image-20200924175020101"></p><p>如果链表在查找的时候，能够避免依次查找元素，那么查找复杂度将降低。而跳跃表就是利用这一思想，在链表之上额外存储了一些节点的索引信息，达到避免依次查找元素的目的，从而将查询复杂度优化为 $O(\log{}n)$。将查询复杂度优化之后，自然也优化了插入和删除的复杂度。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200924175049127.png" alt="image-20200924175049127"></p><p>我们新创建一个链表，它包含的元素为前一个链表的偶数个元素。这样在搜索一个元素时，我们先在上层链表进行搜索，当元素未找到时再到下层链表中搜索。我们以搜索数字 19 为例，先在上层中搜索，到达节点 17 时发现下一个节点为 21，已经大于 19，于是转到下一层搜索，找到的目标数字 19。</p><p>我们知道上层的节点数目为 $n/2$，因此，有了这层索引，我们搜索的时间复杂度降为了：$O(n/2)$。同理，我们可以不断地增加层数，来减少搜索的时间。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200924175111959.png" alt="image-20200924175111959"></p><p>一般地，如果有 k 层，我们需要的搜索次数会小于 $\lceil \frac{n}{2^k} \rceil + k$，这样当层数 k 增加到 $\lceil \log_{2} n \rceil$ 时，搜索的时间复杂度就变成了 $O(\log{}n)$。其实这背后的原理和二叉搜索树或二分查找很类似，通过索引来跳过大量的节点，从而提高搜索效率。</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li>跳跃表由多条分层的链表组成。</li><li>每条链表中的元素都是有序的。</li><li>每条链表都有两个元素：+∞（正无穷大）和-∞（负无穷大），分别表示链表的头部和尾部。</li><li>上层链表元素集合是下层链表元素集合的子集。</li><li>从上到下，上层链表元素集合是下层链表元素集合的子集。</li><li>跳跃表的高度定义为水平链表的层数。</li></ul><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><h4 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h4><ul><li>以左上角元素为起点。</li><li>如果当前节点的后继节点的值小于等于待查询值，则沿着这条链表向后查询，否则，切换到当前节点的下一层链表。</li><li>继续查询，直到找到待查询值或者当前节点为空。</li></ul><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><ul><li>先搜索得到该节点，把节点和对应的所有索引节点全部删除。</li><li>搜索过程中可以记录下路径，表面删除索引层节点时重复搜索。</li></ul><h4 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h4><ul><li><p>上面的示例中简单设置了上层节点和下层节点的个数是 $1:2 $，以此决定跳表的高度。</p></li><li><p>在实际使用中，为了便于实现，采用随机算法决定每个节点的索引层数。</p></li><li><p>伪代码如下：</p><pre><code class="hljs c">randomLevel()    lvl := <span class="hljs-number">1</span>    -- random() that returns a random value in [<span class="hljs-number">0.</span>.<span class="hljs-number">.1</span>)    <span class="hljs-keyword">while</span> random() &lt; p <span class="hljs-keyword">and</span> lvl &lt; MaxLevel <span class="hljs-keyword">do</span>        lvl := lvl + <span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> lvl</code></pre></li><li><p>上面的伪代码相当于抛硬币，如果是正面则层数加一，直到抛出反面为止。其中的 $MaxLevel$ 是防止如果运气太好，层数就会太高，而太高的层数往往并不会提供额外的性能，一般 $MaxLevel=log_{1/p}{n}$， $p=1/4$ 或者 $1/2$。</p></li></ul><h3 id="性能分析"><a href="#性能分析" class="headerlink" title="性能分析"></a>性能分析</h3><p>在最坏的情况下，所有节点都没有创建索引，时间复杂度为 $O(n)$。但在平均情况下，搜索的时间复杂度却是 $O(\log{}n)$。一些严格的证明会涉及到比较复杂的概率统计学知识，以下只做简单说明。</p><ul><li><p>一个节点落在第 $k$ 层的概率为 $p^{k-1}$。</p></li><li><p>一个最底层链表有 $n$ 个元素的跳跃表，总共元素个数为 $\underset{k=1}{Σ} n×p^{k-1}$，其中 $k$ 为跳跃表的高度。</p><p>一个元素落在第 $k$ 层概率为 $p^{k-1}$，则第 $k$ 层插入的元素个数为 $n×p^{k-1}$，所有 $k$ 相加得到上述公式。当 $p≤ \frac{1}{2}$时，上述公式小于  $O(2n)$，所以空间复杂度为 $O(n)$。</p></li><li><p>跳跃表的高度为 $O(\log{}n)$。</p></li><li><p>跳跃表的查询时间复杂度为 $O(\log{}n)$。</p><p>为了计算搜索的时间复杂度，我们可以将查找的过程倒过来，从搜索最后的节点开始，一直向左或向上，直到最顶层。如下图，在路径上的每一点，都可能有两种情况：</p><ol><li>节点有上一层的节点，向上。这种情况出现的概率是 $p$。</li><li>节点没有上一层的节点，向左。出现的概率是 $1-p$。</li></ol><p>于是，设 <code>C(k)</code> 为反向搜索爬到第 <code>k</code> 层的平均路径长度，则有：</p><pre><code class="hljs lisp">C(<span class="hljs-number">0</span>) = <span class="hljs-number">0</span>C(<span class="hljs-name">k</span>) = p * (情况<span class="hljs-number">1</span>) + (<span class="hljs-number">1</span>-p) * (情况<span class="hljs-number">2</span>)</code></pre><p>将两种情况也用 <code>C</code> 代入，有：</p><pre><code class="hljs lisp">C(<span class="hljs-name">k</span>) = p*(<span class="hljs-number">1</span> + C(<span class="hljs-name">k</span>–<span class="hljs-number">1</span>)) + (<span class="hljs-number">1</span>–p)*(<span class="hljs-number">1</span> + C(<span class="hljs-name">k</span>))C(<span class="hljs-name">k</span>) = C(<span class="hljs-name">k</span>–<span class="hljs-number">1</span>) + <span class="hljs-number">1</span>/pC(<span class="hljs-name">k</span>) = k/p</code></pre><p>我们知道跳表的最大层数为 $O(\log{}n)$，因此，搜索的复杂度 $O(\log{}n)/p=O(\log{}n)$。</p></li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://lotabout.me/2018/skip-list/">跳表──没听过但很犀利的数据结构</a></p><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p><p><a href="https://xiaobaoqiu.github.io/blog/2014/12/19/javabing-fa-rong-qi-zhi-skiplist/">Java并发容器之SkipList</a></p>]]></content>
    
    
    <categories>
      
      <category>数据结构和算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
      <tag>Skip List</tag>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase 学习：安装和使用</title>
    <link href="/2020/09/02/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <url>/2020/09/02/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>本文简单介绍单机环境下 HBase 的安装和使用。</p><a id="more"></a><h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>HBase 依赖 Hadoop，需要先安装 Hadoop 并配置。实现完全分布式配置还需要安装 ZooKeeper。</p><p>Hadoop 和 HBase 均有三种模式，Standalone、Pseudo-Distributed（伪分布式）和 Fully-Distributed。开发机资源有限，安装模式选用伪分布式。</p><h4 id="安装配置-Hadoop"><a href="#安装配置-Hadoop" class="headerlink" title="安装配置 Hadoop"></a>安装配置 Hadoop</h4><ul><li><p>下载最新稳定版并解压，本次安装使用版本为 3.2.1</p></li><li><p>配置文件</p><ul><li><p>编辑 etc/hadoop/core-site.xml</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://localhost:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre></li></ul></li><li><ul><li><p>编辑 etc/hadoop/hdfs-site.xml</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre></li><li><p>编辑配置 etc/hadoop/hadoop-env.sh</p><pre><code class="hljs sh"><span class="hljs-comment">#配置JAVA_HOME</span><span class="hljs-built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Hom</code></pre></li></ul></li></ul><ul><li><p>配置 ssh 免密登录本机</p><pre><code class="hljs sh">$ ssh localhost<span class="hljs-comment">#如果需要输入密码，则做以下免密配置</span>$ ssh-keygen -t rsa -P <span class="hljs-string">&#x27;&#x27;</span> -f ~/.ssh/id_rsa$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys$ chmod 0600 ~/.ssh/authorized_keys</code></pre></li></ul><ul><li><p>启动 sbin/start-all.sh，可以通过 JPS 查看进程</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/Hadoop启动后进程.png" alt="Hadoop启动后进程"></p></li></ul><h4 id="安装配置-HBase"><a href="#安装配置-HBase" class="headerlink" title="安装配置 HBase"></a>安装配置 HBase</h4><ul><li><p>下载最新稳定版并解压，本次安装使用版本为 2.2.4</p></li><li><p>配置文件</p><ul><li><p>配置 conf/hbase-site.xml</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.rootdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><span class="hljs-comment">&lt;!-- hdfs配置和hadoop配置对应 --&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><span class="hljs-comment">&lt;!-- 本地目录 --&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/Users/wangrui/zookeeper<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>      Controls whether HBase will check for stream capabilities (hflush/hsync).      Disable this if you intend to run on LocalFileSystem, denoted by a rootdir      with the &#x27;file://&#x27; scheme, but be mindful of the NOTE below.      WARNING: Setting this to false blinds you to potential data loss and      inconsistent system state in the event of process and/or node failures. If      HBase is complaining of an inability to use hsync or hflush it&#x27;s most      likely not a false positive.    <span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.cluster.distributed<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre></li></ul></li></ul><ul><li><p>配置 conf/hbase-env.sh</p><pre><code class="hljs sh"><span class="hljs-comment">#配置JAVA_HOME</span><span class="hljs-built_in">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home<span class="hljs-comment">#使用自带zookeeper</span><span class="hljs-built_in">export</span> HBASE_MANAGES_ZK=<span class="hljs-literal">true</span></code></pre></li></ul><ul><li><p>启动 bin/start-hbase.sh</p><ul><li><p>通过 JPS 可以查到进程</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200902103806676.png" alt="image-20200902103806676"></p></li><li><p>也可以访问 <a href="http://localhost:16010/">http://localhost:16010/</a> 查看 HBase 状态</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/image-20200902104217904.png" alt="image-20200902104217904"></p></li></ul></li></ul><h3 id="HBase-Shell"><a href="#HBase-Shell" class="headerlink" title="HBase Shell"></a>HBase Shell</h3><ul><li>进入 HBase 命令行：<code>hbase shell</code></li><li>查看版本：<code>version</code></li><li>列出所有的 namespace：<code>list_namespace</code></li><li>查看有哪些表：<code>list</code></li><li>建表：<code>create &#39;表名&#39;,&#39;列族&#39;, &#39;列族&#39;</code>，示例 <code>create &#39;person&#39;,&#39;name&#39;, &#39;age&#39;</code></li><li>禁用表：<code>disable &#39;表名&#39;</code></li><li>删除表：<code>drop &#39;表名&#39;</code></li><li>插入记录：<code>put  &#39;&lt;table name&gt;&#39;, &#39;&lt;row&gt;&#39;, &#39;&lt;column name&gt;&#39;, &#39;&lt;value&gt;&#39;</code></li><li>删除记录<ul><li>删除某个属性的记录：<code>delete &#39;&lt;table name&gt;&#39;,  &#39;&lt;row&gt;&#39;,  &#39;&lt;column name&gt;&#39;,  &#39;&lt;time stamp&gt;&#39;</code></li><li>删除行：<code>deleteall &#39;&lt;table_name&gt;&#39;, &#39;&lt;row&gt;&#39;</code></li></ul></li><li>查询记录：<code>get</code></li><li>修改记录：同插入</li><li>扫描：查看 HTable 数据。使用 scan 命令可以得到表中的数据。<code>scan &#39;&lt;table name\&gt;&#39;</code></li><li>统计行数：<code>count</code></li><li>清空表：<code>truncate</code></li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">Hadoop: Setting up a Single Node Cluster.</a></p><p><a href="https://hbase.apache.org/book.html#quickstart">Quick Start - Standalone HBase</a></p>]]></content>
    
    
    <categories>
      
      <category>技术文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HBase 学习：基础知识</title>
    <link href="/2020/08/20/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2020/08/20/HBase%20%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<p>本文主要介绍 HBase 的设计目标、相关生态、数据模型和系统架构。</p><a id="more"></a><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>HBase 是高可靠性、高性能、面向列、可伸缩、可实时读写的分布式数据库，最初是 Google Bigtable 论文的开源实现。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/BigTable和HBase对照.png" alt="image-20200901155317587"></p><h3 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h3><ul><li><p>Scalability：HBase 底层基于 HDFS，支持扩展，并且可以随时添加或者减少节点</p></li><li><p>High Performance：底层的 LSM-Tree 数据结构，使得 HBase 具备非常高的写入性能。RowKey 有序排列、主键索引和缓存机制使得 HBase 具备一定的随机读写性能。</p></li><li><p>High Availability：基于 zookeeper 的协调服务，能够保证服务的高可用行。HBase 使用 WAL 和 replication 机制，前者保证数据写入时不会因为集群异常而导致写入数据的丢失，后者保证集群出现严重问题时，数据不会发生丢失和损坏。</p></li></ul><h3 id="相关生态"><a href="#相关生态" class="headerlink" title="相关生态"></a>相关生态</h3><p>对于 HBase 不适用的场景，可以借助于强大的生态圈，架设 Phoenix、Spark 或者其他第三方组件，就可以有效地扩展 HBase 的使用场景。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/HBase生态.png" alt="HBase生态"></p><ul><li>Spark：基于内存计算的大数据计算引擎，提供海量数据的离线分析计算能力和实时流计算能力</li><li>Phoenix：关系型数据库引擎，提供操作 HBase 的 SQL 接口，支持聚合运算，支持具有完整 ACID 语义的跨行及跨表事务，支持二级索引</li><li>OpenTSDB：时序数据存储，提供基于 Metrics、时间和标签的一些组合维度查询与聚合能力</li><li>GeoMesa：时空数据存储，提供基于时间和空间范围的索引能力</li><li>JanusGraph：图数据存储，提供基于属性、关系的图索引能力</li></ul><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><h4 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h4><ul><li><p>Table（表格）</p><ul><li>A Bigtable is a sparse, distributed, persistent multi-dimensional sorted map.</li><li>一个表是一个包含海量 Key-Value 对的 Map，数据是持久化存储的。</li><li>这个大的 Map 需要支持多个分区来实现分布式。</li><li>这个 Map 按照 Key 进行排序，这个 Key 是一个由 {Row Key, Column Key, Timestamp} 组成的多维结构。</li><li>每一行列的组成并不是严格的结构，而是稀疏的，也就是说，行与行可以由不同的列组成</li></ul></li><li><p>Row（行）</p><ul><li>每一行数据都拥有一个唯一的 RowKey，可以将 Rowkey 理解为主键。</li><li>RowKey 是一个 Byte String，通常长度在 10～100Bytes 左右，建议不超过 4KB，最大为 64KB。一行中包含一个或多个列。</li><li>Bigtable 支持 Row 级别操作的原子性。</li><li>所有的数据按照 Row Key 的字典顺序进行排序。</li><li>数据的存储目标是相近的数据存储到一起。一个常用的行的 key 的格式是网站域名。如果你的行的 key 是域名，你应该将域名进行反转 (org.apache.www, org.apache.mail, org.apache.jira) 再存储。这样的话，所有 Apache 域名将会存储在一起，好过基于子域名的首字母分散在各处。</li></ul></li><li><p>Column（列）</p><ul><li>HBase 中列的组成结构为 Family:Qualifier</li></ul></li><li><p>Column Family（列族）</p><ul><li>权限控制的最小单元。</li><li>HBase 把同一列族里面的数据存储在同一目录下，由几个文件保存。</li><li>一个 Column Family 通常是一个或多个相同类型的列的集合，这样在数据压缩率上可以获取更好的效果。</li><li>Column Families 的数量通常不建议过多，通常小于 5 个。</li><li>每一个列族拥有一系列的存储属性，例如值是否缓存在内存中，数据是否要压缩或者他的行 key 是否要加密等等</li><li>表格中的每一行拥有相同的列族，尽管一个给定的行可能没有存储任何数据在一个给定的列族中。</li></ul></li><li><p>Column Qualifier（列的限定符）</p><ul><li>列的限定符是列族中数据的索引。</li><li>例如给定了一个列族 content，那么限定符可能是 content:html，也可以是 content:pdf。</li><li>HBase 表中的每个列都归属于某个列族，列族必须作为表模式 (schema) 定义的一部分预先给出。如 create ‘test’, ‘course’。</li><li>列名以列族作为前缀，每个“列族”都可以有多个列成员 (column)。如 course:math, course:english，新的列族成员（列）可以随后按需、动态加入。</li></ul></li><li>Cell（单元）<ul><li>由行和列的坐标交叉决定。</li><li>单元格是有版本的。</li><li>单元格的内容是未解析的字节数组。</li><li>单元格是由行、列族、列限定符、值和代表值版本的时间戳组成的（{row key， column( = +)， version} ）唯一确定单元格。</li><li>cell 中的数据是没有类型的，全部是字节码形式存储。</li></ul></li><li>Timestamp（时间戳）<ul><li>时间戳是写在值旁边的一个用于区分值的版本的数据。</li><li>默认情况下，时间戳表示的是当数据写入时 RegionSever 的时间点，但你也可以在写入数据时指定一个不同的时间戳。</li><li>在 HBase 每个 cell 存储单元对同一份数据有多个版本，根据唯一的时间戳来区分每个版本之间的差异，不同版本的数据按照时间倒序排序，最新的数据版本排在最前面。</li><li>时间戳的类型是 64 位整型。时间戳可以由 HBase（在数据写入时自动）赋值，此时时间戳是精确到毫秒的当前系统时间。时间戳也可以由客户显式赋值，如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。</li></ul></li><li>Region（域）<ul><li>BigTable 中称为 Tablet</li><li>HBase 自动把表水平划分成多个区域（Region），每个 Region 会保存一个表里面某段连续的数据。</li><li>Region 是数据分布与负载均衡的基本单元。</li><li>每个表一开始只有一个 Region，一个 Region 增长到一定大小之后可以自动分裂成两个 Region。</li><li>多个 Region 可以合并成一个大的 Region。</li></ul></li></ul><h3 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h3><h4 id="逻辑视图"><a href="#逻辑视图" class="headerlink" title="逻辑视图"></a>逻辑视图</h4><p>一个名为 webable 的表格，表格中有两行（com.cnn.www 和 com.example.www）和三个列族（contents, anchor 和 people）。在这个例子当中，第一行（com.cnn.www）中 anchor 包含两列（anchor:cssnsi.com， anchor:my.look.ca）和 content 包含一列（contents:html）。这个例子中 com.cnn.www 拥有 5 个版本而 com.example.www 有一个版本。contents:html 列中包含给定网页的整个 HTML。anchor 限定符包含能够表示行的站点以及链接中文本。people 列族表示跟站点有关的人。</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/HBase逻辑视图.png" alt="HBase逻辑视图"></p><h4 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h4><p>尽管一个概念层次的表格可能看起来是由一些列稀疏的行组成，但他们是通过列族来存储的。一个新建的限定符 （column_family:column_qualifier）可以随时地添加到已存在的列族中。</p><p>列族 anchor 的所有数据存储在一起：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/列族存储示例1.png" alt="列族存储示例1"></p><p>列族 contents 的所有数据存储在一起：</p><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/20200901162726.png" alt="列族存储示例2"></p><p>在 HBase 中，表格中的单元如果是空将不占用空间或者事实上不存在。因此对于返回时间戳为 t8 的 contents:html 的值的请求，结果为空。同样的，一个返回时间戳为 t9 的 anchor:my.look.ca 的值的请求，结果也为空。如果没有指定时间戳的话，会返回特定列的最新值。对有多个版本的列，优先返回最新的值，因为时间戳是按照递减顺序存储的。因此对于一个返回 com.cnn.www 里面所有的列的值并且没有指定时间戳的请求，返回的结果会是时间戳为 t6 的 contents:html 的值、时间戳 t9 的 anchor:cnnsi.com 的值和时间戳 t8 的 anchor:my.look.ca。</p><h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p><img src="https://cdn.jsdelivr.net/gh/popesaga/img/img/HBase体系结构.png" alt="HBase体系结构"></p><h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><ul><li>包含访问 HBase 的接口并维护 cache 来加快对 HBase 的访问。</li><li>HBase 客户端访问数据行之前，首先需要通过元数据表定位目标数据所在 RegionServer，之后才会发送请求到该 RegionServer。</li><li>同时这些元数据会被缓存在客户端本地，以方便之后的请求访问。</li><li>如果集群 RegionServer 发生宕机或者执行了负载均衡等，从而导致数据分片发生迁移，客户端需要重新请求最新的元数据并缓存在本地。</li></ul><h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><ul><li>保证任何时候，集群中只有一个工作状态的 master。</li><li>存储 HBase 的 schema 和 table 元数据。</li><li>存储所有 Region 的寻址入口。</li><li>实时监控 RegionServer 的上线和下线信息,并实时通知 Master。</li><li>通过心跳可以感知到 RegionServer 是否宕机，并在宕机后通知 Master 进行宕机处理。</li><li>HBase 中对一张表进行各种管理操作（比如 alter 操作）需要先加表锁，防止其他用户对同一张表进行管理操作，造成表状态不一致。和其他 RDBMS 表不同，HBase 中的表通常都是分布式存储，ZooKeeper 可以通过特定机制实现分布式表锁。</li></ul><h4 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h4><ul><li>处理用户的各种管理请求，包括建表、修改表、权限操作、切分表、合并数据分片以及 Compaction 等。</li><li>管理集群中所有 RegionServer，包括为 RegionServer 分配 region，负责 RegionServer 的负载均衡、RegionServer 的宕机恢复以及 Region 的迁移等。</li><li>清理过期日志以及文件，Master 会每隔一段时间检查 HDFS 中 HLog 是否过期、HFile 是否已经被删除，并在过期之后将其删除。</li></ul><h4 id="RegionServer"><a href="#RegionServer" class="headerlink" title="RegionServer"></a>RegionServer</h4><ul><li>主要用来响应用户的 IO 请求，是 HBase 中最核心的模块，由 WAL（HLog）、BlockCache 以及多个 Region 构成。</li><li>WAL 用来保证数据写入的可靠性。</li><li>BlockCache 可以将数据块缓存在内存中以提升数据读取性能。</li><li>Region 是 HBase 中数据表的一个数据分片，一个 RegionServer 上通常会负责多个 Region 的数据读写。</li><li>一个 Region 由多个 Store 组成，每个 Store 存放对应列族的数据，比如一个表中有两个列族，这个表的所有 Region 就都会包含两个 Store。</li><li>每个 Store 包含一个 MemStore 和多个 HFile，用户数据写入时会将对应列族数据写入相应的 MemStore，一旦写入数据的内存大小超过设定阈值，系统就会将 MemStore 中的数据落盘形成 HFile 文件。</li><li>HFile 存放在 HDFS 上，是一种定制化格式的数据存储文件，方便用户进行数据读取。</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://book.douban.com/subject/34819650/">HBase原理与实践</a></p>]]></content>
    
    
    <categories>
      
      <category>技术文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HBase</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>写作工具配置：Typora + PicGo + GitHub 图床 + Textlint</title>
    <link href="/2020/08/02/%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE%EF%BC%9ATypora%20+%20PicGo%20+%20GitHub%20%E5%9B%BE%E5%BA%8A%20+%20Textlint/"/>
    <url>/2020/08/02/%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE%EF%BC%9ATypora%20+%20PicGo%20+%20GitHub%20%E5%9B%BE%E5%BA%8A%20+%20Textlint/</url>
    
    <content type="html"><![CDATA[<p>工欲善其事，必先利其器</p><a id="more"></a><h3 id="写作指南"><a href="#写作指南" class="headerlink" title="写作指南"></a>写作指南</h3><h4 id="写作工具"><a href="#写作工具" class="headerlink" title="写作工具"></a>写作工具</h4><ul><li>支持 Markdown 的文本编辑器<ul><li>常见的文本编辑器如 VS Code 、Sublime Text 都可以原生或者通过扩展支持 MarkDown。也可以选择专门的 MarkDown 编辑器：<a href="https://www.v1tx.com/post/best-markdown-editor/#typora">7款好用的 MarkDown 编辑器推荐</a>。</li><li>选择文本编辑器写作，就必须要熟悉 MarkDown 的语法。好在 MarkDown 的语法比较简单，号称五分钟就能上手，剩下的就多写多练了。</li></ul></li><li>富文本编辑器<ul><li>语雀、简书等</li><li>这里所说的富文本编辑器主要指所见即所得，无须使用 MarkDown 语法就可以使用的编辑器，一般是在线编辑器。好像曾经有道云笔记曾经支持过，但是现在已经没有了。现在可以选择的有简书（好像也差不多没法用了）和语雀。好处是可以省掉学习 MarkDown 语法的过程。而且一般还有一些功能上的扩展，比如贴图即上传到对应服务器的图床上。如何解决图床问题也是博客写作的一个门槛，但是使用富文本编辑器就可以直接上传到对应编辑器的服务器上。</li></ul></li></ul><h4 id="工具选择"><a href="#工具选择" class="headerlink" title="工具选择"></a>工具选择</h4><p>之前在阿里工作的时候，内部就使用过语雀，用的还比较习惯。也看到有人写平时都用语雀编辑完导出再上传到博客。不过都已经开始自己折腾了博客了，这样搞感觉还不如就拿语雀做笔记了，导来导去有点多此一举。我也有心想学一下 MarkDown。</p><p>综合各种推荐，特别是 Mac 环境下，最终选择 Typora 做编辑器。Typora 最大的特点就是可以所见即所得，另外对图片插入支持较好。除此之外的其他特点可以参考介绍：<a href="https://sspai.com/post/54912">Typora 完全使用详解</a> 和 <a href="https://www.typora.net/">官网</a>。</p><h4 id="MarkDown-语法"><a href="#MarkDown-语法" class="headerlink" title="MarkDown 语法"></a>MarkDown 语法</h4><ul><li><a href="https://www.jianshu.com/p/191d1e21f7ed">Markdown 基本语法</a></li><li><a href="[https://yuhongjun.github.io/tech/2017/05/02/Markdown-%E8%AF%AD%E6%B3%95%E6%89%8B%E5%86%8C-%E5%AE%8C%E6%95%B4%E6%95%B4%E7%90%86%E7%89%88.html#33-%E8%87%AA%E5%8A%A8%E9%93%BE%E6%8E%A5](https://yuhongjun.github.io/tech/2017/05/02/Markdown-语法手册-完整整理版.html#33-自动链接">Markdown 语法手册 （完整整理版）</a>)</li><li><a href="https://markdown-zh.readthedocs.io/en/latest/">Markdown 中文文档</a></li></ul><h4 id="排版"><a href="#排版" class="headerlink" title="排版"></a>排版</h4><p>排版主要参考：<a href="https://github.com/sparanoid/chinese-copywriting-guidelines/blob/master/README.zh-CN.md">中文文案排版指北</a>。</p><p>在写作时需要在全半角符号之间大量添加空格，手动添加可能会累死。因此需要一款工具可以自动添加空格，目前使用的是 Textlint。</p><p>Textlint 安装：</p><pre><code class="hljs bash">npm install textlint --globalnpm install textlint-rule-ja-space-between-half-and-full-width --globaltextlint --init</code></pre><p>会在当前目录生成 <code>.textlintrc</code> 文件，更新为以下内容。</p><pre><code class="hljs javascript">&#123;    <span class="hljs-string">&quot;filters&quot;</span>: &#123;&#125;,    <span class="hljs-string">&quot;rules&quot;</span>: &#123;        <span class="hljs-string">&quot;ja-space-between-half-and-full-width&quot;</span>: &#123;            <span class="hljs-string">&quot;space&quot;</span>: <span class="hljs-string">&quot;always&quot;</span>        &#125;    &#125;&#125;</code></pre><p>使用 Textlint 检查：</p><pre><code class="hljs bash">textlint *.md</code></pre><p>使用 Textlint 自动修复：</p><pre><code class="hljs bash">textlint --fix *.md</code></pre><h3 id="图床"><a href="#图床" class="headerlink" title="图床"></a>图床</h3><p>MarkDown 写文档要想写的爽，另外一个要解决的问题就是如何插入图片。可以选择直接在资源目录下配置图片，用相对路径访问，而更好的解决方式是使用图床。图床的选择又是个纠结的过程，下面会结合使用的工具介绍如何选择。</p><h4 id="图床选择"><a href="#图床选择" class="headerlink" title="图床选择"></a>图床选择</h4><ul><li><a href="https://zhuanlan.zhihu.com/p/35270383">盘点国内免费好用的图床</a></li><li><a href="https://meta.appinn.net/t/topic/13989">2020国内能用的图床集合</a></li></ul><p>简单的说如果想免费，博客又选择备案的情况下最推荐使用的是七牛云。如果想免费，又不想备案，一般会推荐使用一些久经考验的公共图床。当然公共图床也可以有会员服务。还要考虑访问问题，国内的图床服务基本上都必须备案，而很多海外用的多的图床又被墙了。备案的问题也有解法，但是我不想搞这么复杂。</p><p>比较推荐的公共图床有 sm.ms 和路过图床。公共图床的问题就是说不定哪天就没了或者被墙了。不过国内的互联网的互联网服务，又何尝不是说没就没呢？互联网没有永恒。</p><p>在纠结中发现有的图床工具内置支持的上传图床有 GitHub。GitHub 不是最被推荐的选择，主要问题和用 GitHub 搭博客一样，国内访问慢，有被墙的风险。不过既然我的博客已经是搭在 GitHub 上了，图床也用博客至少能保证图床不会比博客先挂。我的博客也不想要做成什么吸引访问的站点，主要是用作整理记录而已。因此最后选择用 GitHub 搭建图床。如果以后博客想更新换代或者去做备案了，到时候再更换图床吧。</p><p>再说一下选择的图床工具。基本上最推荐的工具就俩，iPic 和 PicGo。iPic 原来几乎是 Mac 下的最佳选择，久经考验。并且可以和 Typora 集成，在 Typora 中粘贴图片可以自动上床到图床。但是除了默认图床（似乎是已经用不了的微博图床），想选择使用其他图床需要付费。PicGo 也是很多人推荐的，并且是开源且免费的。在最近的版本更新中 Typora 也集成了使用 PicGo 做图像上传服务，内置的图床又已经配好了 GitHub。那我想就无须再纠结，就使用 PicGo + GitHub 做图床好了。</p><h4 id="GitHub-配置"><a href="#GitHub-配置" class="headerlink" title="GitHub 配置"></a>GitHub 配置</h4><ol><li><p>创建 Repository</p><p><img src="https://raw.githubusercontent.com/popesaga/img/master/img/20200803104029.png" alt="创建Repository"></p></li></ol><blockquote><ul><li>仓库最好是public的，因为private的仓库，图片链接会带token，这个token又存在过期的问题。</li><li>为repository初始化一个README.md文件可以根据需求选择，非必选</li></ul></blockquote><ol><li><p>获取授权 token</p><p>操作路径：Settings / Developer settings / Personal access tokens / Generate new Token。token 只会显示一次，注意保存。</p><p><img src="https://raw.githubusercontent.com/popesaga/img/master/img/image-20200803104504262.png" alt="获取授权 token"></p></li></ol><h4 id="PicGo-配置"><a href="#PicGo-配置" class="headerlink" title="PicGo 配置"></a>PicGo 配置</h4><ol><li><p>下载安装 PicGo 后运行。</p></li><li><p>配置图床</p><p><img src="https://raw.githubusercontent.com/popesaga/img/master/img/PicGo%20%E9%85%8D%E7%BD%AE%E5%9B%BE%E5%BA%8A" alt="PicGo 配置图床"></p></li></ol><ol><li><p>图床 CDN 配置</p><p>使用 jsDelivr 做图床 CDN，设定自定义域名为“<a href="https://link.zhihu.com/?target=https%3A//cdn.jsdelivr.net/gh/">https://cdn.jsdelivr.net/gh/</a>用户名/图床仓库名”</p></li><li><p>其他配置</p><p><img src="https://raw.githubusercontent.com/popesaga/img/master/img/20200803105540.png" alt="PicGo 设置"></p></li></ol><h4 id="Typora-配置"><a href="#Typora-配置" class="headerlink" title="Typora 配置"></a>Typora 配置</h4><p>偏好设置 / 图像 / 上传服务设定中选择 PicGo，点击“验证图片上传选项”后提示成功即配置成功。</p><p><img src="https://raw.githubusercontent.com/popesaga/img/master/img/20200803105841.png" alt="Typora 配置 PicGo"></p><h4 id="Typora-粘贴图片"><a href="#Typora-粘贴图片" class="headerlink" title="Typora 粘贴图片"></a>Typora 粘贴图片</h4><p>直接粘贴图片至文档中，会显示文件名和本地路径。Typora 自动调用 PicGo 上传图片至图床成功后会将本地路径替换为图床返回的链接地址。</p>]]></content>
    
    
    <categories>
      
      <category>杂七杂八</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Typora</tag>
      
      <tag>PicGo</tag>
      
      <tag>GitHub</tag>
      
      <tag>MarkDown</tag>
      
      <tag>图床</tag>
      
      <tag>Textlint</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建：GitHub Pages + Hexo + Fluid</title>
    <link href="/2020/07/09/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%EF%BC%9AGitHub%20Pages%20+%20Hexo%20+%20Fluid/"/>
    <url>/2020/07/09/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%EF%BC%9AGitHub%20Pages%20+%20Hexo%20+%20Fluid/</url>
    
    <content type="html"><![CDATA[<p>搞技术的，没有一个自己的个人博客可能说不过去。有注意到最近一些访问的博客站点是使用 Hexo 来搭建的，看起来效果还不错。因此尝试用 Hexo 搭建一个静态的博客站点，再存放到 GitHub Pages 上。以下介绍的一些本地安装配置，均在 macOS 下操作。</p><a id="more"></a><h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>首先需要安装 Git 和 Node.js ，git 已经安装好了。Mac 下安装 Node.js 需要使用 Homebrew ，之前安装的 Homebrew 已经年久失修，还需要更新。顺带补充下 Homebrew 的安装与使用。</p><h4 id="Homebrew-安装"><a href="#Homebrew-安装" class="headerlink" title="Homebrew 安装"></a>Homebrew 安装</h4><ul><li><p><a href="https://brew.sh/index_zh-cn.html">Homebrew 官方网站</a></p></li><li><p>安装<code>XCode</code>或者<code>Command Line Tools for Xcode</code>。Xcode 可以从 AppStore 里下载安装，<code>Command Line Tools for Xcode</code>需要在终端中输入以下代码运行安装：</p><pre><code class="hljs bash">xcode-select --install</code></pre></li><li><p>安装 Homebrew。将以下命令粘贴至终端</p><pre><code class="hljs bash">/bin/bash -c <span class="hljs-string">&quot;<span class="hljs-subst">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)</span>&quot;</span></code></pre><p>脚本会在执行前暂停，并说明将它将做什么。</p><blockquote><p>安装完成以后，需要运行<code>brew doctor</code>命令检测下是否有什么冲突的地方（如没有卸载<code>MacPorts</code>等等）</p></blockquote></li><li><p>基本使用</p><pre><code class="hljs bash"><span class="hljs-comment"># 搜索包</span>brew search mysql<span class="hljs-comment"># 安装包</span>brew install mysql<span class="hljs-comment"># 查看包信息，比如目前的版本，依赖，安装后注意事项等</span>brew info mysql<span class="hljs-comment"># 卸载包</span>brew uninstall wget<span class="hljs-comment"># 显示已安装的包</span>brew list<span class="hljs-comment"># 查看brew的帮助</span>brew –<span class="hljs-built_in">help</span><span class="hljs-comment"># 更新， 这会更新 Homebrew 自己</span>brew update<span class="hljs-comment"># 检查过时（是否有新版本），这会列出所有安装的包里，哪些可以升级</span>brew outdatedbrew outdated mysql<span class="hljs-comment"># 升级所有可以升级的软件们</span>brew upgradebrew upgrade mysql<span class="hljs-comment"># 清理不需要的版本极其安装包缓存</span>brew cleanupbrew cleanup mysql</code></pre></li><li><p>brew update 时遇到的一些问题</p><p>因为网络原因，brew update 会遇到执行出错的情况。</p><ul><li><p>下载失败</p><p>解决方案，使用国内镜像。有阿里巴巴、中科大、清华等可选。但是在公司折腾了前两个都不行，换了环境之后才下载成功。归根结底可能是公司网络环境的原因，因此替换镜像的方法仅做参考。</p><p>Zsh 终端配置：</p><pre><code class="hljs bash"><span class="hljs-comment"># 替换brew.git:</span><span class="hljs-built_in">cd</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(brew --repo)</span>&quot;</span>git remote set-url origin https://mirrors.aliyun.com/homebrew/brew.git<span class="hljs-comment"># 替换homebrew-core.git:</span><span class="hljs-built_in">cd</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(brew --repo)</span>/Library/Taps/homebrew/homebrew-core&quot;</span>git remote set-url origin https://mirrors.aliyun.com/homebrew/homebrew-core.git<span class="hljs-comment"># 替换homebrew-bottles:</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.aliyun.com/homebrew/homebrew-bottles&#x27;</span> &gt;&gt; ~/.zshrc<span class="hljs-built_in">source</span> ~/.zshrc<span class="hljs-comment"># 应用生效</span>brew update<span class="hljs-comment">#恢复默认配置</span><span class="hljs-comment"># 重置brew.git:</span><span class="hljs-built_in">cd</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(brew --repo)</span>&quot;</span>git remote set-url origin https://github.com/Homebrew/brew.git<span class="hljs-comment"># 重置homebrew-core.git:</span><span class="hljs-built_in">cd</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(brew --repo)</span>/Library/Taps/homebrew/homebrew-core&quot;</span>git remote set-url origin https://github.com/Homebrew/homebrew-core.git<span class="hljs-comment">#删掉 HOMEBREW_BOTTLE_DOMAIN 环境变量，打开~/.zshrc，找到 HOMEBREW_BOTTLE_DOMAIN 这行并删除</span><span class="hljs-built_in">source</span> ~/.zshrc</code></pre></li><li><p>“Checksum mismatch”</p><p>解决方案，删除 `Archive:` 后面路径下的文件，然后再重新 update 。</p><pre><code class="hljs bash">rm -rf /Users/aici/Library/Caches/Homebrew/portable-ruby--2.6.3.mavericks.bottle.tar.gz</code></pre></li></ul></li></ul><h4 id="Node-js-安装"><a href="#Node-js-安装" class="headerlink" title="Node.js 安装"></a>Node.js 安装</h4><ul><li><p>安装 Node.js</p><pre><code class="hljs bash">brew install node</code></pre></li><li><p>版本验证</p><pre><code class="hljs bash"><span class="hljs-comment">#出现版本号即说明安装成功</span>node -vnpm -v</code></pre></li><li><p>更换 npm 源</p><pre><code class="hljs bash">npm install -g cnpm --registry=<span class="hljs-string">&quot;https://registry.npm.taobao.org&quot;</span><span class="hljs-comment">#出现结果即可证明更换成功</span>cnpm</code></pre></li></ul><h3 id="Hexo-基础"><a href="#Hexo-基础" class="headerlink" title="Hexo 基础"></a>Hexo 基础</h3><ul><li><p>安装</p><pre><code class="hljs bash"><span class="hljs-comment">#全局安装，局部安装目前看没必要</span>npm install -g hexo-cli</code></pre></li><li><p>创建文件夹</p><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>mkdir hexo</code></pre></li><li><p>初始化</p><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/<span class="hljs-built_in">local</span>/hexohexo init</code></pre></li><li><p>启动服务，打开 <a href="https://www.feihua.xyz/http://localhost:4000">localhost:4000</a> 即可看到 hexo 博客</p><pre><code class="hljs bash">hexo s</code></pre></li></ul><h3 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h3><h4 id="主题选择"><a href="#主题选择" class="headerlink" title="主题选择"></a>主题选择</h4><ul><li><p><a href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></p><p>看起来功能完全覆盖了 Fluid，风格也是和 Fluid 类似的。配置文档太多了加上第一眼看到的是 Fluid，就放弃这个了。 </p></li><li><p><a href="https://github.com/fluid-dev/hexo-theme-fluid">Fluid</a></p><p>先看到这个再看到的 ButterFly，功能看起来不如 ButterFly 丰富，不过第一眼印象还是更喜欢这个。</p></li><li><p><a href="https://github.com/Shen-Yu/hexo-theme-ayer">Ayer</a></p><p>一个简洁也很漂亮的主题，也是让人纠结的选择。</p></li><li><p><a href="https://github.com/tufu9441/maupassant-hexo">Maupassant</a></p><p>非常简洁的主题，不想用 NexT 的可以考虑试一试。</p></li><li><p><a href="https://github.com/next-theme/hexo-theme-next">NexT</a></p><p>几乎是见的最多的主题，我心中 Hexo 博客默认的样子。功能很多，样式也很全面，几乎是不会出错的选择。</p></li></ul><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>完成的配置指南参见：<a href="https://hexo.fluid-dev.com/docs/guide/">Fluid配置指南</a>。下面仅列出一些关键配置。</p><h5 id="获取最新版本"><a href="#获取最新版本" class="headerlink" title="获取最新版本"></a>获取最新版本</h5><p>请优先下载 <a href="https://github.com/fluid-dev/hexo-theme-fluid/releases">最新 release 版本</a>，master 分支无法保证稳定。</p><p>下载后解压到 themes 目录下并重命名为 <code>fluid</code>。</p><h5 id="必要的配置"><a href="#必要的配置" class="headerlink" title="必要的配置"></a>必要的配置</h5><p>必须如下修改博客目录下的 <code>_config.yml</code>：</p><pre><code class="hljs yaml"><span class="hljs-attr">theme:</span> <span class="hljs-string">fluid</span>  <span class="hljs-comment"># 指定主题</span><span class="hljs-attr">language:</span> <span class="hljs-string">zh-CN</span>  <span class="hljs-comment"># 指定语言，可不改</span></code></pre><h5 id="创建「关于页」"><a href="#创建「关于页」" class="headerlink" title="创建「关于页」"></a>创建「关于页」</h5><p>首次使用主题的「关于页」需要手动创建：</p><pre><code class="hljs bash">$ hexo new page about</code></pre><p>创建成功后修改 <code>/source/about/index.md</code>，添加 <code>layout</code> 属性。</p><p>修改后的文件示例如下：</p><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">title:</span> <span class="hljs-string">about</span><span class="hljs-attr">date:</span> <span class="hljs-number">2020-07-20 10:55:54</span><span class="hljs-attr">layout:</span> <span class="hljs-string">about</span><span class="hljs-meta">---</span><span class="hljs-comment"># 这里可以写正文</span><span class="hljs-string">支持</span> <span class="hljs-string">Markdown,</span> <span class="hljs-string">HTML</span></code></pre><h5 id="覆盖配置"><a href="#覆盖配置" class="headerlink" title="覆盖配置"></a>覆盖配置</h5><p><strong>该功能可实现平滑升级主题，推荐所有人学习使用</strong>。</p><p>覆盖配置可以使<strong>主题配置</strong>放置在 fluid 目录之外，避免在更新主题时丢失自定义的配置。</p><p>使用该功能必须保证 Hexo 版本不低于 3.0，因为该功能利用了 <a href="https://hexo.io/zh-cn/docs/data-files.html">Hexo 数据文件</a> 功能。</p><p>使用方式：</p><ol><li>进入博客目录的 source 目录下（不是主题目录的 source），创建 <code>_data</code> 目录（和 <code>_post</code> 目录同级）；</li><li>在 <code>_data</code> 目录下创建 <code>fluid_config.yml</code> 文件，将 <code>/theme/fluid/_config.yml</code> 中全部配置（或部分配置）复制到 <code>fluid_config.yml</code> 中；</li><li>以后配置都在 <code>fluid_config.yml</code> 中修改，配置会在 <code>hexo g</code> 时自动覆盖。</li></ol><p>TIP</p><ul><li>也可以只覆盖部分配置，但注意只要存在于 <code>fluid_config.yml</code> 的配置都是高优先级，修改原 <code>_config.yml</code> 是无效的。</li><li>每次更新主题可能存在配置变更，请注意更新说明，可能需要手动对 <code>fluid_config.yml</code> 同步修改。</li><li>想查看覆盖配置有没有生效，可以通过 <code>hexo g --debug</code> 查看命令行输出。</li></ul><p>如果想将某些配置覆盖为空，注意不要把主键删掉，不然是无法覆盖的，比如：</p><pre><code class="hljs yaml"><span class="hljs-attr">about:</span>  <span class="hljs-attr">icons:</span>  <span class="hljs-comment"># 不要把 icon 注释掉，否则无法覆盖配置</span>    <span class="hljs-comment"># - &#123; class: &#x27;iconfont icon-github-fill&#x27;, link: &#x27;https://github.com&#x27; &#125;</span>    <span class="hljs-comment"># - &#123; class: &#x27;iconfont icon-wechat-fill&#x27;, qrcode: &#x27;/img/favicon.png&#x27; &#125;</span></code></pre><h5 id="博客标题"><a href="#博客标题" class="headerlink" title="博客标题"></a>博客标题</h5><p>页面左上角的博客标题，默认使用<strong>博客配置</strong>中的 <code>title</code>，这个配置同时控制着网页在浏览器标签中的标题。</p><p>如需单独区别设置，可在<strong>主题配置</strong>中设置：</p><pre><code class="hljs yaml"><span class="hljs-attr">navbar:</span>  <span class="hljs-attr">blog_title:</span> <span class="hljs-string">博客标题</span></code></pre><h5 id="统计-PV-与-UV"><a href="#统计-PV-与-UV" class="headerlink" title="统计 PV 与 UV"></a>统计 PV 与 UV</h5><p>页脚可以展示 PV 与 UV 统计数据，目前支持两种数据来源：<a href="https://www.leancloud.cn/">LeanCloud</a> 与 <a href="http://busuanzi.ibruce.info/">不蒜子</a>。此处使用 LeanClound，需要注册后填写相关配置才生效。</p><ul><li>注册 LeanClound 账号</li><li>创建应用</li><li>将应用的 AppId、 AppKey 和 REST API 服务器地址粘贴到 web_analytics 下对应位置</li><li>创建 Class 存储访问量等数据</li><li>footer.statistics.source 填写 leancloud</li></ul><h5 id="首页-Slogan（打字机）"><a href="#首页-Slogan（打字机）" class="headerlink" title="首页 Slogan（打字机）"></a>首页 Slogan（打字机）</h5><p>首页大图中的打字机文字，可在<strong>主题配置</strong>中设定是否开启：</p><pre><code class="hljs yaml"><span class="hljs-attr">index:</span>  <span class="hljs-attr">slogan:</span>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">text:</span> <span class="hljs-string">这是一条</span> <span class="hljs-string">Slogan</span></code></pre><p>如果 <code>text</code> 为空则按<strong>博客配置</strong>的 <code>subtitle</code> 显示。</p><p>相关的打字机动效设置在：</p><pre><code class="hljs yaml"><span class="hljs-attr">fun_features:</span>  <span class="hljs-attr">typing:</span> <span class="hljs-comment"># 为 subtitle 添加打字机效果</span>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">typeSpeed:</span> <span class="hljs-number">70</span> <span class="hljs-comment"># 打印速度</span>    <span class="hljs-attr">cursorChar:</span> <span class="hljs-string">&quot;_&quot;</span> <span class="hljs-comment"># 游标字符</span>    <span class="hljs-attr">loop:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 是否循环播放效果</span></code></pre><p>并不是很喜欢打字效果，直接关闭了。</p><h5 id="文章摘要"><a href="#文章摘要" class="headerlink" title="文章摘要"></a>文章摘要</h5><p>开关自动摘要（默认开启）：</p><pre><code class="hljs yaml"><span class="hljs-attr">index:</span>  <span class="hljs-attr">auto_excerpt:</span>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span></code></pre><p>若要手动指定摘要，使用 <code>&lt;!-- more --&gt;</code> MD 文档里划分，如：</p><pre><code class="hljs markdown">这里是摘要<span class="xml"><span class="hljs-comment">&lt;!-- more --&gt;</span></span>这里是正文</code></pre><p>按照配置文档里使用 <code>&lt;!-- more --&gt;</code> 后会报错，并且摘要未生效。如果配置 Front-matter 可以解决。未配置 Front-matter 时，也可以通过添加 —- 水平线解决，没有查到相关文档的解释。</p><p>或者在 <a href="https://hexo.io/zh-cn/docs/front-matter">Front-matter</a> 里设置 <code>excerpt</code> 字段，如：</p><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">title:</span> <span class="hljs-string">这是标题</span><span class="hljs-attr">excerpt:</span> <span class="hljs-string">这是摘要</span><span class="hljs-meta">---</span></code></pre><blockquote><p>TIP</p><p>优先级: 手动摘要 &gt; 自动摘要</p><p>如果关闭自动摘要，并且没有设置手动摘要，摘要区域空白</p><p>无论哪种摘要都最多显示 3 行，当屏幕宽度不足时会隐藏部分摘要。</p></blockquote><h5 id="文章内容图片"><a href="#文章内容图片" class="headerlink" title="文章内容图片"></a>文章内容图片</h5><p>本地图片存放位置同上。</p><pre><code class="hljs markdown">![](/img/example.jpg)</code></pre><h5 id="日期-字数-阅读时长-阅读数"><a href="#日期-字数-阅读时长-阅读数" class="headerlink" title="日期/字数/阅读时长/阅读数"></a>日期/字数/阅读时长/阅读数</h5><p>显示在文章页大标题下的文章信息，除了阅读次数，其他功能都是默认开启的。</p><pre><code class="hljs yaml"><span class="hljs-attr">post:</span>  <span class="hljs-attr">meta:</span>    <span class="hljs-attr">date:</span>  <span class="hljs-comment"># 日期</span>      <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">format:</span> <span class="hljs-string">&quot;dddd, MMMM Do YYYY, h:mm a&quot;</span>  <span class="hljs-comment"># 格式参照 ISO-8601 日期格式化</span>    <span class="hljs-attr">wordcount:</span>  <span class="hljs-comment"># 字数统计</span>      <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">format:</span> <span class="hljs-string">&quot;&#123;&#125; 字&quot;</span>  <span class="hljs-comment"># 显示的文本，&#123;&#125;是数字的占位符（必须包含)，下同</span>    <span class="hljs-attr">min2read:</span>  <span class="hljs-comment"># 阅读时间</span>      <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">format:</span> <span class="hljs-string">&quot;&#123;&#125; 分钟&quot;</span>    <span class="hljs-attr">views:</span>  <span class="hljs-comment"># 阅读次数</span>      <span class="hljs-attr">enable:</span> <span class="hljs-literal">false</span>      <span class="hljs-attr">source:</span> <span class="hljs-string">&quot;leancloud&quot;</span>  <span class="hljs-comment"># 统计数据来源，可选：leancloud | busuanzi   注意不蒜子会间歇抽风</span>      <span class="hljs-attr">format:</span> <span class="hljs-string">&quot;&#123;&#125; 次&quot;</span></code></pre><h5 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h5><p>开启评论需要在<strong>主题配置</strong>中开启并指定评论模块：</p><pre><code class="hljs yaml"><span class="hljs-attr">post:</span>  <span class="hljs-attr">comments:</span>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">valine</span></code></pre><p>比较推荐的主要是两个：utterances 和 valine。此处使用 valine，据说配置方便又简约，基于 Leancloud 的无后端设计。</p><ul><li>请先<a href="https://leancloud.cn/dashboard/login.html#/signin">登录</a>或<a href="https://leancloud.cn/dashboard/login.html#/signup">注册</a> <code>LeanCloud</code>, 进入<a href="https://leancloud.cn/dashboard/applist.html#/apps">控制台</a>后点击左下角<a href="https://leancloud.cn/dashboard/applist.html#/newapp">创建应用</a></li><li>应用创建好以后，进入刚刚创建的应用，选择左下角的<code>设置</code>&gt;<code>应用Key</code>，然后就能看到你的<code>APP ID</code>和<code>APP Key</code>了</li><li>创建 Class 存储评论数据</li><li>将应用的 AppId、 AppKey 和 REST API 服务器地址粘贴到 valine 下对应位置</li><li>其他配置视情况需要修改</li></ul><h5 id="脚注"><a href="#脚注" class="headerlink" title="脚注"></a>脚注</h5><p>主题内置了脚注语法支持，可以在文章末尾自动生成带有锚点的脚注，该功能在<strong>主题配置</strong>中默认开启：</p><pre><code class="hljs yaml"><span class="hljs-attr">post:</span>  <span class="hljs-attr">footnote:</span>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">header:</span> <span class="hljs-string">&#x27;&#x27;</span></code></pre><p>脚注语法如下：</p><pre><code class="hljs markdown">这是一句话[^1][<span class="hljs-symbol">^1</span>]: <span class="hljs-link">这是对应的脚注</span></code></pre><p>更优雅的使用方式，是将脚注写在文末，比如：</p><pre><code class="hljs markdown">正文<span class="hljs-section">## 参考</span>[<span class="hljs-symbol">^1</span>]: <span class="hljs-link">参考资料1</span>[<span class="hljs-symbol">^2</span>]: <span class="hljs-link">参考资料2</span></code></pre><p>当然你也可以通过修改上方配置项 <code>header</code> 来自动加入节标题，如下所示：</p><pre><code class="hljs yaml"><span class="hljs-attr">post:</span>  <span class="hljs-attr">footnote:</span>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">header:</span> <span class="hljs-string">&#x27;&lt;h2&gt;参考&lt;/h2&gt;&#x27;</span>  <span class="hljs-comment"># 等同于手动写 `## 参考`</span></code></pre><h5 id="分类和标签"><a href="#分类和标签" class="headerlink" title="分类和标签"></a>分类和标签</h5><p>只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p><pre><code class="hljs subunit">categories:- Diary<span class="hljs-keyword">tags:</span>- PS3- Games</code></pre><blockquote><p>分类方法的分歧</p><p>如果您有过使用 WordPress 的经验，就很容易误解 Hexo 的分类方式。WordPress 支持对一篇文章设置多个分类，而且这些分类可以是同级的，也可以是父子分类。但是 Hexo 不支持指定多个同级分类。下面的指定方法：</p><pre><code class="hljs markdown">categories:<span class="hljs-bullet">  -</span> Diary<span class="hljs-bullet">  -</span> Life</code></pre><p>会使分类<code>Life</code>成为<code>Diary</code>的子分类，而不是并列分类。因此，有必要为您的文章选择尽可能准确的分类。</p><p>如果你需要为文章添加多个分类，可以尝试以下 list 中的方法。</p><pre><code class="hljs asciidoc">categories:<span class="hljs-bullet">- </span>[Diary, PlayStation]<span class="hljs-bullet">- </span>[Diary, Games]<span class="hljs-bullet">- </span>[Life]</code></pre><p>此时这篇文章同时包括三个分类： <code>PlayStation</code> 和 <code>Games</code> 分别都是父分类 <code>Diary</code> 的子分类，同时 <code>Life</code> 是一个没有子分类的分类。</p></blockquote><h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><h5 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h5><p>在 github 上创建博客的仓库，创建仓库地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/new">https://github.com/new</a>。如果你希望你的站点能通过 &lt;你的 GitHub 用户名&gt;.github.io 域名访问，你的 repository 应该直接命名为 &lt;你的 GitHub 用户名&gt;.github.io。</p><h5 id="一键部署"><a href="#一键部署" class="headerlink" title="一键部署"></a>一键部署</h5><ul><li><p>修改博客目录下的 <code>_config.yml</code></p><pre><code class="hljs yaml"><span class="hljs-attr">deploy:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span>    <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/&lt;你的</span> <span class="hljs-string">GitHub</span> <span class="hljs-string">用户名&gt;/&lt;你的</span> <span class="hljs-string">GitHub</span> <span class="hljs-string">用户名&gt;.github.io</span>  <span class="hljs-attr">branch:</span> <span class="hljs-string">master</span></code></pre><blockquote><p>TIP</p><p>官方教程有坑！</p><p>- <code>github.com/username/username.github.io</code> 这样的仓库用<code>master</code>分支 </p><p>- <code>github.com/username/cengjingbeikengguo</code> 这样的用<code>gh-pages</code></p></blockquote></li><li><p>安装 <a href="https://github.com/hexojs/hexo-deployer-git">hexo-deployer-git</a>，使用 cnpm</p><pre><code class="hljs sql">$ cnpm <span class="hljs-keyword">install</span> hexo-deployer-git <span class="hljs-comment">--save</span></code></pre></li><li><p>生成站点文件并推送至远程库。 hexo clean &amp;&amp; hexo deploy</p><pre><code class="hljs bash">$ hexo clean &amp;&amp; hexo deploy</code></pre></li><li><p>登入 Github，请在库设置（Repository Settings）中将默认分支设置为 _config.yml 配置中的分支名称。稍等片刻，您的站点就会显示在您的 Github Pages 中。</p></li></ul><h3 id="日常使用"><a href="#日常使用" class="headerlink" title="日常使用"></a>日常使用</h3><h4 id="创建新文章"><a href="#创建新文章" class="headerlink" title="创建新文章"></a>创建新文章</h4><pre><code class="hljs bash"><span class="hljs-comment"># 创建新文章</span>hexo new <span class="hljs-string">&quot;xxx xxx xxx&quot;</span><span class="hljs-comment"># 测试</span>hexo s<span class="hljs-comment"># 生成站点并部署</span>hexo d -g</code></pre><h3 id="更新-hexo-及插件"><a href="#更新-hexo-及插件" class="headerlink" title="更新 hexo 及插件"></a>更新 <code>hexo</code> 及插件</h3><p>更新 <code>hexo</code></p><pre><code class="hljs coffeescript"><span class="hljs-built_in">npm</span> update -g hexo</code></pre><p>更新插件</p><pre><code class="hljs coffeescript"><span class="hljs-built_in">npm</span> update</code></pre><h3 id="更换电脑"><a href="#更换电脑" class="headerlink" title="更换电脑"></a>更换电脑</h3><ul><li>使用 git 同步</li><li>使用坚果云选择性同步这几个文件夹。然后在另一台电脑上搭建博客环境、安装相关插件后。同步这些文件，然后重新生成、部署站点。</li></ul><h3 id="提交到-Google-收录"><a href="#提交到-Google-收录" class="headerlink" title="提交到 Google 收录"></a>提交到 Google 收录</h3><h4 id="网址验证"><a href="#网址验证" class="headerlink" title="网址验证"></a>网址验证</h4><ol><li><p>进入<a href="https://www.google.com/webmasters/tools/home?hl=zh-CN">Google Search Console</a> 登录你的谷歌账号</p></li><li><p>在网址前缀中填写地址</p></li><li><p>下载验证文件</p></li><li><p>拷贝到你的本地博客 hexo/sources/ 下</p><blockquote><p><strong>注意</strong>：hexo在部署source 文件夹下markdown语法格式的文件成html格式时（本身文件格式就是html格式），都会遵守固有的html布局格式，<strong>所以后面Google验证html文件时，此时的“html验证文件”已经不是原本下载的文件，变成遵守固有布局的html文件</strong>，为了正常验证步骤进行，部署服务器前必须先打开“html验证文件“，加入以下内容，让固有的html布局失效。</p><pre><code class="hljs xml">&gt;layout: false&gt;---</code></pre></blockquote></li><li><p>部属到 GitHub 远程仓库</p><pre><code class="hljs bash">hexo g -d</code></pre></li><li><p>本地操作完成后，继续返回到网站页面，点击验证，成功！</p></li></ol><h4 id="生成站点地图"><a href="#生成站点地图" class="headerlink" title="生成站点地图"></a>生成站点地图</h4><p>在本地 Hexo 博客根目录下安装 sitemap 生成插件</p><pre><code class="hljs bash">npm install hexo-generator-sitemap --save</code></pre><p>编辑站点目录下的_config.yml 配置文件，添加以下字段：</p><pre><code class="hljs yaml"><span class="hljs-comment"># 自动生成sitemap</span><span class="hljs-attr">sitemap:</span>  <span class="hljs-attr">path:</span> <span class="hljs-string">sitemap.xml</span><span class="hljs-comment"># 修改url为网站地址</span><span class="hljs-attr">url:</span> <span class="hljs-string">https://popesaga.github.io/</span></code></pre><p>部署到远程仓库</p><pre><code class="hljs sh">$ hexo g -d</code></pre><h4 id="添加站点地图"><a href="#添加站点地图" class="headerlink" title="添加站点地图"></a>添加站点地图</h4><p>这时在远程仓库根目录就可以看到<code>sitemap.xml</code>文件了。<br>返回<a href="https://www.google.com/webmasters/tools/home?hl=zh-CN">谷歌站点地图</a>，选择已经验证过的站点，在站点地图 中，添加新的站点地图，添加 sitemap.xml 的链接。</p><p>添加成功后等待 google 收录即可。</p>]]></content>
    
    
    <categories>
      
      <category>杂七杂八</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GitHub</tag>
      
      <tag>Hexo</tag>
      
      <tag>Fluid</tag>
      
      <tag>Homebrew</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
